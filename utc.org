#+TITLE: Wisconsin Urban Tree Canopy Mapping
#+AUTHOR: Tedward Erker
#+email: erker@wisc.edu
#+PROPERTY:  header-args:R :session *R* :cache no :results output :exports both :tangle yes

* Begin with end in mind
** End Product:
A map of urban cover with associated model uncertainty.  For the
Madison Area.

A reproducible workflow to apply this to any city in WI (or perhaps
country) with minimal work from user.

** Best Classifer
To make this map I need to find the best classifier to apply to the
maps.  To know the best classifier I need to perform an accuracy assessment.


npx = number of pixels in segmentation
cpt = compactness parameter

| Image       | Method | Pixel (Overall Accuracy) | Field Plot (RMSE) | Block (RMSE) |
|-------------+--------+--------------------------+-------------------+--------------|
| NAIP        | SVM    |                          |                   |              |
|             | RF     |                          |                   |              |
|             | SVM    |                          |                   |              |
|             | RF     |                          |                   |              |
|             | SVM    |                          |                   |              |
|             | RF     |                          |                   |              |
|             | SVM    |                          |                   |              |
|             | RF     |                          |                   |              |
|             |        |                          |                   |              |
|             |        |                          |                   |              |
|             |        |                          |                   |              |
|             |        |                          |                   |              |
|             |        |                          |                   |              |
|             |        |                          |                   |              |
| PAN_SPOT... |        |                          |                   |              |

** Directory Structure
*** Pjt_UTC
**** code
- contains this file
**** figs
**** DD
***** NAIP.image
****** pca.model.transformation
****** reprojected.training.regions
****** reprojected.testing.regions
******* grid
******* field.data
****** reprojected.pca.regions
****** training.outputs
****** feature.dfs
******* pixel.feature.df
******* segment.X.feature.df
****** Models
****** testing.accuracy.outputs
******** grid.classified.regions
******** field.classified.regions
***** PansharpenedSPOT.image
****** pca.model.transformation
****** reprojected.training.regions
****** reprojected.testing.regions
******* grid
******* field.data
****** reprojected.pca.regions
****** training.outputs
****** feature.dfs
******* pixel.feature.df
******* segment.X.feature.df
****** Models
****** testing.accuracy.outputs
******** grid.classified.regions
******** field.classified.regions



**** RD_UrbanAreasShapefile
**** RD_WaterShapefile
**** RD_CroplandDataLayer

**** RD_Training_Regions
***** Madison
Shapefile that contains the regions from which I'll do
manual/supervised classification in the Madison Area
***** Wausau
Shapefile that contains the regions from which I'll do
manual/supervised classification in the Madison Area

**** RD_Accuracy
***** Grids
***** Field Plot
***** Robi's Points
**** RD_NAIP
**** RD_SPOT
**** DD_NAIP
***** Training
****** Madison
******* j directories for each polygon of the training shapefile
******** image : raw image that is cropped to tile
******** masked : image that is masked by water, crops, and urban boundary
******** AddedFeatures : image that has added derived bands
******** PCA : image that is the PCA results scaled to 0 - 255
******** Segmentation : image that is the segmentation layer
******** Segmentation Polygons: polygonized Segmentation Layer
******** SegmentFeatures : dataframe that has each row as segment, columns as features, Class from Segmentation Polygons added.
******** Classified Segmentation Polygons: polygonized Segmentation Layer with manual classifications
******* CombinedSegmentFeatures : dataframe that is combined
****** Wausau
Same as in Madison, but for Wausau

***** Testing
****** grids
******* i directories for each polygon of the grids
******** image : raw image that is cropped to tile
******** masked : image that is masked by water, crops, and urban boundary
******** AddedFeatures : image that has added derived bands
******** PCA : image that is the PCA results scaled to 0 - 255
******** Segmentation : image that is the segmentation layer
******** SegmentFeatures : dataframe that has each row as segment, columns as features.
******** ClassifiedImages : classified images that use each model
********* k classified images for each model

****** fieldplots
Same as for grids

***** Accuracy Assessment
****** Dataframe that summary statistics for each classification, accuracy method
****** Tables of Results


***** Models
R model objects
***** Best Model
R model object, determined by accuracy assessment
***** i directories for each urban area
****** image cropped to urban area .tif
****** j directories for each tile within each of the i urban areas
******* image : raw image that is cropped to tile
******* masked : image that is masked by water, crops, and urban boundary
******* AddedFeatures : image that has added derived bands
******* PCA : image that is the PCA results scaled to 0 - 255
******* Segmentation : image that is the segmentation layer
******* SegmentFeatures : dataframe that has each row as segment, columns as features
******* ClassifiedImage : classified image that uses the best model

**** DD_SPOT
Same structure as DD_NAIP


* Workflow
** Functions
*** Reproject Shapefile to Image Coordinate Reference System
#+BEGIN_SRC R
    Reproject_Shapefile_to_Image_CRS <- function(shapefile.dsn,
                                                 shapefile.layer,
                                                 image.path,
                                                 shapefile.out.dsn) {
        r <- stack(image.path)
        shapefile <- readOGR(shapefile.dsn, shapefile.layer)
        shapefile.WimageCRS <- spTransform(shapefile, crs(r))
        writeOGR(shapefile.WimageCRS, shapefile.out.dsn, shapefile.layer, driver = "ESRI Shapefile", overwrite =T)
    }
#+END_SRC

#+RESULTS:

*** Crop image to each Shapefile polygon
#+BEGIN_SRC R
    Crop_image_to_each_Shapefile_polygon <- function(shapefile.dsn,
                                                     shapefile.layer,
                                                     image.path,
                                                     cores,
                                                     output.dir)  {
        shape <- readOGR(shapefile.dsn, shapefile.layer)
        polygons <- as(shape, "SpatialPolygons")

        image <- stack(image.path)

        cl <- makeCluster(cores)
        registerDoParallel(cl)

        foreach (i = seq_along(polygons),
                 .packages = c("raster")) %dopar% {
                r <- image
                r <- crop(r, polygons[i])
                writeRaster(r, paste0(output.dir,"/",i,".tif"),
                            overwrite = T)
            }
        }

#+END_SRC

#+RESULTS:

*** Crop image to regions around shapefile points
#+BEGIN_SRC R

    # assign the polygon name to the points.
    give_polygons_attributes_of_first_point_within <- function(points,
                                                          polygons){
    po <- gIntersects(points, polygons, byid=TRUE)
    out <- foreach(polygon.number = seq_along(polygons), .combine = "rbind") %do% {
        first.point.data <- points[po[polygon.number,],]@data %>%
            slice(1)
        pd <- as(polygons[polygon.number], "SpatialPolygonsDataFrame")
        pd@data <- first.point.data
        pd
    }
  }

            Crop_image_to_regions_around_points_nameBygrid<- function(shapefile.dsn,
                                                             shapefile.layer,
                                                             image.path,
                                                             cores,
                                                             output.dir,
                                                             column.name = "unq__ID")  {

                points <- readOGR(shapefile.dsn, shapefile.layer)
                box <- gBuffer(points, width = 8)
                box <- disaggregate(box)

                polygons <- as(box, "SpatialPolygons")

                polygons <- give_polygons_attributes_of_first_point_within(points,polygons)

                image <- stack(image.path)

                image.extent <- as(extent(image), "SpatialPolygons")
                proj4string(image.extent) <- proj4string(image)


                polygons.in.image <- foreach(i = seq_along(polygons),.combine = "c") %do% {
                    gIntersects(polygons[i,],image.extent)
                }

                polygons <- polygons[polygons.in.image,]

                cl <- makeCluster(cores)
                registerDoParallel(cl)

                foreach (i = seq_along(polygons),
                         .packages = c("raster")) %dopar% {
                        r <- image
                        r <- crop(r, polygons[i,])
                        grid.id <- polygons@data[i,column.name]
                        writeRaster(r, paste0(output.dir,"/",grid.id,".tif"),
                                    overwrite = T)
                    }
                }




#  shapefile.dsn = grid.accuracy.region.imageCRS.dsn
#  shapefile.layer = grid.accuracy.region.layer,
#  output.dir = image.cropped.to.grid.accuracy.dir


            Crop_image_to_regions_around_points <- function(shapefile.dsn,
                                                             shapefile.layer,
                                                             image.path,
                                                             cores,
                                                             output.dir)  {

                points <- readOGR(shapefile.dsn, shapefile.layer)
                box <- gBuffer(points, width = 8)
                box <- disaggregate(box)

                polygons <- as(box, "SpatialPolygons")

                image <- stack(image.path)

                cl <- makeCluster(cores)
                registerDoParallel(cl)

                foreach (i = seq_along(polygons),
                         .packages = c("raster")) %dopar% {
                        r <- image
                        r <- crop(r, polygons[i])
                        writeRaster(r, paste0(output.dir,"/",i,".tif"),
                                    overwrite = T)
                    }
                }

#+END_SRC

#+RESULTS:

#+BEGIN_SRC R :results graphics :file figs/box.png
plot(box[1])
#+END_SRC

#+RESULTS:
[[file:figs/box.png]]

*** Make new ratio bands from image
#+BEGIN_SRC R
    ratio <- function(image_w4bands, numerator_bandNumber) {
        r <- image_w4bands[,,numerator_bandNumber,drop = F] / sum(image_w4bands)
        return(r)
    }

    ndvi_nodrop <- function(image_w4bands,red_bandnumber,nir_bandnumber,...) {
        red_band <- image_w4bands[[red_bandnumber]]
        nir_band <- image_w4bands[[nir_bandnumber]]
        ndvi <- (nir_band - red_band)/(nir_band + red_band)
        return(ndvi)
    }

    add.ratios.ndvi <- function(tile.dir,
                                tile.name,
                                out.tile.name.append = ratio.tile.name.append,
                                band.names = c("blue","green","red","nir"),
                                red.band.number = 3,
                                nir.band.number = 4) {

        in.tile.path <- str_c(tile.dir, "/", tile.name, ".tif")
        tile <- stack(in.tile.path)
        names(tile) <- band.names

            # Create a ratio image for each band
        ratio.brick <- ratio(tile)
        ratio.brick <- ratio.brick*200 # rescale ndvi to save as 'INT1U'
        names(ratio.brick) <- paste0(band.names,rep("_ratio",times = 4))
        ndvi <- ndvi_nodrop(tile, red.band.number, nir.band.number)
        ndvi <- (ndvi+1)*100 # rescale ndvi to savep as 'INT1U'

        # if tile is not scaled 0-255, do it here
        if (getRasterMax(tile) > 255) {
            min <- getRasterMin(tile)
            max <- getRasterMax(tile)
            tile <- rescale.0.255(tile,min,max)
        }

        ratio.tile <- raster::stack(tile, ratio.brick, ndvi)
        writeRaster(ratio.tile,
                    filename = paste0(tile.dir,"/",tile.name,out.tile.name.append, ".tif"),
                    overwrite = T,
                    datatype = 'INT1U')
        }
#+END_SRC

#+RESULTS:

*** Image PCA
#+BEGIN_SRC R
              getRasterMin <- function(t) {
                  return(min(cellStats(t, stat = "min")))
              }

              getRasterMax <- function(t) {
                  return(max(cellStats(t, stat = "max")))
              }

      rescale.0.255 <- function(raster,
                                min,
                                max) {
                      (raster - min)/(max-min) * 255
                }

      image.pca <- function(image.dir,
                            tile.name,
                            in.image.appendage = ratio.tile.name.append,
                            out.image.appendage = pca.tile.name.append,
                            band.names = c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi"),
                            comps.to.use = c(1,2,3),
                            pca.model = pca) {


          out.path <- str_c(image.dir, "/", tile.name, out.image.appendage, ".tif")

          s <- stack(str_c(image.dir, "/", tilex.name, in.image.appendage,".tif"))
          names(s) <- band.names

          r <- predict(s, pca.model, index = comps.to.use)

          min.r <- getRasterMin(r)
          max.r <- getRasterMax(r)
          rescaled.r <- rescale.0.255(r, min.r, max.r)
          writeRaster(rescaled.r, filename = out.path, overwrite=TRUE, datatype = 'INT1U')
      }



  make.and.save.pca.transformation <- function(image.dir,
                                               pca.model.name = "pca.rds",
                                              max.sample.size = 10000,
                                              core.num = cores,
                                              band.names = c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")) {
      tile.paths <- list.files(str_c(image.dir), pattern = "*_with_ratios.tif$", full.names = T)
      tile.names <- list.files(str_c(image.dir), pattern = "*_with_ratios.tif$", full.names = F)

      cl <- makeCluster(core.num)
      registerDoParallel(cl)

      sr <- foreach (i = seq_along(tile.names), .packages = c("raster"), .combine ="rbind") %dopar% {
          tile <- stack(tile.paths[i])
          s <- sampleRandom(tile, ifelse(ncell(tile) > max.sample.size ,max.sample.size, ncell(tile)))
      }

      colnames(sr) <- band.names

                                              # Perform PCA on sample
      pca <- prcomp(sr, scale = T)
      saveRDS(pca,paste0(image.dir,"/",pca.model.name))
      return(pca)
  }




    ## image.dir <- image.cropped.to.training.dir
    ## image.name <- 9
    ##                         in.image.appendage = ratio.tile.name.append
    ##                         out.image.appendage = pca.tile.name.append
    ##                         band.names = c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")
    ##                         max.sample.size = 10000
    ##                         comps.to.use = c(1,2,3)

    ##       out.path <- str_c(image.dir, "/", image.name, out.image.appendage, ".tif")

    ##       s <- stack(str_c(image.dir, "/", image.name, in.image.appendage,".tif"))
    ##       names(s) <- band.names

    ##       sr <- sampleRandom(s, ifelse(ncell(s) > max.sample.size, max.sample.size, ncell(s)))
    ##       pca <- prcomp(sr, scale = T)

    ##       r <- predict(s, pca, index = comps.to.use)

    ##       min.r <- getRasterMin(r)
    ##       max.r <- getRasterMax(r)
    ##       rescaled.r <- rescale.0.255(r, min.r, max.r)
    ##       writeRaster(rescaled.r, filename = out.path, overwrite=TRUE, datatype = 'INT1U')









              # Function takes raster stack, samples data, performs pca and returns stack of first n_pcomp bands
                ## predict_pca_wSampling_parallel <- function(stack, sampleNumber, n_pcomp, nCores = detectCores()-1) {
                ##     sr <- sampleRandom(stack,sampleNumber)
                ##     pca <- prcomp(sr, scale=T)
                ##     beginCluster()
                ##     r <- clusterR(stack, predict, args = list(pca, index = 1:n_pcomp))
                ##     endCluster()
                ##     return(r)
                ## }
#+END_SRC

#+RESULTS:

*** polygonize segment raster with gdal and add Class to shapefile

#+BEGIN_SRC R
            gdal_polygonizeR <- function(x, outshape=NULL, gdalformat = 'ESRI Shapefile',
                                         pypath=NULL, readpoly=TRUE, quiet=TRUE) {
              if (isTRUE(readpoly)) require(rgdal)
              if (is.null(pypath)) {
                pypath <- Sys.which('gdal_polygonize.py')
              }
              if (!file.exists(pypath)) stop("Can't find gdal_polygonize.py on your system.")
              owd <- getwd()
              on.exit(setwd(owd))
              setwd(dirname(pypath))
              if (!is.null(outshape)) {
                outshape <- sub('\\.shp$', '', outshape)
                f.exists <- file.exists(paste(outshape, c('shp', 'shx', 'dbf'), sep='.'))
                if (any(f.exists))
                  stop(sprintf('File already exists: %s',
                               toString(paste(outshape, c('shp', 'shx', 'dbf'),
                                              sep='.')[f.exists])), call.=FALSE)
              } else outshape <- tempfile()
              if (is(x, 'Raster')) {
                require(raster)
                writeRaster(x, {f <- tempfile(fileext='.asc')})
                rastpath <- normalizePath(f)
              } else if (is.character(x)) {
                rastpath <- normalizePath(x)
              } else stop('x must be a file path (character string), or a Raster object.')
              system2('python', args=(sprintf('"%1$s" "%2$s" -f "%3$s" "%4$s.shp"',
                                              pypath, rastpath, gdalformat, outshape)))
              if (isTRUE(readpoly)) {
                shp <- readOGR(dirname(outshape), layer = basename(outshape), verbose=!quiet)
                return(shp)
              }
              return(NULL)
            }


    polygonize.and.add.Class <- function(image.dir,
                                         image.name,
                                         segment.appendage = segment.tile.name.append,
                                         no.class = "N") {
          seg <- raster(paste0(image.dir,"/",image.name,segment.appendage,'.tif'))
          segPoly <- gdal_polygonizeR(seg)
          segPoly$Class <- no.class
          writeOGR(obj = segPoly,
                   dsn = paste0(image.dir,"/",image.name),
                   layer = paste0(image.name,segment.appendage),
                   driver = "ESRI Shapefile",
                   overwrite = T)
  }






#+END_SRC

#+RESULTS:

*** other Functions
#+BEGIN_SRC R

        image_to_classified_image <- function()





              # contained urban, don't intersect water = as is
              # contained urban, intersect water = mask water
              # intersect urban, don't intersect water = mask urban
              # intersect urban, intersect water = mask urban & water
            # if none of the above, don't write the raster



            Mask_water_crops_urban <- function(image.full.path, water, crops, urban) {

            }




              Water_Urban_mask <- function(tile.path, tile.name, urban, water) {
                                                      # load image tile
                  tile <- stack(tile.path)
                                                      # get extent image and make sp object
                  et <- as(extent(tile), "SpatialPolygons")
                  proj4string(et) <- "+init=epsg:26916"
                                                      # Mask out non-urban areas
                  if(gContainsProperly(urban,et) & !gIntersects(water,et)){
                      writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
                  } else if (gContainsProperly(urban,et) & gIntersects(water,et)) {
                      tile <- mask(tile, water, inverse = T)
                      writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
                  } else if (gIntersects(urban, et) & !gIntersects(water,et)) {
                      tile <- mask(tile, urban)
                      writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
                  } else if (gIntersects(urban, et) & gIntersects(water,et)) {
                      tile <- mask(tile, urban)
                      tile <- mask(tile, water, inverse = T)
                      writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
                  }
              }

          Crop_mask <- function(tile.path, tile.name, CDL_stack, n_years){

            tile <- stack(tile.path)
            crops <- crop(CDL_stack, tile)

                  # These are the values in the CDL that correspond to non crop cover types and not water
                  NonCroppedValues <- c(0,63:65, 81:83, 87:88, 112, 121:124, 131, 141:143, 152, 176, 190, 195)
                  # open water is 111

                  NonCroppedValues <- c(0,63:65, 81:83, 87:88, 112, 121:124, 131, 141:143, 152, 176, 190, 195)
                  # open water is 111. I don't include it in the above list so that it gets masked

                  # I'm going to add 37, Other Hay/Non-alfalfa, to the non crop cover types
                  NonCroppedValues <- c(NonCroppedValues, 37)
                  # I'm going to add 36, Alfalfa, to the non crop cover types
                  NonCroppedValues <- c(NonCroppedValues, 36)

                  # find cells that have been assigned crop all three years
                  crops[crops %in% NonCroppedValues] <- 0
                  crops[!(crops %in% NonCroppedValues)] <- 1
                  cropsum <- overlay(crops, fun = sum)

                  dis.cropsum <- disaggregate(cropsum, fact = 20)
                  dis.cropsum <- resample(dis.cropsum, tile, "ngb")
                  masked_tile <- mask(tile, dis.cropsum, maskvalue = n_years)

                  #               Save Image
                  writeRaster(masked_tile, paste0(crop.masked.tiles.directory, "/", tile.name), overwrite = T)
              }








#+END_SRC

#+RESULTS:

*** Make Pixel Feature DF
#+BEGIN_SRC R
Create.Pixel.Feature.df<- function(raster.list,
                                   band.names = c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")) {
    r.df.list <- lapply(raster.list, function(r) {
                            names(r) <- band.names
                            as.data.frame(r, xy=T)
           })
    bind_rows(r.df.list)
}
#+END_SRC

#+RESULTS:

*** Make Segment Feature DF
#+BEGIN_SRC R


    fitXYlm <- function(x,y,z) {
        dat <- data.frame(x,y,z)
        mod <- lm(z ~ x * y, data = dat)
        coefs <-tidy(mod) %>%
            dplyr::select(term,estimate) %>%
            spread(key = term, value = estimate)

        error <- glance(mod) %>%
            select(sigma)

        bind_cols(coefs,error)
    }

    Create.Segment.Feature.df <- function(image.dir,
                                          image.name,
                                          ratio.appendage = ratio.tile.name.append,
                                          segment.appendage = segment.tile.name.append,
                                          segment.feature.df.appendage = segment.feature.df.name.append,
                                          band.names = c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")
                                          ){
        ratio.tile.path <- str_c(image.dir, "/", image.name, ratio.tile.name.append, ".tif")
        r.tile <- stack(ratio.tile.path)
        names(r.tile) <- band.names

        seg.tile.path <-  str_c(image.dir, "/", image.name, segment.tile.name.append, ".tif")
        s.tile <- raster(seg.tile.path)

                                            # Create a data_frame where mean and variances are calculated by zone
        x <- as.data.frame(r.tile, xy = T)
        s <- as.data.frame(s.tile)
        colnames(s) <- "segment"
        r <- bind_cols(x,s)
        r2 <- r %>%
            group_by(segment) %>%
            mutate(x.center = x - quantile(x = x, probs = .5),
                   y.center = y - quantile(x = y, probs = .5))

        spatial.model.coef <- r2 %>%
            do(fitXYlm(x = .$x.center, y = .$y.center, z = .$n_ratio))

        mean.and.sd <- r2 %>%
            summarize(mean(blue),
                  mean(green),
                  mean(red),
                  mean(nir),
                  mean(b_ratio),
                  mean(g_ratio),
                  mean(r_ratio),
                  mean(n_ratio),
                  mean(ndvi),
                  sd(blue),
                  sd(green),
                  sd(red),
                  sd(nir),
                  sd(b_ratio),
                  sd(g_ratio),
                  sd(r_ratio),
                  sd(n_ratio),
                  sd(ndvi))

        out <- left_join(spatial.model.coef, mean.and.sd)

        names <- colnames(out)
        names <- str_replace(names, "\\(",".")
        names <- str_replace(names, "\\)",".")
        names <- str_replace(names, "\\:",".")
        colnames(out) <- names

        saveRDS(out, file = paste0(image.dir,"/", image.name, segment.feature.df.appendage,".rds"))
  }
#+END_SRC

#+RESULTS:

*** Create ModelBuilding dataframe by merging segment feature dfs with manually classified segments
#+BEGIN_SRC R
create.df.toBuildModel.fromTrainingPolygons.and.SegmentFeatureDFs <- function(manuallyClassifiedPolygondir,
                                                                              image.dir,
                                                                              segment.feature.df.appendage = segment.feature.df.name.append,
                                                                              modelBuildingData.name = "modelBuildingData.rds") {

    segment.feature.df.appendage = segment.feature.df.name.append

                                        # list shapefiles with manually classified polygons
    trainingShapefiles <- list.files(manuallyClassifiedPolygondir) %>%
        str_sub(.,end = nchar(.)-4) %>%
        unique()

                                        # load training data from shapefiles into memory
    shapelist.data <- lapply(trainingShapefiles, function(shp) {
        readOGR(dsn = manuallyClassifiedPolygondir, layer = shp)@data %>%
                                                                   na.omit() %>%
                                                                   rename(zone = DN) %>%
                                                                   filter(Class != "N")
    })

    names(shapelist.data) <- trainingShapefiles


                                        # list .rds segment feature dataframe files
    segmentFeatureDF.rds.files <- list.files(image.dir, full.names = T) %>%
        str_extract(pattern = str_c(".*",segment.feature.df.appendage,".rds")) %>%
        na.omit()

    trainingData <- list()

    foreach(j = seq_along(shapelist.data)) %do% {
        d <- readRDS(segmentFeatureDF.rds.files[j])
        trainingData[[j]] <- left_join(shapelist.data[[j]],d, by = c("zone" = "segment"))
    }

    trainingData <- bind_rows(trainingData) %>%
        filter(Class != "N")

    saveRDS(trainingData, file = str_c(image.dir, "/",modelBuildingData.name))

}

#+END_SRC
#+RESULTS:

*** Build and Save Models
#+BEGIN_SRC R
  Build.and.Save.models <- function(
              dir = image.cropped.to.training.dir,
              modelBuildingData = "modelBuildingData.rds",
              models.dir = Models.dir){

      dat <- readRDS(str_c(dir,"/",modelBuildingData)) %>%
          dplyr::select(-zone)


      names <- colnames(dat)
      names <- str_replace(names, "\\(",".")
      names <- str_replace(names, "\\)",".")
      names <- str_replace(names, "\\:",".")
      colnames(dat) <- names

            dat_G <- dat %>%
                mutate(Class = as.character(Class),
                       Class = ifelse(Class == "G", Class, "O"))

            dat_I <- dat %>%
                mutate(Class = as.character(Class),
                       Class = ifelse(Class == "I", Class, "O"))

            dat_T <- dat %>%
                mutate(Class = as.character(Class),
                       Class = ifelse(Class == "T", Class, "O"))

          # Create Tasks
      all.task <- makeClassifTask(id = paste0(image.name,"_all"), data = dat, target = "Class")
      grass.task <- makeClassifTask(id = paste0(image.name,"_grass"), data = dat_G, target = "Class")
      impervious.task <- makeClassifTask(id = paste0(image.name,"_impervious"), data = dat_I, target = "Class")
      tree.task <- makeClassifTask(id = paste0(image.name,"_tree"), data = dat_T, target = "Class",positive = "T")

      task.list <- list(all = all.task, grass = grass.task, impervious = impervious.task, tree = tree.task)

                                                 # Make Learners
         RF_prob <- makeLearner(id = "rf_prob","classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
         RF_response <- makeLearner(id = "rf_resp", "classif.randomForest", predict.type = "response", fix.factors.prediction = TRUE)
         SVM_response <- makeLearner(id = "svm_resp", "classif.svm", predict.type = "response", fix.factors.prediction = TRUE)

         learner_list <- list(RF_prob = RF_prob, RF_response = RF_response, SVM_response = SVM_response)

                                                 # Train Learners on Tasks, Make models
         cl<-makeCluster(cores)
         registerDoParallel(cl)
         models <- foreach(task = task.list, .packages = "mlr") %:%
             foreach(learner = learner_list) %dopar% {
                 train(learner, task)
             }
       saveRDS(models, file = paste0(models.dir,"/models.rds"))
  }
#+END_SRC

#+RESULTS:

*** Classify Raster
#+BEGIN_SRC R

classify.raster <- function(segment.feature.df.dir,
                                segment.dir,
                                model.dir,
                                model.name.rds = "models",
                                segment.feature.appendage = segment.feature.df.name.append,
                                segmentation.appendage = segment.tile.name.append,
                                classify.out.dir,
                                tile.name = i) {
        df <- readRDS(paste0(segment.feature.df.dir,"/",tile.name,segment.feature.appendage,".rds"))
        models <-readRDS(paste0(model.dir,"/",model.name.rds,".rds"))
        umod <- unlist(models, recursive = F)
        seg.path <- paste0(segment.dir,"/",tile.name,segment.tile.name.append,".tif")
        seg <- raster(seg.path)
#	dfRowsWithNA <- which(is.na(df[,2]))
	complete.df <- df[complete.cases(df),] # svm can't predict with NAs
        lapply(umod, function(mod) {
            pred <- predict(mod, newdata = complete.df)
            response <- factor(as.character(pred$data$response), levels = c("G","I","T","O"))
            m <- cbind(zone = complete.df$segment, response)
            m <- left_join(as.data.frame(df["segment"]), as.data.frame(m), by = c("segment" = "zone"))
            r <- reclassify(seg, m)
    #        x <- data.frame(ID = 1:4, LandCover = c("G","I","T","O")) %>%
    #            filter(LandCover %in% levels(factor(response)))
    #        levels(r) <- x
            if (ncol(pred$data) > 2) {
                prob <- (pred$data[,grep("prob.*", x = colnames(pred$data))]) # get columns that contain probabilities
                ProbOfClass <- apply(prob, MARGIN = 1, FUN = max)
                m <- cbind(segment = df$segment, ProbOfClass)
                m <- left_join(as.data.frame(df["segment"]), as.data.frame(m))
                p <- reclassify(seg, m)
                r <- stack(r,p)
            }
            path <- paste0(segment.dir,"/",tile.name,"_",mod$task.desc$id,"_",mod$learner$id,".tif")
            writeRaster(r, path, overwrite=TRUE)
            print(path)
        })
  }

#+END_SRC

#+RESULTS:

*** Calculate Percent Cover in Classified Tiles
#+BEGIN_SRC R
    get.prcnt.class <- function(points,r) {
          r <- crop(r,points)
          g <- cellStats(r == 1, stat = sum)
          im <- cellStats(r == 2, stat = sum)
          tr <- cellStats(r == 3, stat = sum)
          o <-  cellStats(r == 4, stat = sum)
          totC <- ncell(r)
          return(c(pct_g = g/totC, pct_i = im/totC, pct_t = tr/totC, pct_o = o/totC))
      }

  pct.cover.acc.img.classification <- calculate.prct.cover.in.classified.tiles(pts = grd.pts.subset.by.nrow.and.col,
                                                 img.dir = image.cropped.to.grid.accuracy.dir)

  calculate.prct.cover.in.classified.tiles <- function(pts,
                                                       img.dir = image.cropped.to.grid.accuracy.dir,
                                                       pattern.of.classified.tiles =  ".*mad-.*madison.*.tif",
                                                       unique.grid.id.pattern = "mad-[0-9]+m-[0-9]+",
                                                       grid.pattern = "[a-zA-Z]{3}-[0-9]+m-[0-9]+",
                                                       image.pattern = "[a-zA-Z]{5}[a-zA-Z]+",
                                                       target.pattern = "all|grass|impervious|tree",
                                                       model.pattern = "rf_prob|rf_resp|svm_resp"){

      # read in grids of points
      points <- pts

      image.paths <- list.files(str_c(img.dir), full.names = T) %>%
          str_extract(., pattern = pattern.of.classified.tiles) %>%
          na.omit()

      image.paths.short <- list.files(str_c(img.dir), full.names = F) %>%
          str_extract(., pattern = pattern.of.classified.tiles) %>%
          str_sub(.,1,-5) %>%
          na.omit()


      cl <- makeCluster(cores)
      registerDoParallel(cl)

  # apply the function "get.prcnt.class" to every classified tile, using the points that were from the
  # grid that created the tile at the start,

      out <- foreach (img.path = image.paths,
                      .combine = "rbind",
                      .packages = c("stringr","raster","rgeos"),
                      .export = "get.prcnt.class") %dopar% {
          id <- str_extract(img.path,grid.pattern)
          pts <- points[which(points@data$unq__ID == id),]
          img <- raster(img.path, proj4string = "+init:epsg=26916")
          get.prcnt.class(pts,img)
      }

  # take output, convert to data frame and all columns for grid, image name, target, and model
      out <- out %>%
          as.data.frame() %>%
          mutate(grid.img.target.model = image.paths.short,
                 grid = str_extract(grid.img.target.model, grid.pattern),
                 img =  str_extract(grid.img.target.model, image.pattern),
                 target = str_extract(grid.img.target.model, target.pattern),
                 model =  str_extract(grid.img.target.model, model.pattern))
      return(out)
  }

#+END_SRC

#+RESULTS:
:  Error: could not find function "calculate.prct.cover.in.classified.tiles"

** Libraries
#+BEGIN_SRC R
library(ascii)
library(rgeos)
library(mlr)
library(broom)
library(rgdal)
library(raster)
library(plyr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(foreach)
library(doParallel)
#+END_SRC

#+RESULTS:
#+begin_example
rgeos version: 0.3-11, (SVN revision 479)
 GEOS runtime version: 3.4.2-CAPI-1.8.2 r3921
 Linking to sp version: 1.1-1
 Polygon checking: TRUE
Loading required package: BBmisc

Attaching package: ‘BBmisc’

The following object is masked from ‘package:rgeos’:

    symdiff

Loading required package: ggplot2
Loading required package: ParamHelpers
Loading required package: sp
rgdal: version: 1.0-4, (SVN revision 548)
 Geospatial Data Abstraction Library extensions to R successfully loaded
 Loaded GDAL runtime: GDAL 1.10.0, released 2013/04/24
 Path to GDAL shared files: /usr/share/gdal/1.10
 Loaded PROJ.4 runtime: Rel. 4.8.0, 6 March 2012, [PJ_VERSION: 480]
 Path to PROJ.4 shared files: (autodetected)
 Linking to sp version: 1.1-1

Attaching package: ‘raster’

The following object is masked from ‘package:mlr’:

    resample

The following object is masked from ‘package:ParamHelpers’:

    getValues

Attaching package: ‘dplyr’

The following objects are masked from ‘package:plyr’:

    arrange, count, desc, failwith, id, mutate, rename, summarise,
    summarize

The following objects are masked from ‘package:raster’:

    intersect, select, union

The following object is masked from ‘package:BBmisc’:

    collapse

The following objects are masked from ‘package:rgeos’:

    intersect, setdiff, union

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

Attaching package: ‘tidyr’

The following object is masked from ‘package:raster’:

    extract

The following object is masked from ‘package:ascii’:

    expand
foreach: simple, scalable parallel programming from Revolution Analytics
Use Revolution R for scalability, fault tolerance and more.
http://www.revolutionanalytics.com
Loading required package: iterators
Loading required package: parallel
#+end_example

** Determine how to make best classifier for Madison : image, segmentation, model, n.classes, target, and def truth
*** Inputs
***** Input Directories
#+BEGIN_SRC R
 image.dirs <- c("../RD_NAIP","../RD_SPOT")
 pca.dir <- "../RD_PCA_Regions"
 training.dir <- "../RD_Training_Regions"
 accuracy.dir <- "../RD_Accuracy"
 grids.accuracy.dir <- str_c(accuracy.dir, "/Grids")
 fieldplots.accuracy.dir<- str_c(accuracy.dir, "/FieldData")
 crop.dir <- "../RD_CroplandDataLayer"
 water.dir <- "../RD_WI-waterbody-24k"
 urban.dir <- "../RD_US_UrbanAreasShapefile"
 urban.and.incorporated.dir <- "../RD_merged_WIurbanAreas_and_incorporatedAreas"
 #+END_SRC

 #+RESULTS:

***** Image Names and Paths
#+BEGIN_SRC R
   image.names <- c("madisonNAIP","geomatica_SPOT_panshp")

   image.paths <- paste0(image.dirs, "/", image.names, ".tif")
   ratio.tile.name.append <- "_with_ratios"
   pca.tile.name.append <- "_pca"
#   segment.tile.name.append <- "_N-30_C-15"

#   segment.feature.df.name.append <- "_SegmentFeatureDF"
 #+END_SRC

 #+RESULTS:

***** Input Shapefile DSNs and Layers
#+BEGIN_SRC R

 pca.region.dsn <- "../RD_PCA_Regions/Madison_PCA_Regions"
 pca.region.layer <- "PCA_regions"

 training.region.dsn <- "../RD_Training_Regions/Madison_TrainingRegions"
 training.region.layer <- "madisonTrainingPolygons"

 grid.accuracy.region.dsn <- "../RD_Accuracy/Grids"
 grid.accuracy.region.layer <- "All_Grids_Accuracy_Assessment_pts"

 field.accuracy.region.dsn <- "../RD_Accuracy/FieldData"
 field.accuracy.region.layer <- "PlotPointsShpFile"

# grid.accuracy.truthFromAndy.csvpath <- str_c(grid.accuracy.region.dsn,"grid_accuracy_assessment_andy.csv")

 #+END_SRC

 #+RESULTS:

***** Derived Directories
 #+BEGIN_SRC R
   # make derived data directory
   derived.dir <- "../DD"

   derived.image.dir <- paste0(derived.dir, "/", image.names)

   image.cropped.to.training.dir <- str_c(derived.dir, "/", image.names, "/Madison_Training")

   image.cropped.to.pca.dir <- str_c(derived.dir, "/", image.names, "/Madison_pca")

   image.cropped.to.accuracy.dir <- str_c(derived.dir, "/", image.names, "/Testing.Accuracy")

   image.cropped.to.grid.accuracy.dir <- str_c(image.cropped.to.accuracy.dir,"/Grid")

   image.cropped.to.field.accuracy.dir <- str_c(image.cropped.to.accuracy.dir,"/Field")

   Models.dir <- paste0(derived.image.dir,"/","Models")

#+END_SRC

 #+RESULTS:
 : Warning message:
 : In stri_c(..., sep = sep, collapse = collapse, ignore_null = TRUE) :
 :   longer object length is not a multiple of shorter object length
 : Warning message:
 : In stri_c(..., sep = sep, collapse = collapse, ignore_null = TRUE) :
 :   longer object length is not a multiple of shorter object length
 : Warning message:
 : In stri_c(..., sep = sep, collapse = collapse, ignore_null = TRUE) :
 :   longer object length is not a multiple of shorter object length

***** Make Derived Directories
#+BEGIN_SRC R

  dir.create(derived.dir)
  lapply(derived.image.dir,FUN=function(x) dir.create(x))
  lapply(image.cropped.to.training.dir, FUN = function(x) dir.create(x))
  lapply(image.cropped.to.pca.dir, FUN = function(x) dir.create(x))
  lapply(image.cropped.to.accuracy.dir, FUN = function(x) dir.create(x))
  lapply(image.cropped.to.grid.accuracy.dir, FUN = function(x) dir.create(x))
  lapply(image.cropped.to.field.accuracy.dir, FUN = function(x) dir.create(x))
  lapply(Models.dir, FUN = function(x) dir.create(x))
#+END_SRC

#+RESULTS:
#+begin_example
Warning message:
In dir.create(derived.dir) : '../DD' already exists
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) : '../DD/madisonNAIP' already exists
2: In dir.create(x) : '../DD/geomatica_SPOT_panshp' already exists
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) : '../DD/madisonNAIP/Madison_Training' already exists
2: In dir.create(x) :
  '../DD/geomatica_SPOT_panshp/Madison_Training' already exists
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) : '../DD/madisonNAIP/Madison_pca' already exists
2: In dir.create(x) :
  '../DD/geomatica_SPOT_panshp/Madison_pca' already exists
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) : '../DD/madisonNAIP/Testing.Accuracy' already exists
2: In dir.create(x) :
  '../DD/geomatica_SPOT_panshp/Testing.Accuracy' already exists
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) :
  '../DD/madisonNAIP/Testing.Accuracy/Grid' already exists
2: In dir.create(x) :
  '../DD/geomatica_SPOT_panshp/Testing.Accuracy/Grid' already exists
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) :
  '../DD/madisonNAIP/Testing.Accuracy/Field' already exists
2: In dir.create(x) :
  '../DD/geomatica_SPOT_panshp/Testing.Accuracy/Field' already exists
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) : '../DD/madisonNAIP/Models' already exists
2: In dir.create(x) : '../DD/geomatica_SPOT_panshp/Models' already exists
#+end_example

***** Derived Shapefile DSNs and Layers
#+BEGIN_SRC R
  training.region.imageCRS.dsn <- str_c(derived.image.dir,"/reprojected.Training_Regions")

  pca.region.imageCRS.dsn <- str_c(derived.image.dir,"/reprojected.PCA_Regions")

  grid.accuracy.region.imageCRS.dsn <- str_c(derived.image.dir,"/reprojected.Accuracy.Regions")


  lapply(training.region.imageCRS.dsn, FUN = function(x) dir.create(x))
  lapply(pca.region.imageCRS.dsn, FUN = function(x) dir.create(x))
  lapply(grid.accuracy.region.imageCRS.dsn, FUN = function(x) dir.create(x))
 #+END_SRC

#+RESULTS:
#+begin_example
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) :
  '../DD/madisonNAIP/reprojected.Training_Regions' already exists
2: In dir.create(x) :
  '../DD/geomatica_SPOT_panshp/reprojected.Training_Regions' already exists
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) :
  '../DD/madisonNAIP/reprojected.PCA_Regions' already exists
2: In dir.create(x) :
  '../DD/geomatica_SPOT_panshp/reprojected.PCA_Regions' already exists
[[1]]
[1] FALSE

[[2]]
[1] FALSE

Warning messages:
1: In dir.create(x) :
  '../DD/madisonNAIP/reprojected.Accuracy.Regions' already exists
2: In dir.create(x) :
  '../DD/geomatica_SPOT_panshp/reprojected.Accuracy.Regions' already exists
#+end_example


***** number of cores
#+BEGIN_SRC R
 cores <- 15
 #+END_SRC

 #+RESULTS:



*** Foreach image in Naip and Panspot:
**** make pca model to tranform images for slic segmentation algorithm.
***** read in pca model if it exists
 #+BEGIN_SRC R
   pca <- readRDS(str_c(image.cropped.to.pca.dir,"/pca.rds"))
 #+END_SRC
***** Reproject PCA Region Shapefile to Image
#+BEGIN_SRC R
     foreach(i = 1:2) %do% {
         Reproject_Shapefile_to_Image_CRS(pca.region.dsn,
                                         pca.region.layer,
                                         image.paths[i],
                                         pca.region.imageCRS.dsn[i])
     }
#+END_SRC

 #+RESULTS:
 #+begin_example
  OGR data source with driver: ESRI Shapefile
 Source: "../RD_PCA_Regions/Madison_PCA_Regions", layer: "PCA_regions"
 with 8 features
 It has 1 fields
 OGR data source with driver: ESRI Shapefile
 Source: "../RD_PCA_Regions/Madison_PCA_Regions", layer: "PCA_regions"
 with 8 features
 It has 1 fields
 [[1]]
 NULL

 [[2]]
 NULL
#+end_example

***** Crop image to create a smaller image around each of the polygons
 #+BEGIN_SRC R :results none
   foreach(i = 1:2) %do% {
       Crop_image_to_each_Shapefile_polygon(pca.region.imageCRS.dsn[i],
                                        pca.region.layer,
                                        image.paths[i],
                                        cores = cores,
                                        output.dir = image.cropped.to.pca.dir[i])
     }
 #+END_SRC

***** Add Ratios
#+BEGIN_SRC R
           cl <- makeCluster(cores)
           registerDoParallel(cl)

        ratios <- foreach (i = seq_along(image.names)) %do% {
            tile.names <- list.files(image.cropped.to.pca.dir[i]) %>%
                str_extract(., pattern = "[0-9]+.tif") %>%
                    str_extract(., pattern = "[0-9]+") %>%
                        na.omit()

           foreach (j = tile.names,
                    .packages = c("raster","stringr")) %dopar% {
                        add.ratios.ndvi(tile.dir = image.cropped.to.pca.dir[i],
                                        tile.name = j)
                    }

        }

        stopCluster(cl)
 #+END_SRC

#+RESULTS:

***** Create and Save PCA model/rotation
#+BEGIN_SRC R :results none
   pca <- foreach(i = seq_along(image.names)) %do% {
               make.and.save.pca.transformation(image.dir = image.cropped.to.pca.dir[i],
                                                       band.names = c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")
                                                       )
   }
 #+END_SRC

**** Make PixelFeatureDFs and SegmentationFeatureDFs for Training Regions
 1) Input
    - Testing Region Shapefiles
    - image
 2) Operation
    - Reproject Shapefiles to that of image
    - Crop image to each polygon in the shapefile
    - Derive PixelfeatureDFs and SegmentationFeatureDF from each tile of the image in region of each polygon
 3) Output
    - SegmentationFeatureDFs for every training polygon
    - PixelFeatureDFs for every pixel

***** Reproject Training Region Shapefile to Image
 #+BEGIN_SRC R
   foreach(i = seq_along(image.names)) %do% {
         Reproject_Shapefile_to_Image_CRS(training.region.dsn,
                                          training.region.layer,
                                          image.paths[i],
                                          training.region.imageCRS.dsn[i])
     }

 #+END_SRC

 #+RESULTS:
 #+begin_example
  OGR data source with driver: ESRI Shapefile
 Source: "../RD_Training_Regions/Madison_TrainingRegions", layer: "madisonTrainingPolygons"
 with 15 features
 It has 1 fields
 OGR data source with driver: ESRI Shapefile
 Source: "../RD_Training_Regions/Madison_TrainingRegions", layer: "madisonTrainingPolygons"
 with 15 features
 It has 1 fields
 [[1]]
 NULL

 [[2]]
 NULL
#+end_example

***** Crop image to create a smaller image around each of the testing polygons
#+BEGIN_SRC R :results none
foreach(i = seq_along(image.names)) %do% {
   Crop_image_to_each_Shapefile_polygon(training.region.imageCRS.dsn[i],
					training.region.layer,
					image.paths[i],
					cores = cores,
					output.dir = image.cropped.to.training.dir[i])
}
 #+END_SRC

***** Create Pixel and Segment Feature Dataframe for each of these smaller images
****** Start R Loop, for every smaller image, do in parallel, :
#+BEGIN_SRC R

   cl <- makeCluster(cores)
   registerDoParallel(cl)

pixel.added.features.raster.list <- foreach(j = seq_along(image.names)) %do% {

   tile.names <- list.files(image.cropped.to.training.dir[j]) %>%
       str_extract(., pattern = "[0-9]+.tif") %>%
       str_extract(., pattern = "[0-9]+") %>%
       na.omit()

   foreach (i = tile.names,
            .packages = c("raster","stringr")) %dopar% {
 #+END_SRC

 #+RESULTS:

****** Add Ratios
#+BEGIN_SRC R
   add.ratios.ndvi(tile.dir = image.cropped.to.training.dir[j],
                   tile.name = i)
}
}
 #+END_SRC

 #+RESULTS:
****** Save Pixel Feature Dataframe
#+BEGIN_SRC R

    pixel.feature.dfs <- foreach(i = seq_along(image.names)) %do% {
        pixel.feature.df <- Create.Pixel.Feature.df(pixel.added.features.raster.list[[i]])
    }

  foreach(i = seq_along(image.names)) %do% {
      saveRDS(pixel.feature.dfs[[i]], file = str_c(image.cropped.to.training.dir[i],"/","PixelFeatureDF",".rds"))
  }

#+END_SRC
****** Perform PCA
#+BEGIN_SRC R

     cl <- makeCluster(cores)
     registerDoParallel(cl)



  foreach(j = seq_along(image.names)) %do% {

      tile.names <- list.files(image.cropped.to.training.dir[j]) %>%
         str_extract(., pattern = "[0-9]+.tif") %>%
         str_extract(., pattern = "[0-9]+") %>%
         na.omit()

      foreach(i = tile.names, .packages = c("stringr","raster")) %dopar%
          image.pca(image.dir = image.cropped.to.training.dir[j],
               tile.name = i,
               pca.model = pca[[j]])
  }
 #+END_SRC

#+RESULTS:
#+begin_example
[[1]]
class       : RasterBrick
dimensions  : 198, 462, 91476, 3  (nrow, ncol, ncell, nlayers)
resolution  : 1, 1  (x, y)
extent      : 297406, 297868, 4767289, 4767487  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=utm +zone=16 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs
data source : /data2/erker-data2/Pjt_UTC/DD/madisonNAIP/Madison_Training/2_pca.tif
names       : X2_pca.1, X2_pca.2, X2_pca.3
min values  :        0,      104,       65
max values  :      105,      255,      130


[[2]]
class       : RasterBrick
dimensions  : 131, 308, 40348, 3  (nrow, ncol, ncell, nlayers)
resolution  : 1.5, 1.5  (x, y)
extent      : 297405, 297867, 4767290, 4767486  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=utm +zone=16 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0
data source : /data2/erker-data2/Pjt_UTC/DD/geomatica_SPOT_panshp/Madison_Training/2_pca.tif
names       : X2_pca.1, X2_pca.2, X2_pca.3
min values  :        0,       27,       74
max values  :      142,      255,      140


There were 15 warnings (use warnings() to see them)
#+end_example

****** Segmentation

#+NAME: training.dir.NAIP
#+BEGIN_SRC R
message(image.cropped.to.training.dir[1])
#+END_SRC

#+RESULTS: training.dir.NAIP
: ../DD/madisonNAIP/Madison_Training


#+BEGIN_SRC sh :var dir=training.dir.NAIP
   cd $dir
   # pixel size
   # desired area for superpixel/segments
   # compactness value
   # directory
   python ../../../code/fia_segment_cmdArgs.py 1 30 15
   python ../../../code/fia_segment_cmdArgs.py 1 60 30
   python ../../../code/fia_segment_cmdArgs.py 1 105 32
#+END_SRC

#+RESULTS:
| average     | number    | of        | pixels | per                                                        | segment | is | 30 |
| compactness | parameter | is        | 15     |                                                            |         |    |    |
| the         | image     | directory | is     | /data2/erker-data2/Pjt_UTC/DD/madisonNAIP/Madison_Training |         |    |    |
| average     | number    | of        | pixels | per                                                        | segment | is | 60 |
| compactness | parameter | is        | 30     |                                                            |         |    |    |
| the         | image     | directory | is     | /data2/erker-data2/Pjt_UTC/DD/madisonNAIP/Madison_Training |         |    |    |

#+NAME: training.dir.panSPOT
#+BEGIN_SRC R
message(image.cropped.to.training.dir[2])
#+END_SRC

#+RESULTS: training.dir.panSPOT
: ../DD/geomatica_SPOT_panshp/Madison_Training

#+BEGIN_SRC sh :var dir=training.dir.panSPOT
   cd $dir
   # pixel size
   # desired area for superpixel/segments
   # compactness value
   # directory
   python ../../../code/fia_segment_cmdArgs.py 1.5 30 15
   python ../../../code/fia_segment_cmdArgs.py 1.5 60 30
   python ../../../code/fia_segment_cmdArgs.py 1.5 105 32
#+END_SRC

#+RESULTS:
| average     | number    | of        | pixels | per                                                                  | segment | is | 20.0 |
| compactness | parameter | is        | 15     |                                                                      |         |    |      |
| the         | image     | directory | is     | /data2/erker-data2/Pjt_UTC/DD/geomatica_SPOT_panshp/Madison_Training |         |    |      |
| average     | number    | of        | pixels | per                                                                  | segment | is | 40.0 |
| compactness | parameter | is        | 30     |                                                                      |         |    |      |
| the         | image     | directory | is     | /data2/erker-data2/Pjt_UTC/DD/geomatica_SPOT_panshp/Madison_Training |         |    |      |
| average     | number    | of        | pixels | per                                                                  | segment | is | 20.0 |
| compactness | parameter | is        | 32     |                                                                      |         |    |      |
| the         | image     | directory | is     | /data2/erker-data2/Pjt_UTC/DD/geomatica_SPOT_panshp/Madison_Training |         |    |      |



****** Create Segment Feature Dataframe
 #+BEGIN_SRC R
   image.names <- list.files(image.cropped.to.training.dir) %>%
       str_extract(., pattern = "[0-9]+_N-[0-9]+_C-[0-9]+.tif") %>%
       str_extract(., pattern = "[0-9]+") %>%
       na.omit()

   cl <- makeCluster(cores)
   registerDoParallel(cl)

   foreach (i = image.names,
            .packages = c("raster","stringr","dplyr","broom","tidyr")) %dopar% {
		Create.Segment.Feature.df(image.dir = image.cropped.to.training.dir,
                                          image.name = i)}
 #+END_SRC

 #+RESULTS:
 #+begin_example
 [[1]]
 NULL

 [[2]]
 NULL

 [[3]]
 NULL

 [[4]]
 NULL

 [[5]]
 NULL

 [[6]]
 NULL

 [[7]]
 NULL

 [[8]]
 NULL

 [[9]]
 NULL

 [[10]]
 NULL

 [[11]]
 NULL

 [[12]]
 NULL

 [[13]]
 NULL

 [[14]]
 NULL

 [[15]]
 NULL

 [[16]]
 NULL

 [[17]]
 NULL

 [[18]]
 NULL

 [[19]]
 NULL

 [[20]]
 NULL

 [[21]]
 NULL

 [[22]]
 NULL

 [[23]]
 NULL

 [[24]]
 NULL

 [[25]]
 NULL

 [[26]]
 NULL

 [[27]]
 NULL

 [[28]]
 NULL

 [[29]]
 NULL

 [[30]]
 NULL

 [[31]]
 NULL
 #+end_example

**** Supervised Classification/ manual classification of segmentation polygons
 1) Input
    - Segmentation Layer from the Training Regions
 2) Operation
    - Convert Segmentation Layer to Polygons
    - Classify this manually in QGIS and save as Classified Polygons
 3) Output
    - Classified segmentation polygons

***** Convert Segmentation Layer to Polygons
 #+BEGIN_SRC R :results none
     image.names <- list.files(image.cropped.to.training.dir) %>%
         str_extract(., pattern = "[0-9]+_N-[0-9]+_C-[0-9]+.tif") %>%
         str_extract(., pattern = "[0-9]+") %>%
         na.omit()

     cl <- makeCluster(cores)
     registerDoParallel(cl)


     foreach (i = image.names, .packages = c("raster","sp","gdalUtils")) %dopar% {
         polygonize.and.add.Class(image.dir = image.cropped.to.training.dir,
                                  image.name = i)
   }

 #+END_SRC

***** Manually Classify Polygons
I should change this so that polygons that I drew are read in and
pixels or segments that fall within the polygons are assigned that
polygon's class.  This will standardize more the training sets for the
pixelwise and superpixel/segmentwise comparison.


 If this gets evaluated it will copy over the manually classified
 training polygons.  Don't evaluate unless you want to erase the
 training polygons within.

 #+BEGIN_SRC sh :eval no
 #cp ../DD/madison/training/*/*.shp ../DD/madison/ManuallyClassifiedTrainingPolygons
 #cp ../DD/madison/training/*/*.dbf ../DD/madison/ManuallyClassifiedTrainingPolygons
 #cp ../DD/madison/training/*/*.shx ../DD/madison/ManuallyClassifiedTrainingPolygons
 #+END_SRC

 #+RESULTS:

**** Create Models
 1) Input
    - SegmentationFeaturesDF
    - Manually/Supervised classified segmentation polygons from
      training regions

 2) Operation
    - Merge Training polygons Class with Segmentationfeatures
    - Build Models using mlr
      - untuned
      - tuned

 3) Output
    - Models for classifying images
      - RF or SVM (2 options)
	- All 3 classes in one model, or just one class in a model (4 options)
	  - Highly tuned or default parameters (2 options)
    - 2 * 4 * 2 = 16
***** Merge Training Polygons with Segment Feature dataframe
 #+BEGIN_SRC R
   create.df.toBuildModel.fromTrainingPolygons.and.SegmentFeatureDFs(manuallyClassifiedPolygondir = ManuallyClassifiedTrainingPolygons.dir,
                                                                     image.dir = image.cropped.to.training.dir)
 #+END_SRC

***** Build and Save models

 #+BEGIN_SRC R

   Build.and.Save.models(dir = image.cropped.to.training.dir,
                         modelBuildingData = "modelBuildingData.rds",
                         models.dir = Models.dir)



 #+END_SRC

 #+RESULTS:


 #+BEGIN_SRC R
     rdesc <- makeResampleDesc("CV", iters = 3)

     r <- resample(learner = SVM_response, task = all.task, resampling = rdesc)

     rf <- models[[1]][[1]]

     p <- predict(rf, task = all.task)


 #+END_SRC

****** some graphic explorations
 #+BEGIN_SRC R :eval no
                 dir = image.cropped.to.training.dir
		modelBuildingData = "modelBuildingData.rds"


         dat <- readRDS(str_c(dir,"/",modelBuildingData)) %>%
             dplyr::select(-zone)


       library(GGally)

     #  dat2 <- rename(dat, "xy.inter" = `x:y`)
     names <- colnames(dat)
     names <- str_replace(names, "\\(",".")
     names <- str_replace(names, "\\)",".")
     names <- str_replace(names, "\\:",".")

   colnames(dat) <- names

 dat <- mutate(dat, Class = as.factor(Class))

 #+END_SRC

 #+RESULTS:
 [[file:figs/pairs.png]]

 #+BEGIN_SRC R :results graphics :file figs/pairs.png :eval yes :width 3000 :height 3000
 #  ?ggpairs
         ggpairs(dat, mapping = ggplot2::aes(color = Class), axisLabels = "show")
 #+END_SRC

 #+RESULTS:
 [[file:figs/pairs.png]]

 #+BEGIN_SRC R :results graphics :file figs/byGroup.png :height 2000 :width 200 :eval no
   dat.g <- gather(dat, key = band, value = value, -Class)
   ggplot(dat.g, aes(x = Class, y = value)) + geom_boxplot() + facet_grid(band~1, scales = "free")
 #+END_SRC

  #+BEGIN_SRC R

  #+END_SRC


**** Classify Testing Regions
***** Grids
 1) Input
    - Classification Models
    - Testing Region Shapefiles
    - image
 2) Operation
    - Reproject Shapefiles to that of image
    - Crop image to each polygon in the shapefile
    - For each possible model Classify each tile of the image in region of each polygon
 3) Output
    - classified images for each model for each grid polygon
    - see directory structure, "Testing" header

****** Reproject Grid Accuracy Testing Region Shapefile to Image
 #+BEGIN_SRC R
   Reproject_Shapefile_to_Image_CRS(grid.accuracy.region.dsn,
                                    grid.accuracy.region.layer,
                                    image.path,
				    grid.accuracy.region.imageCRS.dsn)
 #+END_SRC

 #+RESULTS:
 :  OGR data source with driver: ESRI Shapefile
 : Source: "../RD_Accuracy/Grids", layer: "All_Grids_Accuracy_Assessment_pts"
 : with 18365 features
 : It has 10 fields

****** Crop image to create a smaller image around each of the accuracy polygons

 #+BEGIN_SRC R :results none
 Crop_image_to_regions_around_points_nameBygrid(grid.accuracy.region.imageCRS.dsn,
					grid.accuracy.region.layer,
					image.path,
					cores = cores,
					output.dir = image.cropped.to.grid.accuracy.dir)
 #+END_SRC

****** Classify images
******* Start R Loop, for every smaller image, do in parallel, :
 #+BEGIN_SRC R
   image.names <- list.files(image.cropped.to.grid.accuracy.dir) %>%
       str_extract(., pattern = ".*.tif") %>%
       str_sub(.,1,-5) %>%
       na.omit()

   cl <- makeCluster(cores)
   registerDoParallel(cl)

   foreach (i = image.names,
            .packages = c("raster","stringr")) %dopar% {
 #+END_SRC

 #+RESULTS:

******* Add Ratios
 #+BEGIN_SRC R
   add.ratios.ndvi(tile.dir = image.cropped.to.grid.accuracy.dir,
                   tile.name = i)
 #+END_SRC

 #+RESULTS:

******* Perform PCA
 #+BEGIN_SRC R

   image.pca(image.dir = image.cropped.to.grid.accuracy.dir,
             image.name = i)

 #+END_SRC
******* End loop
 #+BEGIN_SRC R :results none
 }

 #+END_SRC

******* Segmentation

 #+NAME: grid.accuracy.dir
 #+BEGIN_SRC R
 message(image.cropped.to.grid.accuracy.dir)
 #+END_SRC

 #+RESULTS: grid.accuracy.dir
 : ../DD/madison/Accuracy/Grids


 #+BEGIN_SRC sh :var dir=grid.accuracy.dir
   cd $dir
 pwd
 # pixel size
   # desired area for superpixel/segments
   # compactness value
   # directory
   python ../../../../code/fia_segment_cmdArgs.py 1 30 15
 #+END_SRC

 #+RESULTS:
 | /home/erker/mydata2/Pjt_UTC/DD/madison/Accuracy/Grids |           |           |        |                                                      |         |    |    |
 | average                                               | number    | of        | pixels | per                                                  | segment | is | 30 |
 | compactness                                           | parameter | is        | 15     |                                                      |         |    |    |
 | the                                                   | image     | directory | is     | /data2/erker-data2/Pjt_UTC/DD/madison/Accuracy/Grids |         |    |    |

******* Start R Loop, for every smaller image, do in parallel, :
 #+BEGIN_SRC R
   cl <- makeCluster(cores)
   registerDoParallel(cl)

   foreach (i = image.names,
            .packages = c("raster","stringr","dplyr","broom","tidyr","foreach","mlr")) %dopar% {
 #+END_SRC

 #+RESULTS:

******* Create Segment Feature Dataframe
 #+BEGIN_SRC R :results none
		Create.Segment.Feature.df(image.dir = image.cropped.to.grid.accuracy.dir,
                                          image.name = i)
 #+END_SRC

******* Predict Class of each Segment and create classified images
 #+BEGIN_SRC R :results none

                      classify.raster(segment.feature.df.dir = image.cropped.to.grid.accuracy.dir,
                         model.dir = Models.dir,
                         segment.dir = image.cropped.to.grid.accuracy.dir,
                         classify.out.dir = image.cropped.to.grid.accuracy.dir,
                         tile.name = i,
                         segmentation.appendage = segment.tile.name.append,
                         model.name.rds = "models",
                         segment.feature.appendage = segment.feature.df.name.append)

                  }

   stopCluster(cl)

 #+END_SRC

***** Fieldplot
**** Assess Accuracy
***** Grid
 1) Input:
    - classified images for each model for each grid polygon
    - Grid Shapefile

 2) Operation
    - For the 4 different grid sizes (50x50, 100x100, 150x150, 200x200)
    - Depending on the Model Target (all three cover types or a single
      cover type), calculate the proportion of cover in the classified image.
    - Calculate Proportion of cover within the grid shapefile
    - Combine shapefile information with classified image information
      and create RMSEs and plots.

 3) Output

    - Table of Accuracy by model and grid size
    - RMSE plots


****** Calculate the cover of each grid, according to andy's assessment of google earth

 The grids, that are 200mx200m will have 8 sizes, smaller grids will
 have fewer.

 Grid sizes : 25x25, 50x50,75x75,100x100,125x125,150x150,175x175,200x200


 | Grid Number | Grid Size | % T | % G | % I |
 |-------------+-----------+-----+-----+-----|
 | 1           |           |     |     |     |
 | 1           |           |     |     |     |
 |             |           |     |     |     |
 #+BEGIN_SRC R
   grd <- readOGR(dsn = grid.accuracy.region.dsn, layer = grid.accuracy.region.layer)
   grd <- spTransform(grd, CRS("+init=epsg:32616"))
   grd.df <- grd@data



   filter.by.row.and.col <- function(df,nrow.and.col) {
       nrow <-df %>%
           group_by(unq__ID) %>%
           summarize(nrow = max(row))

       df <- left_join(df,nrow)

       df %>%
           filter(nrow >= nrow.and.col,   # remove grids that have fewer than the number of rows & columns
                  row <= nrow.and.col,    # remove rows greater than the number we are interested in
                  column <=nrow.and.col)  # same for columns as rows
   }

   n.rows.and.columns.for.subset <- 15

   out <- filter.by.row.and.col(grd.df, n.rows.and.columns.for.subset)


   add.n.pts.per.grid <- function(df){
       n.pts<-df %>%
           group_by(unq__ID) %>%
           summarize(n.points = n())

       left_join(df,n.pts)
   }


   grd.pts.subset.by.nrow.and.col <- add.n.pts.per.grid(out)


   get.pct.cvr.typ <- function(df) {
       df %>%
           group_by(unq__ID, cvr_typ,n.points) %>%
           summarize(number = n()) %>%
           ungroup() %>%
           mutate(google.truth.pct.cover = number/n.points) %>%
           dplyr::select(-n.points, -number)
   }

   true.pct.cover.15x15grd <- get.pct.cvr.typ(grd.pts.subset.by.nrow.and.col)

   # eventually I'll want to loop through a number of sizes.  For now I'll do 15 rows and 15 columns.
   #a <- lapply(c(5,10,15,20,25,29), function(x) filter.by.row.and.col(grd.df, x))
 #+END_SRC

 #+RESULTS:
 #+begin_example
 OGR data source with driver: ESRI Shapefile
 Source: "../RD_Accuracy/Grids", layer: "All_Grids_Accuracy_Assessment_pts"
 with 18365 features
 It has 10 fields
 Warning: closing unused connection 32 (<-localhost:11289)
 Warning: closing unused connection 31 (<-localhost:11289)
 Warning: closing unused connection 30 (<-localhost:11289)
 Warning: closing unused connection 29 (<-localhost:11289)
 Warning: closing unused connection 28 (<-localhost:11289)
 Warning: closing unused connection 27 (<-localhost:11289)
 Warning: closing unused connection 26 (<-localhost:11289)
 Warning: closing unused connection 25 (<-localhost:11289)
 Warning: closing unused connection 24 (<-localhost:11289)
 Warning: closing unused connection 23 (<-localhost:11289)
 Warning: closing unused connection 22 (<-localhost:11289)
 Warning: closing unused connection 21 (<-localhost:11289)
 Warning: closing unused connection 20 (<-localhost:11289)
 Warning: closing unused connection 19 (<-localhost:11289)
 Warning: closing unused connection 18 (<-localhost:11289)
 Joining by: "unq__ID"
 Joining by: "unq__ID"
 #+end_example

****** Calculate the cover of each classified tile over grid

 for every grid

 for every target (all, tree, grass, impervious)

 for every model  (rf prob, rf response, svm response)

 calculate the percent cover



 read in grid points shapefile

 filter out the madison ones

 filter out points according to size testing

 read in classfied tile

 extract classified pixels at each point in the grid

 calculate % cover of all pixels in grid



 | Grid_unique_ID | whole grid based classification |   |   |
 |----------------+---------------------------------+---+---|
 |                |                                 |   |   |



 #+BEGIN_SRC R
   xy <- dplyr::select(grd.pts.subset.by.nrow.and.col, x, y)
   coordinates(grd.pts.subset.by.nrow.and.col) <- xy
   proj4string(grd.pts.subset.by.nrow.and.col) <- CRS("+init=epsg:3071") # it's WTM for some reason
   grd.pts.subset.by.nrow.and.col <- spTransform(grd.pts.subset.by.nrow.and.col,CRS("+init=epsg:26916"))

   pct.cover.acc.img.classification <- calculate.prct.cover.in.classified.tiles(pts = grd.pts.subset.by.nrow.and.col,
						img.dir = image.cropped.to.grid.accuracy.dir)
 #+END_SRC

 #+RESULTS:
 #+begin_example
 Error in UseMethod("select_") :
   no applicable method for 'select_' applied to an object of class "c('SpatialPointsDataFrame', 'SpatialPoints', 'Spatial', 'SpatialPointsNULL', 'SpatialVector')"
 Error in `coordinates<-`(`*tmp*`, value = list(x = c(573757.451807, 573764.523236,  :
   setting coordinates cannot be done on Spatial objects, where they have already been set
 Warning in `proj4string<-`(`*tmp*`, value = <S4 object of class "CRS">) :
   A new CRS was assigned to an object with an existing CRS:
 +init=epsg:32616 +proj=utm +zone=16 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0
 without reprojecting.
 For reprojection, use function spTransform in package rgdal
  Error in { (from #29) : task 1 failed - "extents do not overlap"
 #+end_example


****** Combine Classified Results with Google "Truth" Results

 #+BEGIN_SRC R
   pct.cover.acc.img.classification %>%
       head()


   pct.cover.acc.img.classification <- pct.cover.acc.img.classification %>%
       gather(key = Cover, value = ClassifiedImagePercentCover, -grid.img.target.model, -grid, -img, -target, -model) %>%
       filter(target == "all" & Cover == "pct_g" |
              target == "all" & Cover == "pct_i" |
              target == "all" & Cover == "pct_t" |
              target == "grass" & Cover == "pct_g" |
              target == "impervious" & Cover == "pct_i" |
              target == "tree" & Cover == "pct_t") %>%
       mutate(cvr_typ = str_sub(Cover, 5,5)) %>%
       arrange(img, grid, model,target)


   d <- d %>%
       rename(grid = unq__ID,
              google.truth.pct.cover = pct.cover)


   b<-left_join(pct.cover.acc.img.classification, d)

		by = "grid")

   head(b, n = 200)





 #+END_SRC


 #+BEGIN_SRC R
   dat <-left_join(pct.cover.acc.img.classification,d.t, by = c("grid" = 'unq__ID')) %>%
       filter(target == "all", cvr_typ =="t")
 #+END_SRC

 #+RESULTS:
 :  Warning in left_join_impl(x, y, by$x, by$y) :
 :   joining factor and character vector, coercing into character vector

 #+BEGIN_SRC R :results graphics :file figs/acc.png
   ggplot(dat, aes(x = pct.cover, y = pct_t, color = model, size = size.meters)) + geom_point()
 #+END_SRC

 #+RESULTS:
 [[file:figs/acc.png]]

 #+BEGIN_SRC R :results graphics :file figs/acc.png
   ggplot(filter(dat,size.meters == 50), aes(x = pct.cover, y = pct_t, color = model, size = size.meters)) + geom_point() +
 facet_wrap(~grid)
 #+END_SRC

 #+RESULTS:
 [[file:figs/acc.png]]


 #+BEGIN_SRC R :results graphics :file figs/test.png
 ggplot(filter(grid2_accuracy,grid_type == "200m"), aes(x = row, y = column, color = cover_type)) + geom_point() +
 facet_wrap(~CID)

 #+END_SRC

 #+RESULTS:
 [[file:figs/test.png]]


 #+BEGIN_SRC R
   library(rCharts)

   r1 <- rPlot(pct_t ~ pct.cover, data = d, color = "model")
   r1

 #  r1$publish('Scatterplot3', host = 'gist')
   print(r1)
 #+END_SRC

 #+RESULTS:

***** Fieldplot

 Same as with Grid, but adjust the definitions of "tree" in the field
 data and see how accuracy varies.

***** Combine NAIP Accuracy Assessments



*** SPOT
 Change image specfic inputs like image.path, and rerun code for NAIP
 #+BEGIN_SRC R
 spot.image.dir <- "../RD_SPOT"
 spot.image.path <- str_c(spot.image.dir, "/", spot.image.name, ".tif")
 spot.image.name <- "geomatica_SPOT_panshp"
 #+END_SRC


*** Summarize Accuracy Assessment Results

** Test How Madison Model performs for Wausau
** Classify Every Urban Area in the State











