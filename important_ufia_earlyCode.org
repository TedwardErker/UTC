* Workflow before [2016-02-12 Fri]
** Prepwork
*** Load Libraries
#+BEGIN_SRC R
  library(gdalUtils)
  library(stringr)
  library(rgdal)
  library(raster)
  library(rgeos)
  library(plyr)
  library(dplyr)
  library(doParallel)
  library(parallel)
  library(mlr)
#+END_SRC

*** Set image name and directory and other variables
#+BEGIN_SRC R
      cores <- detectCores()
#      cores <- 11

  training.testing.dir <- "../DD_TrainingAndTesting/"
  dir.create(training.testing.dir)
  image.training.testing.dir <- paste0(training.testing.dir,image.name)
  croppedRaster.image.training.testing.dir <- paste0(training.testing.dir,image.name,"/croppedRasters")

          ##################
          #################                Specify image name and directory
          ##################

          image.name <- "madison_wausau_mounthoreb_NAIP"
          image.name <- "madison"
          image.rd.directory <- "../RD_NAIP-imagery/"
          image.dd.directory <- "../DD_NAIP-imagery/"

      dir.create(image.dd.directory)
      image.dir.path <- paste0(image.dd.directory, image.name)
      dir.create(image.dir.path)

      ##################
      #################                Specify WI Urban Area Shapefile name and directory (dsn)
      ##################

    WI_UrbanAreas.dsn <- "../DD_merged_WIurbanAreas_and_incorporatedAreas"
    WI_UrbanAreas.layer <- "Dissolve_Merge_WI_census_inc"


          ##################
          #################                Specify water shapefile name and directory (dsn)
          ##################

          water.name <- "WD-Hydro-Waterbody-WBIC-AR-24K"
          water.dsn <- "../RD_WI-waterbody-24k"


          ##################
          #################                Specify Cropland Datalayer name and directory
          ##################

          crop.directory <- "../RD_CroplandDataLayer/"
          crop2010.name <- "CDL_2010_clip_20160128162252_788770535"
          crop2011.name <- "CDL_2011_clip_20160106190244_1504737741"
          crop2012.name <- "CDL_2012_clip_20151229124713_1037776543"
          crop2013.name <- "CDL_2013_clip_20151229123327_86558742"
          crop2014.name <- "CDL_2014_clip_20151229123327_86558742"

          n_croplandLayers <- 5

      #  Specify the number of tiles to polygonize
      n_TilesToSample <- 20



          ##################
          #################                Model Building Directories
          ##################

  model.building.path <- "../DD_ModelBuilding"
  dir.create(model.building.path)


          ##################
          #################                Training Data inputs
          ##################

    # Specify radius around field plot centers
    rad <- 200 # meters




#+END_SRC

*** Load Functions
#+BEGIN_SRC R
        namedList <- function(...) {
          L <- list(...)
          snm <- sapply(substitute(list(...)),deparse)[-1]
          if (is.null(nm <- names(L))) nm <- snm
          if (any(nonames <- nm=="")) nm[nonames] <- snm[nonames]
          setNames(L,nm)
        }


      ndvi_nodrop <- function(image_w4bands,red_bandnumber,nir_bandnumber,...) {
        red_band <- image_w4bands[[red_bandnumber]]
        nir_band <- image_w4bands[[nir_bandnumber]]
        ndvi <- (nir_band - red_band)/(nir_band + red_band)
        return(ndvi)
      }

        savi_nodrop <- function(image_w4bands,red_bandnumber,nir_bandnumber,L,...) {
          red_band <- image_w4bands[,,red_bandnumber,drop=FALSE]
          nir_band <- image_w4bands[,,nir_bandnumber,drop=FALSE]
          savi <- (nir_band - red_band)/(nir_band + red_band + L) * (1+L)
          return(savi)
        }

        ratio <- function(image_w4bands, numerator_bandNumber) {
          r <- image_w4bands[,,numerator_bandNumber,drop = F] / sum(image_w4bands)
          return(r)
        }

        create_GLCM_layers_parallel <- function(list_rasterlayers, vec_window_sizes, dir, cpus) {
          cl <- makeCluster(spec = cpus, methods = FALSE)
          # Register the cluster with foreach:
          registerDoParallel(cl)
          GLCM_rasters <- foreach(i = 1:length(list_rasterlayers), .packages = c('glcm','raster')) %:%
            foreach (j = 1:length(window_sizes), .packages = c('glcm','raster')) %dopar% {
              raster <- list_rasterlayers[[i]]
              dir <- dir
              window_size <- vec_window_sizes[j]
              w_s <- c(window_size,window_size)
              a <- glcm(raster,shift = dir, window = w_s,na_opt = "center", na_val = 0, asinteger = T)
              names(a)<- paste0(names(list_rasterlayers[[i]]),"_",vec_window_sizes[j],"x",vec_window_sizes[j],"_",names(a))
              a
            }
          stopCluster(cl) # Stops the cluster
          registerDoSEQ()
          return(unlist(GLCM_rasters))
        }

        # Function takes raster stack, samples data, performs pca and returns stack of first n_pcomp bands
          predict_pca_wSampling_parallel <- function(stack, sampleNumber, n_pcomp, nCores = detectCores()-1) {
              sr <- sampleRandom(stack,sampleNumber)
              pca <- prcomp(sr, scale=T)
              beginCluster()
              r <- clusterR(stack, predict, args = list(pca, index = 1:n_pcomp))
              endCluster()
              return(r)
          }

    predict_pca <- function(raster.path, pca, n.comps, out.path) {
        s <- stack(raster.path)
        names(s) <- c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")
        predict(s, pca, index = 1:n.comps, filename = out.path, overwrite=TRUE)
    }




      gdal_polygonizeR <- function(x, outshape=NULL, gdalformat = 'ESRI Shapefile',
                                   pypath=NULL, readpoly=TRUE, quiet=TRUE) {
        if (isTRUE(readpoly)) require(rgdal)
        if (is.null(pypath)) {
          pypath <- Sys.which('gdal_polygonize.py')
        }
        if (!file.exists(pypath)) stop("Can't find gdal_polygonize.py on your system.")
        owd <- getwd()
        on.exit(setwd(owd))
        setwd(dirname(pypath))
        if (!is.null(outshape)) {
          outshape <- sub('\\.shp$', '', outshape)
          f.exists <- file.exists(paste(outshape, c('shp', 'shx', 'dbf'), sep='.'))
          if (any(f.exists))
            stop(sprintf('File already exists: %s',
                         toString(paste(outshape, c('shp', 'shx', 'dbf'),
                                        sep='.')[f.exists])), call.=FALSE)
        } else outshape <- tempfile()
        if (is(x, 'Raster')) {
          require(raster)
          writeRaster(x, {f <- tempfile(fileext='.asc')})
          rastpath <- normalizePath(f)
        } else if (is.character(x)) {
          rastpath <- normalizePath(x)
        } else stop('x must be a file path (character string), or a Raster object.')
        system2('python', args=(sprintf('"%1$s" "%2$s" -f "%3$s" "%4$s.shp"',
                                        pypath, rastpath, gdalformat, outshape)))
        if (isTRUE(readpoly)) {
          shp <- readOGR(dirname(outshape), layer = basename(outshape), verbose=!quiet)
          return(shp)
        }
        return(NULL)
      }


      # Create a function to split the raster using gdalUtils::gdal_translate
      split_rast <- function(infile, outfile, llx, lly, win_width, win_height) {
        library(gdalUtils)
        gdal_translate(infile, outfile,
                       srcwin=c(llx, lly - win_height, win_width, win_height))
      }


      Water_Urban_mask <- function(tile.path, tile.name, urban, water) {
                                              # load image tile
          tile <- stack(tile.path)
                                              # get extent image and make sp object
          et <- as(extent(tile), "SpatialPolygons")
          proj4string(et) <- "+init=epsg:26916"
                                              # Mask out non-urban areas
          if(gContainsProperly(urban,et) & !gIntersects(water,et)){
              writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
          } else if (gContainsProperly(urban,et) & gIntersects(water,et)) {
              tile <- mask(tile, water, inverse = T)
              writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
          } else if (gIntersects(urban, et) & !gIntersects(water,et)) {
              tile <- mask(tile, urban)
              writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
          } else if (gIntersects(urban, et) & gIntersects(water,et)) {
              tile <- mask(tile, urban)
              tile <- mask(tile, water, inverse = T)
              writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
          }
      }

  Crop_mask <- function(tile.path, tile.name, CDL_stack, n_years){

    tile <- stack(tile.path)
    crops <- crop(CDL_stack, tile)

          # These are the values in the CDL that correspond to non crop cover types and not water
          NonCroppedValues <- c(0,63:65, 81:83, 87:88, 112, 121:124, 131, 141:143, 152, 176, 190, 195)
          # open water is 111

          NonCroppedValues <- c(0,63:65, 81:83, 87:88, 112, 121:124, 131, 141:143, 152, 176, 190, 195)
          # open water is 111. I don't include it in the above list so that it gets masked

          # I'm going to add 37, Other Hay/Non-alfalfa, to the non crop cover types
          NonCroppedValues <- c(NonCroppedValues, 37)
          # I'm going to add 36, Alfalfa, to the non crop cover types
          NonCroppedValues <- c(NonCroppedValues, 36)

          # find cells that have been assigned crop all three years
          crops[crops %in% NonCroppedValues] <- 0
          crops[!(crops %in% NonCroppedValues)] <- 1
          cropsum <- overlay(crops, fun = sum)

          dis.cropsum <- disaggregate(cropsum, fact = 20)
          dis.cropsum <- resample(dis.cropsum, tile, "ngb")
          masked_tile <- mask(tile, dis.cropsum, maskvalue = n_years)

          #               Save Image
          writeRaster(masked_tile, paste0(crop.masked.tiles.directory, "/", tile.name), overwrite = T)
      }




#+END_SRC


** Reproject shapefiles to image's projection
#+BEGIN_SRC R
    image <- stack(paste0(image.rd.directory, image.name,".tif"))

    # get image projection, CRS
    ImageCRS <- image@crs

    # Reproject Urban Areas
    WI_UrbanAreas <- readOGR(dsn = WI_UrbanAreas.dsn, layer = WI_UrbanAreas.layer)
    WI_UrbanAreas <- spTransform(WI_UrbanAreas, ImageCRS)

    # Reproject Water

      water <- readOGR(dsn = water.dsn, layer = water.name)
      water <- spTransform(water, ImageCRS)

    # Reproject Cropland Datalayer

    crop2010 <- str_c(crop.directory, crop2010.name, ".tif") %>%
            raster()

        crop2011 <- str_c(crop.directory, crop2011.name, ".tif") %>%
            raster()

        crop2012 <- str_c(crop.directory, crop2012.name, ".tif") %>%
            raster()

        crop2013 <- str_c(crop.directory, crop2013.name, ".tif") %>%
            raster()

        crop2014 <- str_c(crop.directory, crop2014.name, ".tif") %>%
            raster()

    crops <- stack(crop2011, crop2012, crop2013, crop2014)

    # don't need to reproject crop data layer because it's already in utm 16
    #    crops <- projectRaster(from = crops, crs = ImageCRS)
  proj4string(crops) <- ImageCRS

#+END_SRC





** Model Building and Accuracy Assessment
*** Plan/thoughts Assess Model Accuracy

There are 3 ways that I will assess accuracy.
1) Agreement of classification with field data (at aggregate level)
2) Agreement of classification with pixel-based assessment
3) Agreement of classification with 100m x 100m grids


There are 2 categories of parameters that I'm testing:
1) Segmentation parameters
2) Classification algorithm (support vector machines or random forest)
   and their associated parameters.

Can model be extended to other urban areas?
Once I know the best Segmentation and Classification parameters, I
need to assess the performance of a model built with Wausau training
data in predicting Madison land cover.  And vice versa, and with the
training data sets combined.  I will need to get "ground truth" data
from wausau area.
 - This is required to feel comfortable applying models to the rest of
   the urban areas in WI.
 - Note, Wausau among other places' images have long shadows because
   the images were acquired sometimes as early as 9am.

What is the accuracy of the NAIP imagery?  Or in other words, what is
the ceiling on the accuracy of our classification?  The classifier
won't be able to make up for low quality pixels (shadows).  garbage
in, garbage out.  High quality in, accurate maps out.






Note on Google Earth and the NAIP image:
- While the surface of the two images agree (ground cover, grass,
  roads etc), objects that have height such as trees and buildings are
  not in the same location. Parallax.  The errors are often pretty
  high, around 10m or more.
- This is easily seen by loading the image into Google earth using qgis.
- Implication:
  - When using google earth to assess accuracy, there will errors at
    scales smaller than this error.  However if aggregated over larger
    area (the 100m blocks) the accuracy should be relatively high.
    This is because the relative cover of the two images is very
    similar over the whole extent, however there is shift.

Note on shadows:
There are a lot of shadows in the Wausau image.  We are making the
decision to try to classify these, but report the high uncertainty
that will be associated with the areas.

Note on where to select training data:

I need to specify a few areas that I completely classify by hand for
each segmentation.  They don't have to be large, but they have to be
the same.  I'll use these to find the best segmentation.  Then once I
know the best one, I'll collect extra training data.

So I need to define maybe 5 200m x 200m boxes that I completely
classify for each of the segmentation combinations.  This will
standardize my effort.




*** Find Best Segmentation Parameters

**** Read in training region shapefiles
This is a relatively small region in Madison for which I will apply a
number of segmentations and manually classify each segment within.
I'll build the models with these different segmentations and see which
is best, then collect more training data for the best segmentation.

#+BEGIN_SRC R

  MadisonTrainingRegionForSegmentation.dsn <- "../RD_UFIA_TrainingRegions"
  MadisonTrainingRegionForSegmentation.layer <- "TrainingRegionsToEqualizeEffortToFindBestSegmentation"

  MadisonSegmentationTrainingRegions <- readOGR(dsn = MadisonTrainingRegionForSegmentation.dsn,
                                                layer = MadisonTrainingRegionForSegmentation.layer)

  MadisonSegmentationTrainingRegions <- spTransform(MadisonSegmentationTrainingRegions,CRSobj = ImageCRS)

# add a 20m buffer

MadisonSegmentationTrainingRegions_wBuffer <- gBuffer(MadisonSegmentationTrainingRegions, width = 20)

#+END_SRC



**** Read in Accuracy Assessment (testing) shapefiles
***** Field data plot centers

#+BEGIN_SRC R

  ## Read in the plot centers shape file
  fieldPlotCenters.dsn <- "../RD_UFIA_testing-AccuracyAssessment_datasets/FieldData/PlotCenterShpFile"
  fieldPlotCenters.layer <- "plotCenter"

  plot_centers <- readOGR(dsn = fieldPlotCenters.dsn, layer = fieldPlotCenters.layer)
  plot_centers <- spTransform(plot_centers,CRSobj = ImageCRS)


  ## Create polygon shapefile around plot centers

  # Make Diamond polygons around each plot
  dia <- list()
  for (i in seq_along(plot_centers)) {
        dia[i] <- gBuffer(plot_centers[i,], width = rad,quadsegs = 1)
    }

  field_plot_dia <-do.call(bind,dia)
#+END_SRC

***** Robi Points
#+BEGIN_SRC R :eval no :exports none
  ## RobiPoints.dsn <- "../RD_UFIA_testing-AccuracyAssessment_datasets/RD_UFIA_RobiAccuracyCover"
  ## RobiPoints.layer <- "accuracy_cover_2500"

  ## robi_points <- readOGR(dsn = RobiPoints.dsn,
  ##                        layer = RobiPoints.layer)
  ## proj4string(robi_points) <- "+init=epsg:3160"
  ##     robi_points <- spTransform(robi_points,CRSobj = ImageCRS)

  ##     ## Create Polygon around
  ##   robi_points_poly <- gBuffer(robi_points, width = 100)

#+END_SRC

***** Grids of points
#+NAME: ReadGridTestingPoints
#+BEGIN_SRC R
  grid.dsn <- "../RD_UFIA_testing-AccuracyAssessment_datasets/RD_UFIA_GridAccuracy"
  grid.layer <- "ufia-grid-points"

  grid <- readOGR(dsn = grid.dsn,
                  layer = grid.layer)

      grid <- spTransform(grid,CRSobj = ImageCRS)

      ## Create Polygon around
    grid_poly <- gBuffer(grid, width = 20)
#+END_SRC

**** Crop image to training and testing regions
I should probably not rbind training and testing regions, but keep
each type in a separate directory (especially since their idenifiers
get dropped).
#+BEGIN_SRC R

      # merge training and testing polygons

      training.testing <- list(training = MadisonSegmentationTrainingRegions_wBuffer, fieldplot = field_plot_dia, grids = grid_poly)  #robi_points_poly removed from training testing


    library(foreach)
    library(doParallel)

  dir.create(image.training.testing.dir)
  dir.create(croppedRaster.image.training.testing.dir)


  lapply(names(training.testing), function(x) {
      dir.create(str_c(croppedRaster.image.training.testing.dir,"/",x))
  })

    parallel_crop <- function(raster, spatialpolygons, spatialpolygondir, cores = detectCores()-3) {
        polygons <- disaggregate(spatialpolygons)
        s <- raster
        cl <- makeCluster(cores)
        registerDoParallel(cl)
        foreach (i = seq_along(polygons),
                 .packages = c("raster"),
                 .export = "croppedRaster.image.training.testing.dir") %dopar% {
            r <- s
            r <- crop(r, polygons[i])
            writeRaster(r, paste0(croppedRaster.image.training.testing.dir,"/",spatialpolygondir,"/",i,".tif"),
                        overwrite = T)
        }
      }


  lapply(names(training.testing), function(x) {
      parallel_crop(image, training.testing[[x]], spatialpolygondir = x)
  })

#+END_SRC



x**** Mask water urban and crops from image
- Use WI waterbodies, Urban area extent, and cropland datalayer to
  mask out areas that are not of interest.
- Save masked NAIP in masked_Image folder
#+BEGIN_SRC R
        ##############
        #######              Masking non urban landcover
        ##############
                                              # For every tile of the raster, apply the mask

      cl <- makeCluster(cores)
      registerDoParallel(cl)

      masked.tiles.directory <- str_c(image.training.testing.dir,"/MaskedTiles")
      dir.create(path = masked.tiles.directory, showWarnings = T)

      foreach(j = seq_along(names(training.testing))) %do% {
              x <- names(training.testing)[j]
              tiles_fullName <- list.files(path = str_c(croppedRaster.image.training.testing.dir,"/",x), full.names = T)
              tiles_shortName <- list.files(path = str_c(croppedRaster.image.training.testing.dir,"/",x), full.names = F)

              masked.tiles.directory <- str_c(image.training.testing.dir,"/MaskedTiles/",x)
              dir.create(path = masked.tiles.directory, showWarnings = T)

                  foreach (i = seq_along(tiles_fullName), .export = c("Water_Urban_mask", "masked.tiles.directory"), .packages = c("raster","sp","rgeos", "stringr")) %dopar% {
                      print(i)
                      Water_Urban_mask(tile.path = tiles_fullName[i],
                           tile.name = tiles_shortName[i],
                           urban = WI_UrbanAreas,
                           water = water)
                  }

      }



                                              #Options for above function
      # contained urban, don't intersect water = as is
      # contained urban, intersect water = mask water
      # intersect urban, don't intersect water = mask urban
      # intersect urban, intersect water = mask urban & water
    # if none of the above, don't write the raster


    ######## Masking Crops



                                              # For every tile of the raster, apply the mask
      cl <- makeCluster(cores)
      registerDoParallel(cl)

      foreach(j = seq_along(names(training.testing))) %do% {
          x <- names(training.testing)[j]
          tiles_fullName<- list.files(path = str_c(image.training.testing.dir,"/MaskedTiles/",x), full.names = T)
          tiles_shortName <- list.files(path = str_c(image.training.testing.dir,"/MaskedTiles/",x), full.names = F)
          crop.masked.tiles.directory <- str_c(image.training.testing.dir,"/CropMaskedTiles/",x)
          dir.create(path = crop.masked.tiles.directory, showWarnings = F)

          foreach (i = seq_along(tiles_fullName), .packages = c("raster","sp","rgeos", "stringr")) %dopar% {
                Crop_mask(tile.path = tiles_fullName[i],
                    tile.name = tiles_shortName[i],
                    CDL_stack = crops,
                    n_years = n_croplandLayers)
          }
      }

#+END_SRC


**** Add Ratios of image
- Read in Cropped_Image
- Save results in Ratios folder
#+BEGIN_SRC R

                                            # Create directory
    ratio.dir <- str_c(image.training.testing.dir,"/RatioTiles")
    dir.create(path = ratio.dir)

    add_ratios.ndvi <- function(tile.path,tile.name) {
        tile <- stack(tile.path)
        names(tile) <- c("red","green","blue","nir")

        # Create a ratio image for each band
        ratio.brick <- ratio(tile)
        ratio.brick <- ratio.brick*200 # rescale ndvi to save as 'INT1U'
        names(ratio.brick) <- paste0(c("blue","green","red","nir"),rep("_ratio",times = 4))
        ndvi <- ndvi_nodrop(tile, 3, 4)
        ndvi <- (ndvi+1)*100 # rescale ndvi to save as 'INT1U'
        ratio.tile <- raster::stack(tile, ratio.brick, ndvi)
        writeRaster(ratio.tile,
                    filename = paste0(ratio.subdir,"/",tile.name),
                    overwrite = T,
                    datatype = 'INT1U')

    }



    cl <- makeCluster(cores)
    registerDoParallel(cl)

  foreach(j = seq_along(names(training.testing))) %do% {
      x <- names(training.testing)[j]
      ratio.subdir <- str_c(ratio.dir,"/",x)
      dir.create(path = ratio.subdir)
      tile.paths <- list.files(str_c(image.training.testing.dir,"/CropMaskedTiles/",x), pattern = "*.tif$", full.names = T)
      tile.names <- list.files(str_c(image.training.testing.dir,"/CropMaskedTiles/",x), pattern = "*.tif$", full.names = F)

      foreach (i = seq_along(tile.paths), .packages = c("raster")) %dopar% {
          add_ratios.ndvi(tile.paths[i], tile.names[i])
      }
  }

#+END_SRC

**** Perform PCA
I've decided I can perform the pca on each tile separately and apply
the segmentation.

#+BEGIN_SRC R
  dir.create(path = str_c(image.training.testing.dir,"/PCATiles"))

  cl <- makeCluster(cores)
  registerDoParallel(cl)


    foreach(j = seq_along(names(training.testing))) %do% {
        x <- names(training.testing)[j]

                                                # make directory for PCA tiles
        dir.create(path = str_c(image.training.testing.dir,"/PCATiles/",x))

        tile.paths <- list.files(str_c(image.training.testing.dir,"/RatioTiles/",x), pattern = "*.tif$", full.names = T)
        tile.names <- list.files(str_c(image.training.testing.dir,"/RatioTiles/",x), pattern = "*.tif$", full.names = F)


        sr <- foreach (i = seq_along(tile.names), .packages = c("raster")) %dopar% {
            tile <- stack(tile.paths[i])
            s <- sampleRandom(tile, ifelse(ncell(tile) > 10000,10000,ncell(tile)))
            colnames(s) <- c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")
            pca <- prcomp(s, scale = T)
            predict_pca(tile.paths[i],
                        pca,
                        n.comps = 3,
                        out.path = paste0(image.training.testing.dir,"/PCATiles/",x,"/",tile.names[i])
                        )

        }
    }




        getRasterMinMax <- function(t.path) {
            tile <- stack(t.path)
            mn <- cellStats(tile, stat = "min")
            mx <- cellStats(tile, stat = "max")
            mnmx <- c(mn,mx)
            return (mnmx)
        }

        range0255 <- function(tile.path, tile.name){
            r <- stack(tile.path)
            r <- (r - mn)/(mx-mn) * 255
            writeRaster(r, paste0(image.training.testing.dir,"/ScaledPCATiles/",x,"/",tile.name), overwrite=TRUE, datatype = 'INT1U')
          }



  dir.create(str_c(image.training.testing.dir,"/ScaledPCATiles"))


        cl <- makeCluster(cores)
        registerDoParallel(cl)


  foreach(j = seq_along(names(training.testing))) %do% {
        x <- names(training.testing)[j]
                                                # scale pca tiles between 0 and 255
        dir.create(str_c(image.training.testing.dir,"/ScaledPCATiles/",x))
                                                # find min and max
        tile.paths <- list.files(str_c(image.training.testing.dir,"/PCATiles/",x), pattern = "*.tif$", full.names = T)
        tile.names <- list.files(str_c(image.training.testing.dir,"/PCATiles/",x), pattern = "*.tif$", full.names = F)

                                                #get min and max

        minmax <- foreach(i = seq_along(tile.paths), .packages = "raster", .combine = "rbind") %dopar% {
            getRasterMinMax(tile.paths[i])
        }

        mn <- apply(minmax, 2, min, na.rm = T)[1:3]
        mx <- apply(minmax, 2, max, na.rm = T)[4:6]


        foreach (i = seq_along(tile.paths), .packages = "raster") %dopar% {
            range0255(tile.paths[i], tile.names[i])
        }
  }


      # Remove the unscaled PCA tiles to make some disk space
    #  do.call(file.remove,list(tile.paths))

#+END_SRC

**** Segment image

This moves into the directory of the image, then into the directory of
Madison, then runs the segmentation code with arguments that specify
number of pixels per segment and the compactness.


#+BEGIN_SRC sh :results raw
    cd /home/erker/mydata2/DD_TrainingAndTesting/madison

  mkdir SegmentationTiles
  mkdir SegmentationTiles/grids
  mkdir SegmentationTiles/fieldplot
  mkdir SegmentationTiles/training


       python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs_TrainTesting.py 60 30 grids
       python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs_TrainTesting.py 30 15 grids
       python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs_TrainTesting.py 105 32 grids
       python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs_TrainTesting.py 60 30 training
       python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs_TrainTesting.py 30 15 training
       python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs_TrainTesting.py 105 32 training
       python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs_TrainTesting.py 60 30 fieldplot
       python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs_TrainTesting.py 30 15 fieldplot
       python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs_TrainTesting.py 105 32 fieldplot


#+END_SRC
average number of pixels per segment is 60
compactness parameter is 30
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 30
compactness parameter is 15
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 105
compactness parameter is 32
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 60
compactness parameter is 30
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 30
compactness parameter is 15
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 105
compactness parameter is 32
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 60
compactness parameter is 30
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 30
compactness parameter is 15
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 105
compactness parameter is 32
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 60
compactness parameter is 30
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 60
compactness parameter is 30
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
Making segments
Made dem segments
Tiff made successfully
average number of pixels per segment is 60
compactness parameter is 30
average number of pixels per segment is 60
compactness parameter is 30


**** Polygonize Segments
#+BEGIN_SRC R
                                            # make directory for Polygonized Segments
  PolygonDir <- str_c(image.training.testing.dir,"/PolygonizedSegTiles")
  dir.create(path = PolygonDir)

  cl <- makeCluster(cores)
  registerDoParallel(cl)


foreach (j = seq_along(names()) %do% {
  x <-
  dir.create(path = str_c(PolygonDir,"/",x))
                                            # Sample from every raster
    tile.paths <- list.files(str_c(image.training.testing.dir,"/SegmentationTiles/",x), pattern = "*.tif$", full.names = T)
    tile.names <- list.files(str_c(image.training.testing.dir,"/SegmentationTiles/",x), pattern = "*.tif$", full.names = F)

  # I'm going to sample from these to polygonize and use for training
                                          # get just the tile name
  b <- str_extract(tile.names,"^[1-2].tif*")

  # select the segmentations of the sample of segmentations

  tile.pathsToSample <- tile.paths[which(complete.cases(b))]
  tile.namesToSample <- tile.names[which(complete.cases(b))]

  foreach (i = seq_along(tile.pathsToSample), .packages = c("raster","sp","gdalUtils")) %dopar% {
      seg <- raster(tile.pathsToSample[i])
      segPoly <- gdal_polygonizeR(seg)
      segPoly$Class <- "N"
      writeOGR(obj = segPoly,
               dsn = PolygonDir,
               layer = tile.namesToSample[i],
               driver = "ESRI Shapefile",
               overwrite = T)
  }

#+END_SRC


**** manually classify segments in training regions

#+BEGIN_SRC R
dir.create(str_c(image.training.testing.dir,"/classifiedTrainingPolygons"))
#+END_SRC

Move Classified Training Polygons to these folders


**** Build Models using segments from training regions
When I build models , I should test differences between tuned and
untuned models.
***** Calculate zonal statistics for each segment for each tile.
 #+BEGIN_SRC R
   # Create Directory to store features (inputs I will use to predict on)
   segment.features.path <- str_c(image.training.testing.dir,"/SegmentFeatureDFs/")

   dir.create(path = segment.features.path)

   seg.tile.paths <- list.files(str_c(image.training.testing.dir, "/SegmentationTiles"), full.names = T)

   seg.tile.paths <- seg.tile.paths[str_detect(seg.tile.paths, "\\.tif$")]
   seg.tile.names <- list.files(str_c(image.training.testing.dir, "/SegmentationTiles"), full.names = F)
   seg.tile.names <- seg.tile.names[str_detect(seg.tile.names, "\\.tif$")]

   #remove ".tif"
   seg.tile.names <- sapply(seg.tile.names, function(s) str_sub(s, end = nchar(s) - 4))


   tiles <- str_extract(seg.tile.paths, "[0-9]+\\.tif")

   ratio.tile.paths <- sapply(tiles, function(t) {
                                 str_c(image.training.testing.dir, "/RatioTiles/", t)
                             })

   i <- 4
   ratios.tile.path <- ratio.tile.paths[i]
   seg.tile.path <- seg.tile.paths[i]
   seg.tile.name <- seg.tile.names[i]


   extract_segment_features <- function(ratios.tile.path, seg.tile.path, seg.tile.name) {
       r.tile <- stack(ratios.tile.path)
       names(r.tile) <- c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")
       s.tile <- raster(seg.tile.path)

   # Create a data_frame where mean and variances are calculated by zone
       # calculate means
       means <- zonal(x = r.tile, z = s.tile, 'mean', na.rm = F, digits = 1)
       colnames(means)[2:ncol(means)] <- paste0(colnames(means)[2:ncol(means)], "_mean")

                                               # calculate sd
       sds <- zonal(x = r.tile, z = s.tile, 'sd', na.rm = F)
       colnames(sds)[2:ncol(sds)] <- paste0(colnames(sds)[2:ncol(sds)], "_sd")

       d <- merge(means,sds)
       saveRDS(d, file = paste0(segment.features.path, seg.tile.name,".rds"))
       print(seg.tile.name)
   }

     cl <- makeCluster(cores)
     registerDoParallel(cl)

   foreach (i = seq_along(ratio.tile.paths), .packages = c("raster")) %dopar% {
       extract_segment_features(ratio.tile.paths[i], seg.tile.paths[i], seg.tile.names[i])
   }


 #+END_SRC

***** Read in Training Polygons and Create DF to build models
This is a bit I'm doing for my presentation tomorrow <2016-02-02 Tue>,
I should probably rework it later.
#+BEGIN_SRC R
  urban.area.dir.path <- "/home/erker/mydata2/DD_NAIP-imagery/madison_wausau_mounthoreb_NAIP/Madison"
                                          # Read in the training data from the shapefiles
  trainingDSN <- str_c(image.training.testing.dir, "/classifiedTrainingPolygons")
  trainingShapefiles <- list.files(trainingDSN) %>%
      str_sub(.,end = nchar(.)-8) %>%
          unique()

                                          # Get the tiles from the shapefiles
  tiles <- str_extract(trainingShapefiles, pattern = "[0-9]+-[0-9]+")


  # load training data from shapefiles into memory
  shapelist.data <- lapply(trainingShapefiles, function(shp) {
                          readOGR(dsn = trainingDSN, layer = shp)@data %>%
                                             na.omit() %>%
                                                 rename(zone = DN) %>%
                                                     filter(Class != "N")
                      })
  names(shapelist.data) <- trainingShapefiles


  training.tile.names <- trainingShapefiles

  # Join Training Shapefile Data with extracted features from polygons

  # List all dataframes of extracted features
  extractedFeatures.files <- list.files(str_c(urban.area.dir.path, "/SegmentFeatureDFs"), full.names = T)

  # Select the feature dataframes that have training data
  index <- mapply(training.tile.names, FUN = function(x) str_detect(extractedFeatures.files, x))
  index <- apply(index, MARGIN = 1, FUN = sum) %>% as.logical()
  extractedFeatures <- extractedFeatures.files[index]


  # Group data by segmentation parameters

  uniqueSegParameterSets <- str_extract(trainingShapefiles, pattern = "N-[0-9]+_C-[0-9]+") %>% unique()

  # Create Training Data DF by merging polygon classification with polygon features
  TrainingData <- foreach (i = seq_along(uniqueSegParameterSets)) %do% {
      trn <- shapelist.data[str_detect(names(shapelist.data), uniqueSegParameterSets[i])]
      d.path <- extractedFeatures[str_detect(extractedFeatures, uniqueSegParameterSets[i])]
      trainingData <- list()
      foreach(j = seq_along(d.path)) %do% {
          d <- readRDS(d.path[j])
          trainingData[[j]] <- left_join(trn[[j]],d)
      }
      do.call("rbind",trainingData)
  }

  names(TrainingData) <- uniqueSegParameterSets

                                          # Save this list of dataframes
  dir.create(str_c(image.training.testing.dir, "/DataForBuildingModel"))
  saveRDS(TrainingData, file = str_c(image.training.testing.dir, "/DataForBuildingModel/trainingData.rds"))
 #+END_SRC

****** Read in Training Polygons and Create DF to build models

 reads in "TrainingPolygons", "SegmentFeatureDFs".  Writes to "DataForBuildingModels"

 #+BEGIN_SRC R
                                           # Read in the training data from the shapefiles
   trainingDSN <- str_c(image.training.testing.dir, "/classifiedTrainingPolygons")
   trainingShapefiles <- list.files(trainingDSN) %>%
       str_sub(.,end = nchar(.)-4) %>%
           unique()

                                           # Get the tiles from the shapefiles
   tiles <- str_extract(trainingShapefiles, pattern = "[0-9]+")


   # load training data from shapefiles into memory
   shapelist.data <- lapply(trainingShapefiles, function(shp) {
                           readOGR(dsn = trainingDSN, layer = shp)@data %>%
                                              na.omit() %>%
                                                  rename(zone = DN) %>%
                                                      filter(Class != "N")
                       })
   names(shapelist.data) <- trainingShapefiles

   #I'll have to change the pattern when I change the filename pattern for training
   training.tile.names <- str_extract(trainingShapefiles, "[0-9]+[0-9.tif_N-[0-9]+_C-[0-9]+")

   # Join Training Shapefile Data with extracted features from polygons

   # List all dataframes of extracted features
   extractedFeatures.files <- list.files(str_c(image.training.testing.dir, "/SegmentFeatureDFs"), full.names = T)

   # Select the feature dataframes that have training data
   index <- mapply(training.tile.names, FUN = function(x) str_detect(extractedFeatures.files, x))
   index <- apply(index, MARGIN = 1, FUN = sum) %>% as.logical()
   extractedFeatures <- extractedFeatures.files[index]


   # Group data by segmentation parameters

   uniqueSegParameterSets <- str_extract(trainingShapefiles, pattern = "N-[0-9]+_C-[0-9]+") %>% unique()

   # Create Training Data DF by merging polygon classification with polygon features
   TrainingData <- foreach (i = seq_along(uniqueSegParameterSets)) %do% {
       trn <- shapelist.data[str_detect(names(shapelist.data), uniqueSegParameterSets[i])]
       d.path <- extractedFeatures[str_detect(extractedFeatures, uniqueSegParameterSets[i])]
       trainingData <- list()
       foreach(j = seq_along(d.path)) %do% {
           d <- readRDS(d.path[j])
           trainingData[[j]] <- left_join(trn[[j]],d)
       }
       do.call("rbind",trainingData)
   }

   names(TrainingData) <- uniqueSegParameterSets

                                           # Save this list of dataframes
   dir.create(str_c(image.training.testing.dir, "/DataForBuildingModel"))
   saveRDS(TrainingData, file = str_c(image.training.testing.dir, "/DataForBuildingModel/trainingData.rds"))
 #+END_SRC

***** Build SVM and RF models
 #+BEGIN_SRC R

   # For each combination of segmentation parameters:

   # Read in data
   pathToTrainingDFs <-  str_c(image.training.testing.dir, "/DataForBuildingModel/trainingData.rds")

   dat.list <- readRDS(pathToTrainingDFs)
   # Copy Data for Cover specfic models
   dat.list_T <- lapply(dat.list, function(d) {
                            mutate(d, Class = as.character(Class)) %>%
                            mutate(Class = ifelse(Class == "T", Class, "O"))
                        })

   dat.list_G <- lapply(dat.list, function(d) {
                            mutate(d, Class = as.character(Class)) %>%
                            mutate(Class = ifelse(Class == "G", Class, "O"))
                        })

   dat.list_I <- lapply(dat.list, function(d) {
                            mutate(d, Class = as.character(Class)) %>%
                            mutate(Class = ifelse(Class == "I", Class, "O"))
                        })


   # Create Tasks

   task.list <- lapply(seq_along(dat.list), function(i) {
       makeClassifTask(id = paste0(image.name,"_",names(dat.list)[[i]],"_all"), data = dat.list[[i]], target = "Class") %>%
           dropFeatures("zone")
   })

   tree.task.list <- lapply(seq_along(dat.list_T), function(i) {
       makeClassifTask(id = paste0(image.name,"_",names(dat.list_T)[[i]],"_tree"), data = dat.list_T[[i]], target = "Class", positive = "T") %>%
           dropFeatures("zone")
   })

   grass.task.list <- lapply(seq_along(dat.list_G), function(i) {
       makeClassifTask(id = paste0(image.name,"_",names(dat.list_G)[[i]],"_grass"), data = dat.list_G[[i]], target = "Class", positive = "G") %>%
           dropFeatures("zone")
   })

   impervious.task.list <- lapply(seq_along(dat.list_I), function(i) {
       makeClassifTask(id = paste0(image.name,"_",names(dat.list_I)[[i]],"_impervious"), data = dat.list_I[[i]], target = "Class", positive = "I") %>%
           dropFeatures("zone")
   })

   task.list <- list(all = task.list, tree = tree.task.list, grass = grass.task.list, impervious = impervious.task.list) %>%
       unlist(recursive = F)


                                             # Make Learners
     # RF
     RF_prob <- makeLearner(id = "rf_prob","classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
     RF_response <- makeLearner(id = "rf_resp", "classif.randomForest", predict.type = "response", fix.factors.prediction = TRUE)
     SVM_prob <- makeLearner(id = "svm_prob", "classif.svm", predict.type = "prob", fix.factors.prediction = TRUE)
     SVM_response <- makeLearner(id = "svm_resp", "classif.svm", predict.type = "response", fix.factors.prediction = TRUE)

     learner_list <- list(RF_prob = RF_prob, RF_response = RF_response, SVM_prob = SVM_prob, SVM_response = SVM_response)


                                             # Train Learners on Tasks, Make models

     cl<-makeCluster(cores)
     registerDoParallel(cl)


     models <- foreach(task = task.list, .packages = "mlr") %:%
         foreach(learner = learner_list) %dopar% {
             train(learner, task)
         }

   dir.create(str_c(image.training.testing.dir, "/Models"))
   saveRDS(models, file = paste0(image.training.testing.dir,"/Models/models.rds"))

 #+END_SRC



**** Apply models to classify the testing regions

I should change the script so that it writes rasters with the smallest
datatype possible.

#+BEGIN_SRC R
        #load models
  models <- readRDS(paste0(image.training.testing.dir,"/Models/models.rds"))

  # unlist models
  models <- unlist(models, recursive = F)


                                          # Apply each model to each Tile

  seg.files <- list.files(str_c(image.training.testing.dir,"/SegmentationTiles"), full.names = T) %>%
      str_extract(., ".*.tif$") %>%
          na.omit()


  tile.names <- list.files(str_c(image.training.testing.dir,"/SegmentationTiles")) %>%
      str_extract(., "[0-9]+")

  features.files <- list.files(str_c(image.training.testing.dir,"/SegmentFeatureDFs"), full.names = T) %>%
      str_extract(., ".*rds$") %>%
          na.omit()



                                          # Create directories to save tiles into


  dir.create(paste0(image.training.testing.dir, "/ClassifiedTiles"))

  # Segmentation Parameters
  foreach(i = seq_along(models)) %do% {
          directory.path <- paste0(image.training.testing.dir,"/ClassifiedTiles/",models[[i]]$task.desc$id)
          dir.create(directory.path)
      }

  # Target (all classes, grass, tree, or impervious)
  foreach(i = seq_along(models)) %do% {
      directory2.path <- paste0(image.training.testing.dir,"/ClassifiedTiles/",models[[i]]$task.desc$id,"/",models[[i]]$learner$id)
      dir.create(directory2.path)
  }


  i <- 40
  seg.file.path <- seg.files[i]
  feature.file.path <- features.files[i]
  tile.name <- tile.names[i]
  j <- 1

  # I have to apply the correct model to each segmentation
  PredictOnSegmentedRaster <- function(seg.file.path, feature.file.path, tile.name, models) {
      seg <- raster(seg.file.path)
      features <- readRDS(feature.file.path)
      # get models that were built on this set of segmentation parameters
      seg.params <- str_extract(seg.file.path,"N-[0-9]+_C-[0-9]+")
      index <- sapply(models, FUN = function(mod) {
                          str_detect(mod$task.desc$id, pattern = seg.params)
                      })
      mods <- models[index]
      featuresRowsWithNA <- which(is.na(features[,2]))
      complete.features <- features[complete.cases(features),] # svm can't predict with NAs
      foreach(j = seq_along(mods)) %do% {
          mod <- mods[[j]]
          pred <- predict(mod, newdata = complete.features[2:19])
          response <- factor(as.character(pred$data$response), levels = c("G","I","T","O"))
          m <- cbind(zone = complete.features$zone, response)
          m <- left_join(as.data.frame(features["zone"]), as.data.frame(m))
          r <- reclassify(seg, m)
  #        x <- data.frame(ID = 1:4, LandCover = c("G","I","T","O")) %>%
  #            filter(LandCover %in% levels(factor(response)))
  #        levels(r) <- x
          if (ncol(pred$data) > 2) {
              prob <- (pred$data[,grep("prob.*", x = colnames(pred$data))]) # get columns that contain probabilities
              ProbOfClass <- apply(prob, MARGIN = 1, FUN = max)
              m <- cbind(zone = complete.features$zone, ProbOfClass)
              m <- left_join(as.data.frame(features["zone"]), as.data.frame(m))
              p <- reclassify(seg, m)
              r <- stack(r,p)
          }
          path <- paste0(image.training.testing.dir,"/ClassifiedTiles/",mods[[j]]$task.desc$id,"/",mods[[j]]$learner$id,"/",tile.name,".tif")
          writeRaster(r, path, overwrite=TRUE)
      }
  }




  cl<-makeCluster(cores)

  registerDoParallel(cl)

  foreach(i = seq_along(seg.files), .packages = c("raster","dplyr","stringr","foreach","mlr")) %dopar% {
      PredictOnSegmentedRaster(seg.files[i], features.files[i], tile.names[i], models)
  }

#+END_SRC

**** merge classifications

#+BEGIN_SRC sh
cd ../DD_TrainingAndTesting/madison/ClassifiedTiles/madison_N-105_C-32_all/rf_prob
rm combinedClassification.tif
   ls -1 *.tif > tiff_list.txt
   gdal_merge.py -v -o combinedClassification.tif --optfile tiff_list.txt

#+END_SRC

#+BEGIN_SRC sh
cd ../DD_TrainingAndTesting/madison/ClassifiedTiles/madison_N-60_C-30_all/rf_prob
   ls -1 [0-9]+.tif > tiff_list.txt
   gdal_merge.py -n 0 -v -o combinedClassification.tif --optfile tiff_list.txt &
#+END_SRC


#+BEGIN_SRC sh
cd ../DD_TrainingAndTesting/madison/ClassifiedTiles/madison_N-30_C-15_all/rf_prob
   ls -1  [0-9]+.tif > tiff_list.txt
   gdal_merge.py -n 0 -v -o combinedClassification.tif --optfile tiff_list.txt &
#+END_SRC
**** Assess Accuracy
Functions that take the "truth" and the classified image

#+CALL: ReadGridTestingPoints

#+BEGIN_SRC R

  grid

  r <- stack("../DD_TrainingAndTesting/madison/ClassifiedTiles/madison_N-105_C-32_all/rf_prob/combinedClassification.tif")

  r <- stack("../DD_TrainingAndTesting/madison/ClassifiedTiles/madison_N-105_C-32_all/rf_prob/1001.tif")

  levelplot(r)

#  Calculate_AccuracyAccordingFieldData <- function(classifiedRaster,FieldData)
#    Calculate_AccuracyAccordingToRobiPoints <- function(classifiedRaster, RobiPoints)
    Calculate_AccuracyAccordingToGrid <- function(classifiedRaster, GridPoints)

#+END_SRC

***** From first accuracy assessment attempt
****** pan spot?
#+BEGIN_SRC R
#############
#                The objective of this document
#
#         I have to provide an accuracy assesment tomorrow
#        This document strives to get me there as fast as possible
#         I will have to organize this code next week
#
##############################


##### Loading Libraries

library(dplyr)
  library(rgdal)
  library(raster)
  library(rgeos)
  library(glcm)
  library(spatial.tools)
  library(dplyr)
  library(doParallel)
  library(mlr)
library(randomForest)

# First I am going to perform accuracy assessment for the pansharpened spot
## Loading the Pansharpened image and addtional layers

PAN_SPOT <- brick("../PAN_SPOT/geomatica_SPOT_panshp_wRatios.tif")
PAN_SPOT_tex<- brick("../PAN_SPOT/geomatica_SPOT_panshp_wRatios_wTexture.tif")
image <- stack(PAN_SPOT, PAN_SPOT_tex)
image <- image[[-18]]
image

## Load shapefiles of training data
  l_water <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_water", encoding = "ESRI Shapefile")
  l_grass <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_grass", encoding = "ESRI Shapefile")
  l_tree <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_tree", encoding = "ESRI Shapefile")
  l_soil_readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_soil", encoding = "ESRI Shapefile")
  l_impervious <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_impervious", encoding = "ESRI Shapefile")
  names(l_water) <- "water"
  names(l_grass) <- "grass"
  names(l_tree) <- "tree"
  names(l_soil) <- "soil"
  names(l_impervious) <- "impervious"

### create buffers around training data
  l_water_buf <- gBuffer(l_water, width = 10)
  l_grass_buf <- gBuffer(l_grass, width = 10)
  l_tree_buf <- gBuffer(l_tree, width = 10)
  l_soil_buf <- gBuffer(l_soil, width = 10)
  l_impervious_buf <- gBuffer(l_impervious, width = 10)

## Load shapefiles of accuracy asssesment areas
### field data
plot_centers <- readOGR(dsn = "../FieldData/PlotCenterShpFile", layer = "plotCenter")
plot_centers <- spTransform(plot_centers,CRSobj = CRS("+proj=utm +zone=16 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
rad <- 25*.3048+3  #radius of fieldplot plus 3m buffer
#rad <- 15
dia <- list()
for (i in seq_along(plot_centers)) {
      dia[i] <- gBuffer(plot_centers[i,], width = rad,quadsegs = 1)
  }

r_list <- list()
for (i in seq_along(dia)){
    r_list[[i]] <- crop(image,dia[[i]])
}
plots_101 <- r_list

plots_101_merged <- do.call(merge, args = r_list)

### Andy's Grid of Points
  grd_ass <- readOGR( dsn = "../RD_UFIA_GridAccuracy", layer = "ufia-grid-points")
  grd_ass <- spTransform(grd_ass, CRS("+init=epsg:26916"))
  grd_box <- gBuffer(grd_ass, width = 8)
grd_box <- disaggregate(grd_box)
str(grd_box)
grd_box

r_list <- list()
for (i in seq_along(grd_box@polygons)){
    r_list[[i]] <- crop(image, grd_box[i,])
}
grds_30 <- r_list

grds_30_merged <- do.call(merge, args = r_list)


### Robi's Random Points
rand_points <- readOGR("../RD_UFIA_RobiAccuracyCover", "accuracy_cover_2500")
proj4string(rand_points) <- "+init=epsg:26916"
#### crop to extent of madison, middleton, and shorewood hills
 municipalities <- readOGR("../RD_WI-Municipalities-Shapefiles", "MCD")

  ## ##                                         # Filter out Madison, Middleton, and Shorewood hills
 study.cities <- municipalities[municipalities$NAME10 == "MADISON" | municipalities$NAME10 == "Madison" | municipalities$NAME10 == "Shorewood Hills" | municipalities$NAME10 == "Middleton", ]
 study.cities <- spTransform(study.cities, CRS("+init=epsg:26916"))

rand_points <- rand_points[study.cities,]

#### add buffer
rand_points_buffer <- gBuffer(rand_points, width = 5)
rand_points_buffer <- disaggregate(rand_points_buffer)

r_list <- list()
for (i in seq_along(rand_points_buffer@polygons)){
    r_list[[i]] <- crop(image, rand_points_buffer[i,])
}
rand_points_r <- r_list
rand_points_r_merged <- do.call(merge, args = r_list)


#### Combine all these cropped images into one to create
#### the pansharpened spot image (with all added layers)
#### which contains the areas
#### for training and testing accuracy

list_rasters <- list(plots_101_merged, grds_30_merged, rand_points_r_merged)
r <- do.call(merge, list_rasters)
plot(r[[1]])
writeRaster(r, "../PAN_SPOT/raster_at_accuracyLocations.tif")



### drop last layer of r
d <- dropLayer(r, 18)



#### Create Pansharp models
image


#### Apply RF model to raster
rf_mod <- best_models[[1]]
names(r) <- attributes(rf_mod$learner.model$terms)$term.labels # assign names that the model was trained on.


rf_spot_raster <- raster::predict(object = r, rf_mod$learner.model)

#### save pan_spot_rf_accuracy_raster.tif
writeRaster(rf_spot_raster, "../PAN_SPOT/ClassifiedImages/pan_spot_rf_accuracy_raster.tif")




#### Apply SVM model to raster
svm_mod <- best_models[[i]]$learner.model
svm_spot_raster <- raster::predict(object = r, svm_mod)

#### save pan_spot_svm_accuracy_raster.tif
writeRaster(svm_spot_raster, "../PAN_SPOT/ClassifiedImages/pan_spot_svm_accuracy_raster.tif")








### Calculate Accuracy of pan spot rf using 1 ha blocks
#### read in 1 ha block data, tree defined by Andy via google earth
grid_accuracy <- read.csv("../RD_UFIA_GridAccuracy/grid_accuracy_assessment_andy.csv", header = T)
head(grid_accuracy)



#### get percent tree cover by plot
d <- grid_accuracy %>%
      group_by(Plot, Cover_Type) %>%
      summarize(number = n()) %>%
      mutate(percent_cover = number/225) %>%
      filter(Cover_Type == "t")

e <- rbind(d,data.frame(Plot = c(1,11,26), Cover_Type = c("t","t","t"), number = c(0,0,0),percent_cover = c(0,0,0)))
e <- arrange(e, Plot)

R_plotNames <- c(10,1,4,29,16,13,14,18,6,22,5,11,15,26,17,9,25,30,20,3,21,23,7,12,2,27,8,19,24,28)
d <- cbind(e,R_plotNames)
d
d <- arrange(d, R_plotNames)


#### overlay of grid boxes on top of pan spot rf classification and extract pixel values by plot
best_models <- readRDS("../PAN_SPOTbestModels.Rdata")
#### Apply RF model to raster

pred_grd30 <- lapply(grds_30, FUN= function(x){
           raster::predict(object = x, model = lei.rf.mod$learner.model)
       })
lei.rf.mod$learner.model

#### get frequency table for each raster
freq_grd30 <- lapply(pred_grd30, raster::freq)
freqdf_grd30 <- lapply(freq_grd30, as.data.frame)
#### get percent tree cover by plot (according to image)


percent_tree <- lapply(freqdf_grd30, FUN = function(x) {
                x[x$value == 4,2]/sum(x[,2])
            })
percent_tree <- unlist(percent_tree)


freqdf_grd30[[2]][freqdf_grd30[[2]]$value == 4,2]
freqdf_grd30[[2]]

percent_tree
#### calculate RMSE
andy_pctTree <- d$percent_cover
andy_pctTree



mean((percent_tree - andy_pctTree)^2)^0.5
plot(percent_tree,andy_pctTree)
d$percent_cover
percent_tree

png(filename = "pan_spot_rf_grid_prediction.png")
pan_spot_rf_predicted_percent_tree <- percent_tree
plot(andy_pctTree,pan_spot_rf_predicted_percent_tree, main = "RMSE = 18.288%")
abline(a=0, b= 1, col = "red")
dev.off()



### Calculate Accuracy of pan spot svm using 1 ha blocks
#### read in 1 ha block data, tree defined by Andy via google earth
#### get percent tree cover by plot
# This is andy_pctTree


best_models <- readRDS("../PAN_SPOTbestModels.Rdata")
#### Apply RF model to raster
#best_models
#lei.svm.mod <- best_models$lei.svm.mod

pred_grd30 <- lapply(grds_30, FUN= function(x){
           raster::predict(object = x, model = lei.svm.mod$learner.model)
       })
plot(pred_grd30[[1]])

#### get frequency table for each raster
freq_grd30 <- lapply(pred_grd30, raster::freq)
freqdf_grd30 <- lapply(freq_grd30, as.data.frame)
#### get percent tree cover by plot (according to image)


percent_tree <- lapply(freqdf_grd30, FUN = function(x) {
                x[x$value == 4,2]/sum(x[,2])
            })
percent_tree <- unlist(percent_tree)
pan_spot_svm_predicted_precent_tree <- percent_tree
#### calculate RMSE
mean((percent_tree - andy_pctTree)^2)^0.5

png(filename = "pan_spot_svm_grid_prediction.png")
plot(andy_pctTree, pan_spot_svm_predicted_precent_tree, main = "RMSE = 19.6%")
abline(a=0,b=1, col = "red")
dev.off()





### Calculate Accuracy of pan spot rf using field data
best_models <- readRDS("../PAN_SPOTbestModels.Rdata")
#### Apply RF model to raster

pred_plots <- lapply(plots_101, FUN = function(x){
           raster::predict(object = x, model = lei.rf.mod$learner.model)
       })


#### read field data, get percent tree cover by plot
field <- read.csv("../FieldData/UFIA_FieldData_summarizedbyPlot.csv", header = T)
str(field)
field_pct_tree <- field$PctGreaterThan0percentVegAbove


#### overlay of field plot diamonds on top of pan spot rf classification and extract pixel values by plot
pred_plots_inBuf <- list()
for (i in 1:101){
    pred_plots_inBuf[[i]] <- extract(x = pred_plots[[i]], y = dia[[i]])
}

plot(pred_plots[[94]])
pred_plots_inBuf <- unlist(pred_plots_inBuf,recursive = F)

#### get percent tree cover by plot (according to image)
pan_sharp_rf_pct_tree_fieldplots <- list()
for (i in 1:101){
    pan_sharp_rf_pct_tree_fieldplots[[i]] <- sum(pred_plots_inBuf[[i]] == 4)/length(pred_plots_inBuf[[i]])
}
pan_sharp_rf_pct_tree_fieldplots <- unlist(pan_sharp_rf_pct_tree_fieldplots)




#### calculate RMSE
mean((field_pct_tree - pan_sharp_rf_pct_tree_fieldplots)^2)^0.5

png(filename = "pan_spot_rf_fielddata_prediction.png")
plot(field_pct_tree, pan_sharp_rf_pct_tree_fieldplots, main = "RMSE = 22.5%")
abline(a= 0, b = 1, col = "red")
dev.off()






### Calculate Accuracy of pan spot svm using field data
#### Apply SVM to raster
pred_plots <- lapply(plots_101, FUN = function(x){
           raster::predict(object = x, model = lei.svm.mod$learner.model)
       })

pred_plots_inBuf <- list()
for (i in 1:101){
    pred_plots_inBuf[[i]] <- extract(x = pred_plots[[i]], y = dia[[i]])
}

plot(pred_plots[[1]])
pred_plots_inBuf <- unlist(pred_plots_inBuf,recursive = F)

#### get percent tree cover by plot (according to image)
pan_sharp_svm_pct_tree_fieldplots <- list()
for (i in 1:101){
    pan_sharp_svm_pct_tree_fieldplots[[i]] <- sum(pred_plots_inBuf[[i]] == 4)/length(pred_plots_inBuf[[i]])
}
pan_sharp_svm_pct_tree_fieldplots <- unlist(pan_sharp_svm_pct_tree_fieldplots)
#### calculate RMSE
mean((field_pct_tree - pan_sharp_svm_pct_tree_fieldplots)^2)^0.5

png(filename = "pan_spot_svm_fielddata_prediction.png")
plot(field_pct_tree, pan_sharp_svm_pct_tree_fieldplots, main = "RMSE = 23.4")
abline(a= 0, b = 1, col = "red")
dev.off()


#+END_SRC
****** naip ?
#+BEGIN_SRC R
  ##### Loading Libraries

  ## This is the accuracy assessment for the NAIP image.
  ## I will need to load the layers of naip that I have, crop to training and testing areas, then add texture
  ## Then build models and predict
  ## Then Assess accuracy.

  naip <- stack(x = "../NAIP/madison.tif")
  ratio1 <- stack(x = "../NAIP/madison_wRatio1.tif")
  ratio2 <- stack(x = "../NAIP/madison_wRatio2.tif")
  ratio3 <- stack(x = "../NAIP/madison_wRatio3.tif")
  ndvi <- stack("../NAIP/madison_NDVI.tif")
  savi <- stack("../NAIP/madison_SAVI.tif")
  image <- stack(naip, ratio1, ratio2, ratio3,ndvi, savi)

  ## Load shapefiles of training data
    l_water <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_water", encoding = "ESRI Shapefile")
    l_grass <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_grass", encoding = "ESRI Shapefile")
    l_tree <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_tree", encoding = "ESRI Shapefile")
    l_soil <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_soil", encoding = "ESRI Shapefile")
    l_impervious <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_impervious", encoding = "ESRI Shapefile")
    names(l_water) <- "water"
    names(l_grass) <- "grass"
    names(l_tree) <- "tree"
    names(l_soil) <- "soil"
    names(l_impervious) <- "impervious"

  ### create buffers around training data
    l_water_buf <- gBuffer(l_water, width = 10) %>%
        disaggregate()
    l_grass_buf <- gBuffer(l_grass, width = 10) %>%
        disaggregate()
    l_tree_buf <- gBuffer(l_tree, width = 10) %>%
        disaggregate()
    l_soil_buf <- gBuffer(l_soil, width = 10) %>%
        disaggregate()
    l_impervious_buf <- gBuffer(l_impervious, width = 10) %>%
        disaggregate()




  ## get rasterlayers at location of training data
  r_list <- list()
  for (i in seq_along(l_water_buf@polygons)){
          r_list[[i]] <- crop(image, l_water_buf[i,])
      }
  water_rasters <- r_list


  r_list <- list()
  for (i in seq_along(l_grass_buf@polygons)){
          r_list[[i]] <- crop(image, l_grass_buf[i,])
      }
  grass_rasters <- r_list

  r_list <- list()
  for (i in seq_along(l_tree_buf@polygons)){
          r_list[[i]] <- crop(image, l_tree_buf[i,])
      }
  tree_rasters <- r_list

  r_list <- list()
  for (i in seq_along(l_soil_buf@polygons)){
          r_list[[i]] <- crop(image, l_soil_buf[i,])
      }
  soil_rasters <- r_list

  r_list <- list()
  for (i in seq_along(l_impervious_buf@polygons)){
          r_list[[i]] <- crop(image, l_impervious_buf[i,])
      }
  impervious_rasters <- r_list


  ## Load shapefiles of accuracy asssesment areas
  ### field data
  plot_centers <- readOGR(dsn = "../FieldData/PlotCenterShpFile", layer = "plotCenter")
  plot_centers <- spTransform(plot_centers,CRSobj = CRS("+proj=utm +zone=16 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
  # rad_25*.3048+3  #radius of fieldplot plus 3m buffer
  rad <- 15
  dia <- list()
  for (i in seq_along(plot_centers)) {
            dia[i] <- gBuffer(plot_centers[i,], width = rad,quadsegs = 1)
        }

  r_list <- list()
  for (i in seq_along(dia)){
          r_list[[i]] <- crop(image,dia[[i]])
      }
  plots_101 <- r_list

  plots_101_merged <- do.call(merge, args = r_list)

  ### Andy's Grid of Points
    grd_ass <- readOGR( dsn = "../RD_UFIA_GridAccuracy", layer = "ufia-grid-points", drop_unsupported_fields = T, pointDropZ = T)
    grd_ass <- spTransform(grd_ass, CRS("+init=epsg:26916"))
    grd_box <- gBuffer(grd_ass, width = 8)
  grd_box <- disaggregate(grd_box)

  r_list <- list()
  for (i in seq_along(grd_box@polygons)){
          r_list[[i]] <- crop(image, grd_box[i,])
      }
  grds_30 <- r_list
  grds_30_merged <- do.call(merge, args = r_list)



  ### Robi's Random Points
  rand_points <- readOGR("../RD_UFIA_RobiAccuracyCover", "accuracy_cover_2500")
  proj4string(rand_points) <- "+init=epsg:26916"
  #### crop to extent of madison, middleton, and shorewood hills
   municipalities <- readOGR("../RD_WI-Municipalities-Shapefiles", "MCD")

    ## ##                                         # Filter out Madison, Middleton, and Shorewood hills
   study.cities <- municipalities[municipalities$NAME10 == "MADISON" | municipalities$NAME10 == "Madison" | municipalities$NAME10 == "Shorewood Hills" | municipalities$NAME10 == "Middleton", ]
   study.cities <- spTransform(study.cities, CRS("+init=epsg:26916"))

  rand_points <- rand_points[study.cities,]

  #### add buffer
  rand_points_buffer <- gBuffer(rand_points, width = 5)
  rand_points_buffer <- disaggregate(rand_points_buffer)


  r_list <- list()
  for (i in seq_along(rand_points_buffer@polygons)){
          r_list[[i]] <- crop(image, rand_points_buffer[i,])
      }

  rand_points_r <- do.call(merge, args = r_list)


  list_rasters <- list(soil_rasters, grass_rasters, tree_rasters, impervious_rasters, plots_101, grds_30)
  training_testing_rasters <- unlist(list_rasters)
  training_testing_rasters[[71]] <- NULL


  training_testing_rasters_wText <- lapply(training_testing_rasters, FUN = function(x) {
                                               texture <- glcm(x[[8]], na_opt = "ignore")
                                               stack(x,texture)
                                           })


  training_testing_rasters_wText

   ## training_testing_rasters_wText <- list()
  ## for (i in seq_along(training_testing_rasters)){
  ##     texture <- glcm(training_testing_rasters[[i]][[8]])
  ##     training_testing_rasters_wText[[i]] <- stack(training_testing_rasters[[i]],texture)
  ##                     print(i)
  ## }



  plot(training_testing_rasters_wText[[71]])






  ### Merge the training testing rasters with all the layers into one raster

  training_testing_rasters_wText_merged <- do.call(merge, args = training_testing_rasters_wText)
  image <- training_testing_rasters_wText_merged
  image
  ### extract the values of the raster at the training data locations
  extract_bind_df_addclass <- function(x) {
    w <- raster::extract(image,x)
    w <- do.call("rbind",w)
    w <- data.frame(w)
    w$Class <- names(x)
    return(w)
  }

  list_classes <- list(l_water, l_grass, l_tree, l_soil, l_impervious)

  beginCluster()
  b <- lapply(list_classes, function(x) extract_bind_df_addclass(x))
  endCluster()


  classified_px <- do.call("rbind", b)

  classified_px$Class %<>% as.factor()

  lei_naip_df <- classified_px
  write.csv(lei_naip_df, "../NAIP/lei_naip_df.csv")

  ### remove water
  lei_naip_df <- lei_naip_df %>%
      filter(Class != "water")

  #remove missing values
  lei_naip_df <- lei_naip_df %>%
      filter(complete.cases(.))

  ### build models

  # Make Task
  lei.classif.task <- makeClassifTask(id= "lei_naip", data = lei_naip_df, target = "Class")

  # Set parameters for tuning
  ctrl <- makeTuneControlIrace(maxExperiments = 200L)
  rdesc <- makeResampleDesc("CV",iters = 3L)

  rf.ps <- makeParamSet(
      makeIntegerParam("nodesize", lower = 1L, upper = 20L),
      makeIntegerParam("ntree", lower = 1L, upper = 10L,
                        trafo = function(x) 2^x),
      makeIntegerLearnerParam("mtry", lower = 1L, upper = 10L))

  svm.ps <- makeParamSet(
                      makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot")),
                      makeNumericParam("sigma", lower = -10, upper = 10,
                                       trafo = function(x) 2^x,
                                       requires = quote(kernel == "rbfdot")),
                      makeIntegerParam("degree", lower = 2L, upper = 5L,
                                       requires = quote(kernel == "polydot")),
                      makeIntegerParam("C", lower = 5L, upper = 30L,
                                       trafo = function(x) 5*x + 50))
  # Tune    ... This is not working
  lei.rf.res <-  tuneParams("classif.randomForest", lei.classif.task, rdesc, par.set = rf.ps, control = ctrl)
  lei.svm.res <- tuneParams("classif.ksvm", lei.classif.task, rdesc, par.set = svm.ps, control = ctrl)

  # Create Learners
  lei.rf.lrn <-setHyperPars(makeLearner("classif.randomForest",predict.type="prob", fix.factors.prediction = T))
  lei.rf.lrn

  lei.svm.lrn <- setHyperPars(makeLearner("classif.ksvm", predict.type = "prob", fix.factors.prediction = T))
  lei.svm.lrn

  # Train Learners to create models
  lei.rf.mod <- train(lei.rf.lrn, lei.classif.task)
  lei.svm.mod <- train(lei.svm.lrn, lei.classif.task)

  ### predict on the list of rasters
  #### Apply RF model to raster
  training_testing_rasters_wText[[443]]

  for (i in seq_along(training_testing_rasters_wText)){
                                               names(training_testing_rasters_wText[[i]]) <-attributes(lei.rf.mod$learner.model$terms)$term.labels
  }


  rf_pred_training_testing_rasters_wText<- lapply(training_testing_rasters_wText, FUN= function(x){
                                         raster::predict(object = x, model = lei.rf.mod$learner.model, na.rm = T, inf.rm = T)
                                     })

  ### grids 414-443
  grd <- rf_pred_training_testing_rasters_wText[414:443]
  grd
  freq(grd[[1]])

  #### get frequency table for each raster
  freq_grd30 <- lapply(grd, raster::freq)
  freqdf_grd30 <- lapply(freq_grd30, as.data.frame)
  #### get percent tree cover by plot (according to image)


  percent_tree <- lapply(freqdf_grd30, FUN = function(x) {
                                                x[x$value == 4,2]/sum(x[,2])
                                            })


  percent_tree <- unlist(percent_tree)


  percent_tree <- percent_tree[seq(1,60,2)]
  percent_tree

  ### read in andy's accuracy assessment
  grid_accuracy <- read.csv("../RD_UFIA_GridAccuracy/grid_accuracy_assessment_andy.csv", header = T)
  head(grid_accuracy)



  #### get percent tree cover by plot
  d <- grid_accuracy %>%
            group_by(Plot, Cover_Type) %>%
            summarize(number = n()) %>%
            mutate(percent_cover = number/225) %>%
            filter(Cover_Type == "t")

  e <- rbind(d,data.frame(Plot = c(1,11,26), Cover_Type = c("t","t","t"), number = c(0,0,0),percent_cover = c(0,0,0)))
  e <- arrange(e, Plot)

  R_plotNames <- c(10,1,4,29,16,13,14,18,6,22,5,11,15,26,17,9,25,30,20,3,21,23,7,12,2,27,8,19,24,28)
  d <- cbind(e,R_plotNames)
  d
  d <- arrange(d, R_plotNames)


  andy_pctTree <- d$percent_cover

  ### RMSE
  mean((percent_tree - andy_pctTree)^2)^0.5
  png(filename = "naip_rf_grid_prediction.png")
  plot(andy_pctTree,percent_tree, main = "RMSE = 16.5%")
  abline(a=0,b=1,col="red")
  dev.off()


  #### predicting on field data
  plots_101

  ### assess accuracy




  library(dplyr)
  grid_accuracy <- read.csv("../RD_UFIA_GridAccuracy/grid_accuracy_assessment_andy.csv", header = T)
  head(grid_accuracy)


  d <- grid_accuracy %>%
      group_by(Plot, Cover_Type) %>%
      summarize(number = n()) %>%
      mutate(percent_cover = number/225) %>%
      filter(Cover_Type == "t")
  d


#+END_SRC

*** Find whether training data from Wausau can be used in Madison


*** Once I have generally best approach, collect more training data as needed to improve the model accuracy
For example, if the model is calling bright trees grass, collect more
bright tree data points.  Or consider adding some new features to help
discriminate if possible.


*** Save best model
I hope that this is the model that includes both wausau and madison
training data combined.









** Classify urban tree canopy for all urban areas in state

For Urban Area in WI, add features, perform pca, segment image,
classify image

Before I loop through all the WI urban areas I need to make sure I'll
have enough disk space for all the intermediate images.  If I won't I
should delete intermediate steps as I go.

I should also make sure that the rasters I write have a small datatype

*** Update any parameters

- e.g. segmentation parameters

*** START FOR LOOP
For urban.area in WI.urban.areas
#+BEGIN_SRC R
  WI_urban.area.names <- as.character(WI_UrbanAreas$NAME10)
# remove Madison and Wausau, because they are already done.

  for (urban.area in WI_urban.area.names) {


       urban.area.dir.path <- str_extract(urban.area.name, pattern = ".*,") %>%
            str_sub(.,1,-2) %>%
           str_replace_all(., " ", "_") %>%
           str_c(image.dir.path, "/",.)

            dir.create(urban.area.dir.path)

#+END_SRC



*** Crop image
Crop image to the extent of the urban area of interest
#+BEGIN_SRC R
  UrbanArea <- WI_UrbanAreas[str_detect(WI_UrbanAreas$NAME10, urban.area.name), ]



  # Crop image
  e <- extent(UrbanArea)

  inFile <- str_c(image.rd.directory,image.name,".tif")
  outFile <- str_c(urban.area.dir.path,"/urbanExtent.tif")

  gdal_translate(inFile, outFile,
                 projwin = c(xmin(e), ymax(e), xmax(e), ymin(e)))
#+END_SRC

*** tile image
#+BEGIN_SRC R
      ##############
      #######                     Split image for parallel masking
    ##############
  # input
    filename <- str_c(urban.area.dir.path,"/urbanExtent.tif")
  # output
    dir.create(path = str_c(urban.area.dir.path,"/tiles"))


    dims <- as.numeric(
      strsplit(gsub('Size is|\\s+', '', grep('Size is', gdalinfo(filename), value=TRUE)), ',')[[1]]
    )


    # Set the window increment, width and height
    incr <- 1000
    win_width <- 1000
    win_height <- 1000

    # Create a data.frame containing coordinates of the lower-left
    #  corners of the windows, and the corresponding output filenames.
    xy <- setNames(expand.grid(seq(0, dims[1], incr), seq(dims[2], 0, -incr)),
                   c('llx', 'lly'))

    xy$nm <- paste0(xy$llx, '-', dims[2] - xy$lly, '.tif')





    cl <- makeCluster(cores) # e.g. use 4 cores
    clusterExport(cl, c('split_rast', 'xy','filename','win_width','win_height','urban.area.dir.path'))

    system.time({
      parLapply(cl, seq_len(nrow(xy)), function(i) {
        split_rast(filename, paste0(urban.area.dir.path,"/tiles/",xy$nm[i]), xy$llx[i], xy$lly[i], win_width, win_height)
      })
    })

    stopCluster(cl)


#+END_SRC

*** Mask image
- Use WI waterbodies, Urban area extent, and cropland datalayer to
  mask out areas that are not of interest.
- Save masked NAIP in masked_Image folder
#+BEGIN_SRC R
      ##############
      #######              Masking non urban landcover
      ##############
                                            # For every tile of the raster, apply the mask

    tiles_fullName<- list.files(path = str_c(urban.area.dir.path,"/tiles"), full.names = T)
    tiles_shortName <- list.files(path = str_c(urban.area.dir.path,"/tiles"), full.names = F)

    masked.tiles.directory <- str_c(urban.area.dir.path,"/MaskedTiles")
    dir.create(path = masked.tiles.directory, showWarnings = F)

                                            #Options
    # contained urban, don't intersect water = as is
    # contained urban, intersect water = mask water
    # intersect urban, don't intersect water = mask urban
    # intersect urban, intersect water = mask urban & water
  # if none of the above, don't write the raster

    cl <- makeCluster(cores)
    registerDoParallel(cl)

    foreach (i = seq_along(tiles_fullName), .packages = c("raster","sp","rgeos", "stringr")) %dopar% {
        print(i)
        Water_Urban_mask(tile.path = tiles_fullName[i],
                         tile.name = tiles_shortName[i],
                         urban = UrbanArea,
                         water = water)
    }




  ######## Masking Crops



                                            # For every tile of the raster, apply the mask
  tiles_fullName<- list.files(path = str_c(urban.area.dir.path,"/MaskedTiles"), full.names = T)
    tiles_shortName <- list.files(path = str_c(urban.area.dir.path,"/MaskedTiles"), full.names = F)

    crop.masked.tiles.directory <- str_c(urban.area.dir.path,"/CropMaskedTiles")
    dir.create(path = crop.masked.tiles.directory, showWarnings = F)


  cl <- makeCluster(cores)
    registerDoParallel(cl)

    foreach (i = seq_along(tiles_fullName), .packages = c("raster","sp","rgeos", "stringr")) %dopar% {
        print(i)
        Crop_mask(tile.path = tiles_fullName[i],
                  tile.name = tiles_shortName[i],
                  CDL_stack = crops,
		  n_years = n_croplandLayers)
    }


#+END_SRC


*** Add Ratios of image
- Read in Cropped_Image
- Save results in Ratios folder
#+BEGIN_SRC R
  # load masked tiles names
  tile.paths <- list.files(str_c(urban.area.dir.path,"/CropMaskedTiles"), pattern = "*.tif$", full.names = T)
  tile.names <- list.files(str_c(urban.area.dir.path,"/CropMaskedTiles"), pattern = "*.tif$", full.names = F)

                                          # Create directory
  ratio.dir <- str_c(urban.area.dir.path,"/RatioTiles")
  dir.create(path = ratio.dir)

  add_ratios.ndvi <- function(tile.path,tile.name) {
      tile <- stack(tile.path)
      names(tile) <- c("red","green","blue","nir")

      # Create a ratio image for each band
      ratio.brick <- ratio(tile)
      ratio.brick <- ratio.brick*200 # rescale ndvi to save as 'INT1U'
      names(ratio.brick) <- paste0(c("blue","green","red","nir"),rep("_ratio",times = 4))
      ndvi <- ndvi_nodrop(tile, 3, 4)
      ndvi <- (ndvi+1)*100 # rescale ndvi to save as 'INT1U'
      ratio.tile <- raster::stack(tile, ratio.brick, ndvi)
      writeRaster(ratio.tile,
                  filename = paste0(ratio.dir,"/",tile.name),
                  overwrite = T,
                  datatype = 'INT1U')

  }


  cl <- makeCluster(cores)
  registerDoParallel(cl)

  foreach (i = seq_along(tile.paths), .packages = c("raster")) %dopar% {
      add_ratios.ndvi(tile.paths[i], tile.names[i])
  }






#+END_SRC

*** Perform PCA
I need to sample from the entire image to perform the pca, not each
tile separately.
#+BEGIN_SRC R
                                            # make directory for PCA tiles
    dir.create(path = str_c(urban.area.dir.path,"/PCATiles"))

                                            # Sample from every raster
    tile.paths <- list.files(str_c(urban.area.dir.path,"/RatioTiles"), pattern = "*.tif$", full.names = T)
    tile.names <- list.files(str_c(urban.area.dir.path,"/RatioTiles"), pattern = "*.tif$", full.names = F)


    cl <- makeCluster(cores)
    registerDoParallel(cl)

    sr <- foreach (i = seq_along(tile.names), .packages = c("raster"), .combine ="rbind") %dopar% {
        tile <- stack(tile.paths[i])
        s <- sampleRandom(tile, 100)
    }

    colnames(sr) <- c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")

                                            # Perform PCA on sample
    pca <- prcomp(sr, scale = T)

                                            # Apply PC rotation to each tile
    cl <- makeCluster(cores)
    registerDoParallel(cl)

    foreach (i = seq_along(tile.paths), .packages = c("raster")) %dopar% {
        predict_pca(tile.paths[i],
                    pca,
                    n.comps = 3,
                    out.path = paste0(urban.area.dir.path,"/PCATiles/",tile.names[i])
                    )
    }

                                            # scale pca tiles between 0 and 255
    dir.create(str_c(urban.area.dir.path,"/ScaledPCATiles"))
                                            # find min and max
    tile.paths <- list.files(str_c(urban.area.dir.path,"/PCATiles"), pattern = "*.tif$", full.names = T)
    tile.names <- list.files(str_c(urban.area.dir.path,"/PCATiles"), pattern = "*.tif$", full.names = F)

                                            #get min and max

    getRasterMinMax <- function(t.path) {
        tile <- stack(t.path)
        mn <- cellStats(tile, stat = "min")
        mx <- cellStats(tile, stat = "max")
        mnmx <- c(mn,mx)
        return (mnmx)
    }

    cl <- makeCluster(cores)
    registerDoParallel(cl)

    minmax <- foreach(i = seq_along(tile.paths), .packages = "raster", .combine = "rbind") %dopar% {
        getRasterMinMax(tile.paths[i])
    }

    mn <- apply(minmax, 2, min, na.rm = T)[1:3]
    mx <- apply(minmax, 2, max, na.rm = T)[4:6]



    range0255 <- function(tile.path, tile.name){
        r <- stack(tile.path)
        r <- (r - mn)/(mx-mn) * 255
        writeRaster(r, paste0(urban.area.dir.path,"/ScaledPCATiles/",tile.name), overwrite=TRUE)
      }


    registerDoParallel(cl)

    foreach (i = seq_along(tile.paths), .packages = "raster") %dopar% {
        range0255(tile.paths[i], tile.names[i])
    }


  # Remove the unscaled PCA tiles to make some disk space
  do.call(file.remove,list(tile.paths))

#+END_SRC


*** Add Texture?
*** END FOR LOOP
#+BEGIN_SRC R

}

#+END_SRC

*** Segment image

This moves into the directory of the image, then into the directory of
Madison, then runs the segmentation code with arguments that specify
number of pixels per segment and the compactness.


#+BEGIN_SRC sh :results raw
  cd /home/erker/mydata2/DD_NAIP-imagery/madison_wausau_mounthoreb_NAIP

#    for urban_area_dir in *     by uncommenting this line, it will run segmentation on all urban areas
    for urban_area_dir in Madison
    do
     cd $urban_area_dir
     mkdir SegmentationTiles
     python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs.py 60 30 &
     python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs.py 30 15 &
     python /home/erker/mydata2/Pjt_UFIA/fia_segment_cmdArgs.py 105 32 &
    done

#+END_SRC



*** START FOR LOOP AGAIN
*** Calculate zonal statistics for each segment for each tile.
#+BEGIN_SRC R
  # Create Directory to store features (inputs I will use to predict on)
  segment.features.path <- str_c(urban.area.dir.path,"/SegmentFeatureDFs/")

  dir.create(path = segment.features.path)

  seg.tile.paths <- list.files(str_c(urban.area.dir.path, "/SegmentationTiles"), full.names = T)

  seg.tile.paths <- seg.tile.paths[str_detect(seg.tile.paths, "\\.tif$")]


  seg.tile.names <- list.files(str_c(urban.area.dir.path, "/SegmentationTiles"), full.names = F)
  seg.tile.names <- seg.tile.names[str_detect(seg.tile.names, "\\.tif$")]

  #remove ".tif"
  seg.tile.names <- sapply(seg.tile.names, function(s) str_sub(s, end = nchar(s) - 4))


  tiles <- str_extract(seg.tile.paths, "[0-9]+-[0-9]+\\.tif")

  ratio.tile.paths <- sapply(tiles, function(t) {
                                str_c(urban.area.dir.path, "/RatioTiles/", t)
                            })

  extract_segment_features <- function(ratios.tile.path, seg.tile.path, seg.tile.name) {
      r.tile <- stack(ratios.tile.path)
      names(r.tile) <- c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")
      s.tile <- raster(seg.tile.path)

  # Create a data_frame where mean and variances are calculated by zone
      # calculate means
      means <- zonal(x = r.tile, z = s.tile, 'mean', na.rm = F, digits = 1)
      colnames(means)[2:ncol(means)] <- paste0(colnames(means)[2:ncol(means)], "_mean")

                                              # calculate sd
      sds <- zonal(x = r.tile, z = s.tile, 'sd', na.rm = F)
      colnames(sds)[2:ncol(sds)] <- paste0(colnames(sds)[2:ncol(sds)], "_sd")

      d <- merge(means,sds)
      saveRDS(d, file = paste0(segment.features.path, seg.tile.name,".rds"))
      print(seg.tile.name)
  }

    cl <- makeCluster(cores)
    registerDoParallel(cl)

  foreach (i = seq_along(ratio.tile.paths), .packages = c("raster")) %dopar% {
      extract_segment_features(ratio.tile.paths[i], seg.tile.paths[i], seg.tile.names[i])
  }


#+END_SRC












*** Apply best Model to create classification.  I need to modify this!
#+BEGIN_SRC R
        #load models
  models <- readRDS(paste0(urban.area.dir.path,"-Models/models.rds"))

  # unlist models
  models <- unlist(models, recursive = F)


                                          # Apply each model to each Tile

  seg.files <- list.files(str_c(urban.area.dir.path,"-SegTiles"), full.names = T) %>%
      str_extract(., ".*.tif$") %>%
          na.omit()

  tile.names <- list.files(str_c(urban.area.dir.path,"-SegTiles")) %>%
      str_extract(., "[0-9]+-[0-9]+")

  features.files <- list.files(str_c(urban.area.dir.path,"-SegmentFeatureDFs"), full.names = T) %>%
      str_extract(., ".*rds$") %>%
          na.omit()



                                          # Create directories to save tiles into


  dir.create(paste0(image.dd.directory, image.name, "/ClassifiedTiles"))

  # Segmentation Parameters
  foreach(i = seq_along(models)) %do% {
          directory.path <- paste0(urban.area.dir.path,"/ClassifiedTiles/",models[[i]]$task.desc$id)
          dir.create(directory.path)
      }

  # Target (all classes, grass, tree, or impervious)
  foreach(i = seq_along(models)) %do% {
      directory2.path <- paste0(urban.area.dir.path,"/ClassifiedTiles/",models[[i]]$task.desc$id,"/",models[[i]]$learner$id)
      dir.create(directory2.path)
  }


  ## i <- 100
  ## seg.file.path <- seg.files[i]
  ## feature.file.path <- features.files[i]
  ## tile.name <- tile.names[i]
  ## j <- 1

  # I have to apply the correct model to each segmentation
  PredictOnSegmentedRaster <- function(seg.file.path, feature.file.path, tile.name, models) {
      seg <- raster(seg.file.path)
      features <- readRDS(feature.file.path)
      # get models that were built on this set of segmentation parameters
      seg.params <- str_extract(seg.file.path,"N-[0-9]+_C-[0-9]+")
      index <- sapply(models, FUN = function(mod) {
                          str_detect(mod$task.desc$id, pattern = seg.params)
                      })
      mods <- models[index]
      featuresRowsWithNA <- which(is.na(features[,2]))
      complete.features <- features[complete.cases(features),] # svm can't predict with NAs
      foreach(j = seq_along(mods)) %do% {
          mod <- mods[[j]]
          pred <- predict(mod, newdata = complete.features[2:19])
          response <- factor(as.character(pred$data$response), levels = c("G","I","T","O"))
          m <- cbind(zone = complete.features$zone, response)
          m <- left_join(as.data.frame(features["zone"]), as.data.frame(m))
          r <- reclassify(seg, m)
  #        x <- data.frame(ID = 1:4, LandCover = c("G","I","T","O")) %>%
  #            filter(LandCover %in% levels(factor(response)))
  #        levels(r) <- x
          if (ncol(pred$data) > 2) {
              prob <- (pred$data[,grep("prob.*", x = colnames(pred$data))]) # get columns that contain probabilities
              ProbOfClass <- apply(prob, MARGIN = 1, FUN = max)
              m <- cbind(zone = complete.features$zone, ProbOfClass)
              m <- left_join(as.data.frame(features["zone"]), as.data.frame(m))
              p <- reclassify(seg, m)
              r <- stack(r,p)
          }
          path <- paste0(urban.area.dir.path,"/ClassifiedTiles/",mods[[j]]$task.desc$id,"/",mods[[j]]$learner$id,"/",tile.name,".tif")
          writeRaster(r, path, overwrite=TRUE)
      }
  }






  cl<-makeCluster(cores)
  cl <- makeCluster(15)
  registerDoParallel(cl)

  foreach(i = seq_along(seg.files), .packages = c("raster","dplyr","stringr","foreach","mlr")) %dopar% {
      PredictOnSegmentedRaster(seg.files[i], features.files[i], tile.names[i], models)
  }




#+END_SRC

















**** Maybe Delete, but check: Read in Training Polygons and Create DF to build models
Do this for Madison alone, Wausau alone, and Madison Wausau combined.

reads in "TrainingPolygons", "SegmentFeatureDFs".  Writes to "DataForBuildingModels"



#+BEGIN_SRC R
                                          # Read in the training data from the shapefiles
  trainingDSN <- str_c(image.dd.directory, image.name, "-TrainingPolygons")
  trainingShapefiles <- list.files(trainingDSN) %>%
      str_sub(.,end = nchar(.)-4) %>%
          unique()

                                          # Get the tiles from the shapefiles
  tiles <- str_extract(trainingShapefiles, pattern = "[0-9]+-[0-9]+")


  # load training data from shapefiles into memory
  shapelist.data <- lapply(trainingShapefiles, function(shp) {
                          readOGR(dsn = trainingDSN, layer = shp)@data %>%
                                             na.omit() %>%
                                                 rename(zone = DN) %>%
                                                     filter(Class != "N")
                      })
  names(shapelist.data) <- trainingShapefiles


  training.tile.names <- str_extract(trainingShapefiles, "[0-9]+-[0-9]+.tif_N-[0-9]+_C-[0-9]+")

  # Join Training Shapefile Data with extracted features from polygons

  # List all dataframes of extracted features
  extractedFeatures.files <- list.files(str_c(urban.area.dir.path, "-SegmentFeatureDFs"), full.names = T)

  # Select the feature dataframes that have training data
  index <- mapply(training.tile.names, FUN = function(x) str_detect(extractedFeatures.files, x))
  index <- apply(index, MARGIN = 1, FUN = sum) %>% as.logical()
  extractedFeatures <- extractedFeatures.files[index]


  # Group data by segmentation parameters

  uniqueSegParameterSets <- str_extract(trainingShapefiles, pattern = "N-[0-9]+_C-[0-9]+") %>% unique()

  # Create Training Data DF by merging polygon classification with polygon features
  TrainingData <- foreach (i = seq_along(uniqueSegParameterSets)) %do% {
      trn <- shapelist.data[str_detect(names(shapelist.data), uniqueSegParameterSets[i])]
      d.path <- extractedFeatures[str_detect(extractedFeatures, uniqueSegParameterSets[i])]
      trainingData <- list()
      foreach(j = seq_along(d.path)) %do% {
          d <- readRDS(d.path[j])
          trainingData[[j]] <- left_join(trn[[j]],d)
      }
      do.call("rbind",trainingData)
  }

  names(TrainingData) <- uniqueSegParameterSets

                                          # Save this list of dataframes
  dir.create(str_c(image.dd.directory, image.name, "-DataForBuildingModel"))
  saveRDS(TrainingData, file = str_c(image.dd.directory, image.name, "-DataForBuildingModel/trainingData.rds"))
#+END_SRC

**** Maybe Delete, but check: build models/train learners

#+BEGIN_SRC R

    # Read in data
    pathToTrainingDFs <-  str_c(image.dd.directory, image.name, "-DataForBuildingModel/trainingData.rds")

    dat.list <- readRDS(pathToTrainingDFs)

    # Copy Data for Cover specfic models
    dat.list_T <- lapply(dat.list, function(d) {
                             mutate(d, Class = as.character(Class)) %>%
                             mutate(Class = ifelse(Class == "T", Class, "O"))
                         })



    # Create Tasks

    task.list <- lapply(seq_along(dat.list), function(i) {
        makeClassifTask(id = paste0(image.name,"_",names(dat.list)[[i]],"_all"), data = dat.list[[i]], target = "Class") %>%
            dropFeatures("zone")
    })

    tree.task.list <- lapply(seq_along(dat.list_T), function(i) {
        makeClassifTask(id = paste0(image.name,"_",names(dat.list_T)[[i]],"_tree"), data = dat.list_T[[i]], target = "Class", positive = "T") %>%
            dropFeatures("zone")
    })


    task.list <- list(all = task.list, tree = tree.task.list) %>%
        unlist(recursive = F)


                                              # Make Learners
      # RF
      RF_prob <- makeLearner(id = "rf_prob","classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
  #    RF_response <- makeLearner(id = "rf_resp", "classif.randomForest", predict.type = "response", fix.factors.prediction = TRUE)

      SVM_response <- makeLearner(id = "svm_resp", "classif.svm", predict.type = "response", fix.factors.prediction = TRUE)

  #    learner_list <- list(RF_prob = RF_prob, RF_response = RF_response, SVM_response = SVM_response)
      learner_list <- list(RF_prob = RF_prob, SVM_response = SVM_response)

                                              # Train Learners on Tasks, Make models

    cl<-makeCluster(cores)
    registerDoParallel(cl)


      models <- foreach(task = task.list, .packages = "mlr") %:%
          foreach(learner = learner_list) %dopar% {
              train(learner, task)
          }

    dir.create(str_c(urban.area.dir.path, "-Models"))
    saveRDS(models, file = paste0(urban.area.dir.path,"-Models/models.rds"))

#+END_SRC





*** merge classification tiles into single images
Run the below loop from the */ClassifiedTiles directory.

The parenthesis and the "&" at the end of the expression make it run
in parallel.
#+BEGIN_SRC sh

  for r in */*
   do
   (ls -1 $r/*000.tif > $r/tiff_list.txt
   gdal_merge.py -n 0 -v -o $r/combinedClassification.tif --optfile $r/tiff_list.txt) &
   done

#+END_SRC




* old
** 2016-01-22 Workflow
*** Set image name and directory and other variables
 #+BEGIN_SRC R

       ##################
       #################                Specify image name and directory
       ##################

     #  image.name <- "longMadison"

       image.name <- "madison"
       image.rd.directory <- "../RD_NAIP-imagery/"
       image.dd.directory <- "../DD_NAIP-imagery/"

   dir.create(image.dd.directory)


 #+END_SRC

*** Load Libraries
 #+BEGIN_SRC R
   library(gdalUtils)
   library(stringr)
     library(rgdal)
     library(raster)
     library(rgeos)
   #  library(glcm)
   library(plyr)
     library(dplyr)
     library(doParallel)
   library(parallel)
   library(mlr)
 #+END_SRC

*** Load Functions
 #+BEGIN_SRC R
       namedList <- function(...) {
         L <- list(...)
         snm <- sapply(substitute(list(...)),deparse)[-1]
         if (is.null(nm <- names(L))) nm <- snm
         if (any(nonames <- nm=="")) nm[nonames] <- snm[nonames]
         setNames(L,nm)
       }


     ndvi_nodrop <- function(image_w4bands,red_bandnumber,nir_bandnumber,...) {
       red_band <- image_w4bands[[red_bandnumber]]
       nir_band <- image_w4bands[[nir_bandnumber]]
       ndvi <- (nir_band - red_band)/(nir_band + red_band)
       return(ndvi)
     }

       savi_nodrop <- function(image_w4bands,red_bandnumber,nir_bandnumber,L,...) {
         red_band <- image_w4bands[,,red_bandnumber,drop=FALSE]
         nir_band <- image_w4bands[,,nir_bandnumber,drop=FALSE]
         savi <- (nir_band - red_band)/(nir_band + red_band + L) * (1+L)
         return(savi)
       }

       ratio <- function(image_w4bands, numerator_bandNumber) {
         r <- image_w4bands[,,numerator_bandNumber,drop = F] / sum(image_w4bands)
         return(r)
       }

       create_GLCM_layers_parallel <- function(list_rasterlayers, vec_window_sizes, dir, cpus) {
         cl <- makeCluster(spec = cpus, methods = FALSE)
         # Register the cluster with foreach:
         registerDoParallel(cl)
         GLCM_rasters <- foreach(i = 1:length(list_rasterlayers), .packages = c('glcm','raster')) %:%
           foreach (j = 1:length(window_sizes), .packages = c('glcm','raster')) %dopar% {
             raster <- list_rasterlayers[[i]]
             dir <- dir
             window_size <- vec_window_sizes[j]
             w_s <- c(window_size,window_size)
             a <- glcm(raster,shift = dir, window = w_s,na_opt = "center", na_val = 0, asinteger = T)
             names(a)<- paste0(names(list_rasterlayers[[i]]),"_",vec_window_sizes[j],"x",vec_window_sizes[j],"_",names(a))
             a
           }
         stopCluster(cl) # Stops the cluster
         registerDoSEQ()
         return(unlist(GLCM_rasters))
       }

       # Function takes raster stack, samples data, performs pca and returns stack of first n_pcomp bands
         predict_pca_wSampling_parallel <- function(stack, sampleNumber, n_pcomp, nCores = detectCores()-1) {
             sr <- sampleRandom(stack,sampleNumber)
             pca <- prcomp(sr, scale=T)
             beginCluster()
             r <- clusterR(stack, predict, args = list(pca, index = 1:n_pcomp))
             endCluster()
             return(r)
         }
   predict_pca <- function(raster.path, pca, n.comps, out.path) {
       s <- stack(raster.path)
       names(s) <- c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")
       predict(s, pca, index = 1:n.comps, filename = out.path, overwrite=TRUE)
   }




     gdal_polygonizeR <- function(x, outshape=NULL, gdalformat = 'ESRI Shapefile',
                                  pypath=NULL, readpoly=TRUE, quiet=TRUE) {
       if (isTRUE(readpoly)) require(rgdal)
       if (is.null(pypath)) {
         pypath <- Sys.which('gdal_polygonize.py')
       }
       if (!file.exists(pypath)) stop("Can't find gdal_polygonize.py on your system.")
       owd <- getwd()
       on.exit(setwd(owd))
       setwd(dirname(pypath))
       if (!is.null(outshape)) {
         outshape <- sub('\\.shp$', '', outshape)
         f.exists <- file.exists(paste(outshape, c('shp', 'shx', 'dbf'), sep='.'))
         if (any(f.exists))
           stop(sprintf('File already exists: %s',
			toString(paste(outshape, c('shp', 'shx', 'dbf'),
                                       sep='.')[f.exists])), call.=FALSE)
       } else outshape <- tempfile()
       if (is(x, 'Raster')) {
         require(raster)
         writeRaster(x, {f <- tempfile(fileext='.asc')})
         rastpath <- normalizePath(f)
       } else if (is.character(x)) {
         rastpath <- normalizePath(x)
       } else stop('x must be a file path (character string), or a Raster object.')
       system2('python', args=(sprintf('"%1$s" "%2$s" -f "%3$s" "%4$s.shp"',
                                       pypath, rastpath, gdalformat, outshape)))
       if (isTRUE(readpoly)) {
         shp <- readOGR(dirname(outshape), layer = basename(outshape), verbose=!quiet)
         return(shp)
       }
       return(NULL)
     }


     # Create a function to split the raster using gdalUtils::gdal_translate
     split_rast <- function(infile, outfile, llx, lly, win_width, win_height) {
       library(gdalUtils)
       gdal_translate(infile, outfile,
                      srcwin=c(llx, lly - win_height, win_width, win_height))
     }


     Water_Urban_mask <- function(tile.path, tile.name, urban, water) {
                                             # load image tile
         tile <- stack(tile.path)
                                             # get extent image and make sp object
         et <- as(extent(tile), "SpatialPolygons")
         proj4string(et) <- "+init=epsg:26916"
                                             # Mask out non-urban areas
         if(gContainsProperly(urban,et) & !gIntersects(water,et)){
             writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
         } else if (gContainsProperly(urban,et) & gIntersects(water,et)) {
             tile <- mask(tile, water, inverse = T)
             writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
         } else if (gIntersects(urban, et) & !gIntersects(water,et)) {
             tile <- mask(tile, urban)
             writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
         } else if (gIntersects(urban, et) & gIntersects(water,et)) {
             tile <- mask(tile, urban)
             tile <- mask(tile, water, inverse = T)
             writeRaster(tile, filename = str_c(masked.tiles.directory,"/",tile.name), overwrite = T)
         }
     }


   Crop_mask <- function(tile.path, tile.name, CDL_stack){

   tile <- stack(tile.path)
   crops <- crop(CDL_stack, tile)

         # These are the values in the CDL that correspond to non crop cover types and not water
         NonCroppedValues <- c(0,63:65, 81:83, 87:88, 112, 121:124, 131, 141:143, 152, 176, 190, 195)
         # open water is 111

         # find cells that have been assigned crop all three years
         crops[crops %in% NonCroppedValues] <- 0
         crops[!(crops %in% NonCroppedValues)] <- 1
         cropsum <- overlay(crops, fun = sum)

         dis.cropsum <- disaggregate(cropsum, fact = 20)
         dis.cropsum <- resample(dis.cropsum, tile, "ngb")
         masked_tile <- mask(tile, dis.cropsum, maskvalue = 4)

         #               Save Image
         writeRaster(masked_tile, paste0(crop.masked.tiles.directory, "/", tile.name), overwrite = T)
     }


 #+END_SRC

*** read in image
 #+BEGIN_SRC R
     image <- str_c(image.rd.directory,image.name,".tif") %>%
             stack()
     plotRGB(image,4,3,2)
 #+END_SRC
*** Reproject image to epsg:26916
 #+BEGIN_SRC R

   inFile <- str_c(image.rd.directory,image.name,".tif")
   outFile <- str_c(urban.area.dir.path,".tif")
   t_srs <- "EPSG:26916"

   gdalwarp(inFile, outFile, t_srs)



 #+END_SRC


*** Crop image
 Crop image to the extent of the urban area of interest, Madison
 #+BEGIN_SRC R

   ##################
   #################                Specify WI Urban Area Shapefile name and directory (dsn)
   ##################

   US_UrbanAreas.name <- "cb_2013_us_ua10_500k"
   US_UrbanAreas.dsn <- "../RD_US_UrbanAreasShapefile/"


   US_UrbanAreas <- readOGR(dsn = US_UrbanAreas.dsn, layer = US_UrbanAreas.name)

   #WI_UrbanAreas <- US_UrbanAreas[str_detect(US_UrbanAreas$NAME10, "WI"),]

   Madison_UrbanArea <- US_UrbanAreas[str_detect(US_UrbanAreas$NAME10, "Madison, WI"), ]

   Madison_UrbanArea <- spTransform(Madison_UrbanArea, CRS("+init=epsg:26916"))

   # plot(Madison_UrbanArea)


   # Crop image

   e <- extent(Madison_UrbanArea)

   inFile <- str_c(urban.area.dir.path,".tif")
   outFile <- str_c(urban.area.dir.path,"-urbanExtent.tif")

   gdal_translate(inFile, outFile,
                  projwin = c(xmin(e), ymax(e), xmax(e), ymin(e)))


   image <- stack(outFile)

   plotRGB(image, 4, 3, 2)


 #+END_SRC

*** tile image
 #+BEGIN_SRC R
     ##############
     #######                     Split image for parallel masking
   ##############

   filename <- str_c(urban.area.dir.path,"-urbanExtent.tif")

   dims <- as.numeric(
     strsplit(gsub('Size is|\\s+', '', grep('Size is', gdalinfo(filename), value=TRUE)), ',')[[1]]
   )



   # Set the window increment, width and height
   incr <- 1000
   win_width <- 1000
   win_height <- 1000

   # Create a data.frame containing coordinates of the lower-left
   #  corners of the windows, and the corresponding output filenames.
   xy <- setNames(expand.grid(seq(0, dims[1], incr), seq(dims[2], 0, -incr)),
                  c('llx', 'lly'))

   xy$nm <- paste0(xy$llx, '-', dims[2] - xy$lly, '.tif')


   dir.create(path = str_c(urban.area.dir.path,"-tiles"))

   cores <- detectCores()
   cl <- makeCluster(cores) # e.g. use 4 cores
   clusterExport(cl, c('split_rast', 'xy','filename','win_width','win_height','image.dd.directory'))


   system.time({
     parLapply(cl, seq_len(nrow(xy)), function(i) {
       split_rast(filename, paste0(image.dd.directory,"tiles/",xy$nm[i]), xy$llx[i], xy$lly[i], win_width, win_height)
     })
   })

   stopCluster(cl)


 #+END_SRC

*** Mask image
 - Use WI waterbodies, Urban area extent, and cropland datalayer to
   mask out areas that are not of interest.
 - Save masked NAIP in masked_Image folder
 #+BEGIN_SRC R

       ##################
       #################                Specify US Urban Area Shapefile name and directory (dsn)
       ##################

       US_UrbanAreas.name <- "cb_2013_us_ua10_500k"
       US_UrbanAreas.dsn <- "../RD_US_UrbanAreasShapefile/"


       ##################
       #################                Specify water shapefile name and directory (dsn)
       ##################

       water.name <- "WD-Hydro-Waterbody-WBIC-AR-24K"
       water.dsn <- "../RD_WI-waterbody-24k"


       ##################
       #################                Specify Cropland Datalayer name and directory
       ##################

       crop.directory <- "../RD_CroplandDataLayer/"
       crop2011.name <- "CDL_2011_clip_20160106190244_1504737741"
       crop2012.name <- "CDL_2012_clip_20151229124713_1037776543"
       crop2013.name <- "CDL_2013_clip_20151229123327_86558742"
       crop2014.name <- "CDL_2014_clip_20151229123327_86558742"




       ################# load urban area, watershapefile, crop land datalayer

     US_UrbanAreas <- readOGR(dsn = US_UrbanAreas.dsn, layer = US_UrbanAreas.name)
     Madison_UrbanArea <- US_UrbanAreas[str_detect(US_UrbanAreas$NAME10, "Madison, WI"), ]
     Madison_UrbanArea <- spTransform(Madison_UrbanArea, CRS("+init=epsg:26916"))


     water <- readOGR(dsn = water.dsn, layer = water.name)
       water <- spTransform(water, CRS("+init=epsg:26916"))

       crop2011 <- str_c(crop.directory, crop2011.name, ".tif") %>%
           raster()

       crop2012 <- str_c(crop.directory, crop2012.name, ".tif") %>%
           raster()

       crop2013 <- str_c(crop.directory, crop2013.name, ".tif") %>%
           raster()

       crop2014 <- str_c(crop.directory, crop2014.name, ".tif") %>%
           raster()

       crops <- stack(crop2011, crop2012, crop2013, crop2014)



       ##############
       #######              Masking non urban landcover
       ##############


                                             # For every tile of the raster, apply the mask

     tiles_fullName<- list.files(path = str_c(urban.area.dir.path,"-tiles"), full.names = T)
     tiles_shortName <- list.files(path = str_c(urban.area.dir.path,"-tiles"), full.names = F)

     masked.tiles.directory <- str_c(urban.area.dir.path,"-MaskedTiles")
     dir.create(path = masked.tiles.directory, showWarnings = F)

                                             #Options
     # contained urban, don't intersect water = as is
     # contained urban, intersect water = mask water
     # intersect urban, don't intersect water = mask urban
     # intersect urban, intersect water = mask urban & water
   # if none of the above, don't write the raster

     cl <- makeCluster(detectCores())
     registerDoParallel(cl)

     foreach (i = seq_along(tiles_fullName), .packages = c("raster","sp","rgeos", "stringr")) %dopar% {
         print(i)
         Water_Urban_mask(tile.path = tiles_fullName[i],
                          tile.name = tiles_shortName[i],
                          urban = Madison_UrbanArea,
                          water = water)
     }




   ######## Masking Crops



                                             # For every tile of the raster, apply the mask
   tiles_fullName<- list.files(path = str_c(urban.area.dir.path,"-MaskedTiles"), full.names = T)
     tiles_shortName <- list.files(path = str_c(urban.area.dir.path,"-MaskedTiles"), full.names = F)

     crop.masked.tiles.directory <- str_c(urban.area.dir.path,"-CropMaskedTiles")
     dir.create(path = crop.masked.tiles.directory, showWarnings = F)


   cl <- makeCluster(detectCores())
     registerDoParallel(cl)

     foreach (i = seq_along(tiles_fullName), .packages = c("raster","sp","rgeos", "stringr")) %dopar% {
         print(i)
         Crop_mask(tile.path = tiles_fullName[i],
                   tile.name = tiles_shortName[i],
                   CDL_stack = crops)
     }







   ######### Recombine tiles for a check

   combined.masked.image.dir <- str_c(image.dd.directory, image.name, "-MaskedImage")
   dir.create(combined.masked.image.dir)

                                           # read in masked tiles
   tiles.names <- list.files(crop.masked.tiles.directory, pattern = "*.tif$")
   tiles.paths <- list.files(crop.masked.tiles.directory, full.names = T, pattern = "*.tif$")


   r <- lapply(tiles.paths, stack)

   r.args <- r
   r.args$fun <- mean

   combined.image <- do.call(mosaic,r.args)

   writeRaster(combined.image, str_c(combined.masked.image.dir,"/",image.name,"-masked.tif")


   ###############################################################################################################################################

     ## # old way, not in function by tile or parallel


     ##   ###### Mask out non-urban areas
     ##   WI_UrbanAreas <- crop(WI_UrbanAreas, image)
     ## urbanmasked_image <- mask(image, WI_UrbanAreas) %>%
     ##     trim()


     ##   ###### Mask out water
     ##   water <- crop(water, urbanmasked_image)  # Crop the Wisconsin water to image extent
     ## #  water_mask <- rasterize(water, urbanmasked_image, updateValue = NA)
     ##   watermasked_image <- mask(urbanmasked_image, water, inverse = T) %>%
     ##       trim()

     ##   ###### Mask out croplayer
     ## crops <- crop(crops, watermasked_image)
     ## crops <- crop(crops, image)

     ##   # These are the values in the CDL that correspond to non crop cover types and not water
     ##   NonCroppedValues <- c(0,63:65, 81:83, 87:88, 112, 121:124, 131, 141:143, 152, 176, 190, 195)
     ##   # open water is 111

     ##   # find cells that have been assigned crop all three years
     ##   crops[crops %in% NonCroppedValues] <- 0
     ##   crops[!(crops %in% NonCroppedValues)] <- 1
     ##   cropsum <- overlay(crops, fun = sum)

     ##   dis.cropsum <- disaggregate(cropsum, fact = 20)
     ##   dis.cropsum <- resample(dis.cropsum, watermasked_image, "ngb")
     ##   cropped_image <- mask(watermasked_image, dis.cropsum, maskvalue = 4)


     ##   ################################
     ##   ########################               Save Image
     ##   ################################

     ##                                           # create directory in which to save cropped image

     ##   dir.create(path = "CroppedImage", showWarnings = F)

     ##                                           # name of cropped image

     ##   cropped_image.name <- str_c("CroppedImage/cropped_",image.name,".tif")


     ##                                           # save cropped image

     ##   writeRaster(cropped_image, cropped_image.name)









       ## Attempt at faster way to rasterize using gdalUtils





       ##                                         # create directory in which to save cropped image

       ## dir.create(path = "CroppedImage", showWarnings = F)

       ##                                         # name of cropped image

       ## cropped_image.name <- str_c("CroppedImage/cropped_",image.name,".tif")

       ##                                         # Save copy of image that we can burn into
       ## writeRaster(image, cropped_image.name, overwrite = TRUE)




       ## urbanAreas <- system.file("../RD_WI_UrbanAreasShapefile/cb_2013_us_ua10_500k.shp", package = "gdalUtils")

       ## urban_mask <- gdal_rasterize(urbanAreas,
       ##                              cropped_image.name,
       ##                              b = c(1,2,3,4),
       ##                              i = T,
       ##                             output_Raster = T)



       ## water <- system.file("../RD_WI-waterbody-24k/WD-Hydro-Waterbody-WBIC-AR-24K.shp", package = "gdalUtils")

       ## water <- "../RD_WI-waterbody-24k/WD-Hydro-Waterbody-WBIC-AR-24K.shp"


       ## water_mask <- gdal_rasterize(water,
       ##                              cropped_image.name,
       ##                              b = c(1),
       ##                              burn = 1,
       ##                              i = T,
       ##                             output_Raster = T)





 #+END_SRC


*** Add Ratios of image
 - Read in Cropped_Image
 - Save results in Ratios folder
**** for tiled image
 #+BEGIN_SRC R
   # load masked tiles names
   tile.paths <- list.files(str_c(urban.area.dir.path,"-CropMaskedTiles"), pattern = "*.tif$", full.names = T)
   tile.names <- list.files(str_c(urban.area.dir.path,"-CropMaskedTiles"), pattern = "*.tif$", full.names = F)


                                           # Create directory
   ratio.dir <- str_c(urban.area.dir.path,"-RatioTiles")
   dir.create(path = ratio.dir)

   add_ratios.ndvi <- function(tile.path,tile.name) {
       tile <- stack(tile.path)
       names(tile) <- c("red","green","blue","nir")

       # Create a ratio image for each band
       ratio.brick <- ratio(tile)
       names(ratio.brick) <- paste0(c("blue","green","red","nir"),rep("_ratio",times = 4))

       ndvi <- ndvi_nodrop(tile, 3, 4)

       ratio.tile <- raster::stack(tile, ratio.brick, ndvi)
       writeRaster(ratio.tile, filename = paste0(ratio.dir,"/",tile.name))

   }


   cl <- makeCluster(detectCores())
   registerDoParallel(cl)

   foreach (i = seq_along(tile.paths), .packages = c("raster")) %dopar% {
       add_ratios.ndvi(tile.paths[i], tile.names[i])
   }






 #+END_SRC

**** For untiled image
 #+BEGIN_SRC R
     cropped_image.name <- str_c("CroppedImage/cropped_",image.name,".tif")

     image <- stack(cropped_image.name)
     names(image) <- c("red","green","blue","nir")

     # Create a ratio image for each band
     ratios <- lapply(1:4, function(x){
         ratio(image, x)
     })

   ratio.brick <- ratio(image)
   names(ratio.brick) <- str_c(c("blue","green","red","nir"),rep("_ratio",times = 4))

                                               # save ratios
   dir.create("RatiosImage")
   writeRaster(ratio.brick, str_c("RatiosImage/ratios_",image.name,".tif"), overwrite = T)



                                           # Create NDVI image


   ndvi <- ndvi_nodrop(image, 3, 4)

   dir.create("NDVIImage")
   writeRaster(ndvi, str_c("NDVIImage/ndvi_",image.name,".tif"), overwrite = T)
 #+END_SRC

*** Add Texture to image?
 - Read in Cropped_Image
 - Save results in texture folder


*** Perform PCA
**** For Tiled image
 I need to sample from the entire image to perform the pca, not each
 tile separately.
 #+BEGIN_SRC R
                                           # make directory for PCA tiles
   dir.create(path = str_c(urban.area.dir.path,"/PCATiles"))

                                           # Sample from every raster
   tile.paths <- list.files(str_c(urban.area.dir.path,"-RatioTiles"), pattern = "*.tif$", full.names = T)
   tile.names <- list.files(str_c(urban.area.dir.path,"-RatioTiles"), pattern = "*.tif$", full.names = F)


   cl <- makeCluster(detectCores())
   registerDoParallel(cl)

   sr <- foreach (i = seq_along(tile.names), .packages = c("raster"), .combine ="rbind") %dopar% {
       tile <- stack(tile.paths[i])
       s <- sampleRandom(tile, 100)
   }

   colnames(sr) <- c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")

                                           # Perform PCA on sample
   pca <- prcomp(sr, scale = T)

                                           # Apply PC rotation to each tile
   cl <- makeCluster(detectCores())
   registerDoParallel(cl)

   foreach (i = seq_along(tile.paths), .packages = c("raster")) %dopar% {
       predict_pca(tile.paths[i],
                   pca,
                   n.comps = 3,
                   out.path = paste0(urban.area.dir.path,"/PCATiles/",tile.names[i])
                   )
   }

                                           # scale pca tiles between 0 and 255
   dir.create(str_c(urban.area.dir.path,"/ScaledPCATiles"))
                                           # find min and max
   tile.paths <- list.files(str_c(urban.area.dir.path,"/PCATiles"), pattern = "*.tif$", full.names = T)
   tile.names <- list.files(str_c(urban.area.dir.path,"/PCATiles"), pattern = "*.tif$", full.names = F)

                                           #get min and max

   getRasterMinMax <- function(t.path) {
       tile <- stack(t.path)
       mn <- cellStats(tile, stat = "min")
       mx <- cellStats(tile, stat = "max")
       mnmx <- c(mn,mx)
       return (mnmx)
   }

   cl <- makeCluster(detectCores())
   registerDoParallel(cl)

   minmax <- foreach(i = seq_along(tile.paths), .packages = "raster", .combine = "rbind") %dopar% {
       getRasterMinMax(tile.paths[i])
   }

   mn <- apply(minmax, 2, min, na.rm = T)[1:3]
   mx <- apply(minmax, 2, max, na.rm = T)[4:6]


   registerDoParallel(cl)

   foreach (i = seq_along(tile.paths), .packages = "raster") %dopar% {
       range0255(tile.paths[i], tile.names[i])
   }


   range0255 <- function(tile.path, tile.name){
       r <- stack(tile.path)
       r <- (r - mn)/(mx-mn) * 255
       writeRaster(r, paste0(image.dd.directory, image.name,"/ScaledPCATiles/",tile.name), overwrite=TRUE)
     }






 #+END_SRC

**** Merge All PCA into one image (mainly to look at)
 Again, I'm having issues running this from org mode.  Just copy in
 command line.
 #+BEGIN_SRC sh

 # make list of files to merge
 ls -1 ../DD_NAIP-imagery/madison/ScaledPCATiles/*.tif

 # Merge tiles
 gdal_merge.py -n 0 -v -o ../DD_NAIP-imagery/madison-ScaledPCA.tif --optfile tiff_list.txt

 #+END_SRC

**** For untiled image
 - Read in Cropped_Image (specify NAIP)
 - Save results in PCA folder

 #+BEGIN_SRC R
                                             # Read in Cropped Image


   ratios <-str_c("RatiosImage/ratios_",image.name,".tif") %>%
       stack()
   ndvi <- str_c("NDVIImage/ndvi_",image.name,".tif") %>%
       stack()
   image <- cropped_image.name <- str_c("CroppedImage/cropped_",image.name,".tif") %>%
       stack()


   combined.image <- stack(image, ratios, ndvi)


                                             # Perform PCA
   pca.image <- predict_pca_wSampling_parallel(stack = combined.image,
                                               sampleNumber = 10000,
                                               n_pcomp =  5)

   # plot(pca.image)


   range0255 = function(raster){
       mn <- cellStats(raster, stat = "min")
       mx <- cellStats(raster, stat = "max")
       (raster - mn)/(mx-mn) * 255
   }

   pca.image <- range0255(pca.image)


   # plotRGB(pca.image, 1,2,3, stretch = "lin")




                                             # Save PCA


   dir.create("PCAImage")
     writeRaster(pca.image,str_c("PCAImage/pca_",image.name,".tif"), overwrite = T)
 #+END_SRC



*** Segment image
**** For Tiles
 #+BEGIN_SRC sh

 python fia_segment1.py &
 python fia_segment2.py &
 python fia_segment3.py &
 python fia_segment4.py &

 #+END_SRC

 In the terminal I run the scripts fia_segmentX.py.

 There are four scripts, one for each average pixel size and
 compactness combination that I want to investigate.  This saves time,
 by running four scripts at once.  Don't forget to run them in
 background with "&"



**** Smaller image (longMadison) to determine the best parameters for segmentation
 for x number of segmentation parameter combinations
 - Read in PCA_image
 -  Save results in segmentation folder

 As of now (2016-01-06) I haven't been able to run this script within
 org mode.  Instead I open the script separately and run it.  I think
 there are memory issues that cause the segmentation to fail with
 certain combinations of number of pixels per segment and compactness values.

 #+BEGIN_SRC sh
   python Python/fia_segment.py
 #+END_SRC

*** Polygonize Segments
**** of a subset of tiles for training
 #+BEGIN_SRC R
                                             # make directory for Polygonized Segments
   PolygonDir <- str_c(urban.area.dir.path,"-PolygonizedSegTiles")
   dir.create(path = PolygonDir)

                                             # Sample from every raster
     tile.paths <- list.files(str_c(urban.area.dir.path,"-SegTiles"), pattern = "*.tif$", full.names = T)
     tile.names <- list.files(str_c(urban.area.dir.path,"-SegTiles"), pattern = "*.tif$", full.names = F)


   # I'm going to sample from these to polygonize and use for training

                                           # get just the tile name
   b <- str_extract(tile.names,"[0-9]+-[0-9]+.tif")
   c <- unique(b)
                                           # randomly sample from those
   set.seed(3)
   d <- sample(c, 10)
   # select the segmentations of the sample of segmentations

   index <- sapply(d, function(d) grepl(d, x = tile.paths))
   index <- apply(index, MARGIN = 1, FUN = sum)
   tile.pathsToSample <- tile.paths[as.logical(index)]
   tile.namesToSample <- tile.names[as.logical(index)]

   cl <- makeCluster(detectCores())
   registerDoParallel(cl)

   foreach (i = seq_along(tile.pathsToSample), .packages = c("raster","sp","gdalUtils")) %dopar% {
       seg <- raster(tile.pathsToSample[i])
       segPoly <- gdal_polygonizeR(seg)
       segPoly$Class <- "N"
       writeOGR(obj = segPoly,
		dsn = PolygonDir,
		layer = tile.namesToSample[i],
		driver = "ESRI Shapefile",
		overwrite = T)
   }

 #+END_SRC
**** of a single small scene
 #+BEGIN_SRC R
   segPath <- paste0("SegmentationImage/")
   segNames <- list.files(path = segPath)

   lapply(segNames, function(x) {
       seg <- raster(paste0(segPath,x))
       s <- gdal_polygonizeR(seg)
       writeOGR(obj = s, dsn = paste0("Seg_Shapefiles"), layer = x,
		driver = "ESRI Shapefile",
		overwrite=TRUE)
   })
 #+END_SRC

 Segmentations that I will use:



 |   | Average Pixel number | Compactness |
 |---+----------------------+-------------|
 |   |                   15 |          10 |
 |   |                   30 |          15 |
 |   |                   45 |          31 |
 |   |                   60 |          30 |
 |   |                  105 |          32 |
 |   |                      |             |


 think I'll try the best compactness from pixel number 15, 30, 60,
 and 105.  Other sizes looked good too, but I think 105 is near the
 upper limit.  Smaller than 15 doesn't make much sense either.


*** Manually classify segments for training data in qgis
 Note on Google Earth and the NAIP image:
 - While the surface of the two images agree (ground cover, grass,
   roads etc), objects that have height such as trees and buildings are
   not in the same location. Parallax.  The errors are often pretty
   high, around 10m or more.
 - This is easily seen by loading the image into Google earth using qgis.
 - Implication:
   - When using google earth to assess accuracy, there will errors at
     scales smaller than this error.  However if aggregated over larger
     area (the 100m blocks) the accuracy should be relatively high.
     This is because the relative cover of the two images is very
     similar over the whole extent, however there is shift.

**** Load Segments and original image into qgis

**** classify features
**** Save results in TrainingData folder
 #+BEGIN_SRC R

   dir.create(path = str_c(urban.area.dir.path, "-TrainingPolygons")

 #+END_SRC
 names are the same as the segmentation polygons, but they have
 additional Classes in their attributes

*** Build models
**** Calculate zonal statistics for each segment for each tile.
 #+BEGIN_SRC R
   # Create Directory to store features (inputs I will use to predict on)
   segment.features.path <- str_c(image.dd.directory, image.name,"-SegmentFeatureDFs/")

   dir.create(path = segment.features.path)

   seg.tile.paths <- list.files(str_c(image.dd.directory, image.name, "-SegTiles"), full.names = T)

   seg.tile.paths <- seg.tile.paths[str_detect(seg.tile.paths, "\\.tif$")]


   seg.tile.names <- list.files(str_c(image.dd.directory, image.name, "-SegTiles"), full.names = F)
   seg.tile.names <- seg.tile.names[str_detect(seg.tile.names, "\\.tif$")]

   #remove ".tif"
   seg.tile.names <- sapply(seg.tile.names, function(s) str_sub(s, end = nchar(s) - 4))


   tiles <- str_extract(seg.tile.paths, "[0-9]+-[0-9]+\\.tif")

   ratio.tile.paths <- sapply(tiles, function(t) {
                                 str_c(image.dd.directory, image.name, "-RatioTiles/", t)
                             })

   extract_segment_features <- function(ratios.tile.path, seg.tile.path, seg.tile.name) {
       r.tile <- stack(ratios.tile.path)
       names(r.tile) <- c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")
       s.tile <- raster(seg.tile.path)

   # Create a data_frame where mean and variances are calculated by zone
       # calculate means
       means <- zonal(x = r.tile, z = s.tile, 'mean', na.rm = F, digits = 1)
       colnames(means)[2:ncol(means)] <- paste0(colnames(means)[2:ncol(means)], "_mean")

                                               # calculate sd
       sds <- zonal(x = r.tile, z = s.tile, 'sd', na.rm = F)
       colnames(sds)[2:ncol(sds)] <- paste0(colnames(sds)[2:ncol(sds)], "_sd")

       d <- merge(means,sds)
       saveRDS(d, file = paste0(segment.features.path, seg.tile.name,".rds"))
       print(seg.tile.name)
   }

     cl <- makeCluster(detectCores())
     registerDoParallel(cl)

   foreach (i = seq_along(ratio.tile.paths), .packages = c("raster")) %dopar% {
       extract_segment_features(ratio.tile.paths[i], seg.tile.paths[i], seg.tile.names[i])
   }


 #+END_SRC

**** Read in Training Polygons and Create DF to build models

 reads in "TrainingPolygons", "SegmentFeatureDFs".  Writes to "DataForBuildingModels"

 #+BEGIN_SRC R
                                           # Read in the training data from the shapefiles
   trainingDSN <- str_c(image.dd.directory, image.name, "-TrainingPolygons")
   trainingShapefiles <- list.files(trainingDSN) %>%
       str_sub(.,end = nchar(.)-4) %>%
           unique()

                                           # Get the tiles from the shapefiles
   tiles <- str_extract(trainingShapefiles, pattern = "[0-9]+-[0-9]+")


   # load training data from shapefiles into memory
   shapelist.data <- lapply(trainingShapefiles, function(shp) {
                           readOGR(dsn = trainingDSN, layer = shp)@data %>%
                                              na.omit() %>%
                                                  rename(zone = DN) %>%
                                                      filter(Class != "N")
                       })
   names(shapelist.data) <- trainingShapefiles


   training.tile.names <- str_extract(trainingShapefiles, "[0-9]+-[0-9]+.tif_N-[0-9]+_C-[0-9]+")

   # Join Training Shapefile Data with extracted features from polygons

   # List all dataframes of extracted features
   extractedFeatures.files <- list.files(str_c(urban.area.dir.path, "-SegmentFeatureDFs"), full.names = T)

   # Select the feature dataframes that have training data
   index <- mapply(training.tile.names, FUN = function(x) str_detect(extractedFeatures.files, x))
   index <- apply(index, MARGIN = 1, FUN = sum) %>% as.logical()
   extractedFeatures <- extractedFeatures.files[index]


   # Group data by segmentation parameters

   uniqueSegParameterSets <- str_extract(trainingShapefiles, pattern = "N-[0-9]+_C-[0-9]+") %>% unique()

   # Create Training Data DF by merging polygon classification with polygon features
   TrainingData <- foreach (i = seq_along(uniqueSegParameterSets)) %do% {
       trn <- shapelist.data[str_detect(names(shapelist.data), uniqueSegParameterSets[i])]
       d.path <- extractedFeatures[str_detect(extractedFeatures, uniqueSegParameterSets[i])]
       trainingData <- list()
       foreach(j = seq_along(d.path)) %do% {
           d <- readRDS(d.path[j])
           trainingData[[j]] <- left_join(trn[[j]],d)
       }
       do.call("rbind",trainingData)
   }

   names(TrainingData) <- uniqueSegParameterSets

                                           # Save this list of dataframes
   dir.create(str_c(image.dd.directory, image.name, "-DataForBuildingModel"))
   saveRDS(TrainingData, file = str_c(image.dd.directory, image.name, "-DataForBuildingModel/trainingData.rds"))
 #+END_SRC

**** Build SVM and RF models
 #+BEGIN_SRC R

   # For each combination of segmentation parameters:




   # Read in data
   pathToTrainingDFs <-  str_c(image.dd.directory, image.name, "-DataForBuildingModel/trainingData.rds")

   dat.list <- readRDS(pathToTrainingDFs)

   # Copy Data for Cover specfic models
   dat.list_T <- lapply(dat.list, function(d) {
                            mutate(d, Class = as.character(Class)) %>%
                            mutate(Class = ifelse(Class == "T", Class, "O"))
			})

   dat.list_G <- lapply(dat.list, function(d) {
                            mutate(d, Class = as.character(Class)) %>%
                            mutate(Class = ifelse(Class == "G", Class, "O"))
			})

   dat.list_I <- lapply(dat.list, function(d) {
                            mutate(d, Class = as.character(Class)) %>%
                            mutate(Class = ifelse(Class == "I", Class, "O"))
			})


   # Create Tasks

   task.list <- lapply(seq_along(dat.list), function(i) {
       makeClassifTask(id = paste0(image.name,"_",names(dat.list)[[i]],"_all"), data = dat.list[[i]], target = "Class") %>%
           dropFeatures("zone")
   })

   tree.task.list <- lapply(seq_along(dat.list_T), function(i) {
       makeClassifTask(id = paste0(image.name,"_",names(dat.list_T)[[i]],"_tree"), data = dat.list_T[[i]], target = "Class", positive = "T") %>%
           dropFeatures("zone")
   })

   grass.task.list <- lapply(seq_along(dat.list_G), function(i) {
       makeClassifTask(id = paste0(image.name,"_",names(dat.list_G)[[i]],"_grass"), data = dat.list_G[[i]], target = "Class", positive = "G") %>%
           dropFeatures("zone")
   })

   impervious.task.list <- lapply(seq_along(dat.list_I), function(i) {
       makeClassifTask(id = paste0(image.name,"_",names(dat.list_I)[[i]],"_impervious"), data = dat.list_I[[i]], target = "Class", positive = "I") %>%
           dropFeatures("zone")
   })

   task.list <- list(all = task.list, tree = tree.task.list, grass = grass.task.list, impervious = impervious.task.list) %>%
       unlist(recursive = F)


                                             # Make Learners
     # RF
     RF_prob <- makeLearner(id = "rf_prob","classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
     RF_response <- makeLearner(id = "rf_resp", "classif.randomForest", predict.type = "response", fix.factors.prediction = TRUE)
     SVM_prob <- makeLearner(id = "svm_prob", "classif.svm", predict.type = "prob", fix.factors.prediction = TRUE)
     SVM_response <- makeLearner(id = "svm_resp", "classif.svm", predict.type = "response", fix.factors.prediction = TRUE)

     learner_list <- list(RF_prob = RF_prob, RF_response = RF_response, SVM_prob = SVM_prob, SVM_response = SVM_response)


                                             # Train Learners on Tasks, Make models
   nCores <- detectCores()
     cl<-makeCluster(nCores)
     registerDoParallel(cl)


     models <- foreach(task = task.list, .packages = "mlr") %:%
         foreach(learner = learner_list) %dopar% {
             train(learner, task)
         }

   dir.create(str_c(urban.area.dir.path, "-Models"))
   saveRDS(models, file = paste0(urban.area.dir.path,"-Models/models.rds"))

 #+END_SRC

*** Assess Model Accuracy
*** Classify Image
**** Apply models to create classifications
 #+BEGIN_SRC R
         #load models
   models <- readRDS(paste0(urban.area.dir.path,"-Models/models.rds"))

   # unlist models
   models <- unlist(models, recursive = F)


                                           # Apply each model to each Tile

   seg.files <- list.files(str_c(urban.area.dir.path,"-SegTiles"), full.names = T) %>%
       str_extract(., ".*.tif$") %>%
           na.omit()

   tile.names <- list.files(str_c(urban.area.dir.path,"-SegTiles")) %>%
       str_extract(., "[0-9]+-[0-9]+")

   features.files <- list.files(str_c(urban.area.dir.path,"-SegmentFeatureDFs"), full.names = T) %>%
       str_extract(., ".*rds$") %>%
           na.omit()



   # Create directories to save tiles into
   # Segmentation Parameters
   foreach(i = seq_along(models)) %do% {
           directory.path <- paste0(urban.area.dir.path,"/ClassifiedTiles/",models[[i]]$task.desc$id)
           dir.create(directory.path)
       }

   # Target (all classes, grass, tree, or impervious)
   foreach(i = seq_along(models)) %do% {
       directory2.path <- paste0(urban.area.dir.path,"/ClassifiedTiles/",models[[i]]$task.desc$id,"/",models[[i]]$learner$id)
       dir.create(directory2.path)
   }



   # I have to apply the correct model to each segmentation
   PredictOnSegmentedRaster <- function(seg.file.path, feature.file.path, tile.name, models) {
       seg <- raster(seg.file.path)
       features <- readRDS(feature.file.path)
       # get models that were built on this set of segmentation parameters
       seg.params <- str_extract(seg.file.path,"N-[0-9]+_C-[0-9]+")
       index <- sapply(models, FUN = function(mod) {
                           str_detect(mod$task.desc$id, pattern = seg.params)
                       })
       mods <- models[index]
       featuresRowsWithNA <- which(is.na(features[,2]))
       complete.features <- features[complete.cases(features),] # svm can't predict with NAs
       foreach(j = seq_along(mods)) %do% {
           mod <- mods[[j]]
           pred <- predict(mod, newdata = complete.features[2:19])
           response <- factor(as.character(pred$data$response), levels = c("G","I","T","O"))
           m <- cbind(zone = complete.features$zone, response)
           m <- left_join(as.data.frame(features["zone"]), as.data.frame(m))
           r <- reclassify(seg, m)
           x <- data.frame(ID = 1:4, LandCover = c("G","I","T","O")) %>%
               filter(LandCover %in% levels(factor(response)))
           levels(r) <- x
           path <- paste0(urban.area.dir.path,"/ClassifiedTiles/",mods[[j]]$task.desc$id,"/",mods[[j]]$learner$id,"/",tile.name,".tif")
           writeRaster(r, path, overwrite=TRUE)
       }
   }

   dir.create(paste0(image.dd.directory, image.name, "/ClassifiedTiles"))

   cl<-makeCluster(detectCores())
   registerDoParallel(cl)

   foreach(i = seq_along(seg.files), .packages = c("raster","dplyr","stringr","foreach","mlr")) %dopar% {
       PredictOnSegmentedRaster(seg.files[i], features.files[i], tile.names[i], models)
   }


   foreach(i = 55:length(seg.files), .packages = c("raster","dplyr","stringr","foreach","mlr")) %dopar% {
       PredictOnSegmentedRaster(seg.files[i], features.files[i], tile.names[i], models)
       i
   }
 #+END_SRC

**** merge classifications into single images
 Again, I'm having issues running this from org mode.  Just copy in
 command line.
 #+BEGIN_SRC sh

   # make list of files to merge
   #ls -1 ../DD_NAIP-imagery/madison/ScaledPCATiles/*.tif > tiff_list.txt

   # Merge tiles
   #gdal_merge.py -n 0 -v -o ../DD_NAIP-imagery/madison-ScaledPCA.tif --optfile tiff_list.txt

   for r in */*
    do
    ls -1 $r/*.tif > tiff_list.txt
    gdal_merge.py -n 0 -v -o $r/combinedClassification.tif --optfile tiff_list.txt
    done

 #+END_SRC

**** make values less than zero in combined PCA image NA

 I maybe shouldn't do this.  Perhaps qgis is just interpreting NA as
 very low

 #+BEGIN_SRC R

   r <- stack(paste0(urban.area.dir.path,"-ScaledPCA.tif"))

   ZeroToNA <- function(x) {
       x[x <= 0] <- NA
       return(x)
   }

   beginCluster(cores)

   rn <- clusterR(r, calc, args = list(fun = ZeroToNA))

   endCluster()

   writeRaster(rn, paste0(urban.area.dir.path,"-ScaledPCA_0min.tif"), overwrite = T)

   png(paste0("figs/",image.name,"-ScaledPCA_0min.png"))
   plotRGB(rn,1,2,3)
   dev.off()


 #+END_SRC
 [[file:figs/*-ScaledPCA.png]]
 [[file:figs/*-ScaledPCA*.png]]

** Delete this scratch
 #+BEGIN_SRC R

   r <- raster()

   r[] <- runif(ncell(r))

   r[r[] < .5] <- 2
   plot(r)
   summary(values(r))
 #+END_SRC




*** Assess Accuracy of image
**** Load Classifications
**** Assess Accuracy, 3 methods
**** Save results in Accuracy Assessments folder
 consider cases with different numbers of classes

*** Compare methods
**** Load Accuracy Assessments
**** Make table of accuracy assessments
 - this should be a function



** Old way
 --------------------------------------------------------------------------------
*** Load Libraries

*** beginning
*** Read in Image
**** NAIP
     #+BEGIN_SRC R :results graphics :file naip.png
   naip <- stack("../RD_NAIP-imagery/madison.tif")
 #  plotRGB(naip,4,3,2, stretch = "lin")
     #+END_SRC

***** Crop NAIP
      #+BEGIN_SRC R
	##   plotRGB(naip,4,3,2, stretch = "lin")
	## e <- drawExtent()
	## med_naip <- crop(naip, e)
	## plotRGB(med_naip, 4,3,2, stretch = "lin")
	## writeRaster(x = med_naip, filename= "NAIP/med_Mad.tif")
	small_naip <- stack("NAIP/small_Mad.tif")
	med_naip <- stack("NAIP/med_Mad.tif")
      #+END_SRC


**** PAN_SPOT
     #+BEGIN_SRC R :results graphics :file panspot.png
   panspot <- stack("../RD_PansharpenedSPOT/geomatica_SPOT_panshp.tif")
   plotRGB(panspot,4,3,2, stretch = "lin")
     #+END_SRC

*** Read in Municipal boundaries or Urban Census Tract boundaries
    #+BEGIN_SRC R :exports both :results graphics :file studycities.png :tangle yes
   panspot_path <- "../RD_PansharpenedSPOT/geomatica_SPOT_panshp.tif"
   municipalities <- readOGR("../RD_WI-Municipalities-Shapefiles", "MCD")

                                           # Filter out Madison, Middleton, and Shorewood hills
   study.cities <- municipalities[municipalities$NAME10 == "MADISON" | municipalities$NAME10 == "Madison" | municipalities$NAME10 == "Shorewood Hills" | municipalities$NAME10 == "Middleton", ]
   study.cities <- spTransform(study.cities, CRS("+init=epsg:26916"))
   plot(study.cities)
    #+END_SRC

*** Processing NAIP
    #+BEGIN_SRC R
      image <- small_naip
      image <- stack("NAIP/madison_naip_croppedToCityExtent.tif")
      directory <- "NAIP/"
      imagename <- "small_naip"
      names(image) <- c("blue", "green", "red", "nir")
    #+END_SRC

**** crop naip image
     #+BEGIN_SRC R :exports both
   urban.image <- crop(image, study.cities)
   png(paste0(directory,"madison_",imagename))
   plotRGB(urban.image,4,3,2, stretch = "lin")
   dev.off()
     #+END_SRC

**** save cropped naip image
     #+BEGIN_SRC R :exports both :tangle yes
   ## library(raster)
   ## writeRaster(x = urban.naip, "NAIP/madison_naip_croppedToCityExtent.tif")
     #+END_SRC

**** Add Layers to image
***** add ratios
      #+BEGIN_SRC R
	## namedList <- function(...) {
	##   L <- list(...)
	##   snm <- sapply(substitute(list(...)),deparse)[-1]
	##   if (is.null(nm <- names(L))) nm <- snm
	##   if (any(nonames <- nm=="")) nm[nonames] <- snm[nonames]
	##   setNames(L,nm)
	## }

	## savi_nodrop <- function(image_w4bands,red_bandnumber,nir_bandnumber,L,...) {
	##   red_band <- image_w4bands[,,red_bandnumber,drop=FALSE]
	##   nir_band <- image_w4bands[,,nir_bandnumber,drop=FALSE]
	##   savi <- (nir_band - red_band)/(nir_band + red_band + L) * (1+L)
	##   return(savi)
	## }

	## ratio <- function(image_w4bands, numerator_bandNumber) {
	##   r <- image_w4bands[,,numerator_bandNumber,drop = F] / sum(image_w4bands)
	##   return(r)
	## }

	calc_ndvi <- function(x){(x$nir - x$red)/(x$nir + x$red)}

	beginCluster()
	ndvi <- clusterR(x = image, fun = calc_ndvi)
	endCluster()

	writeRaster(x = ndvi, paste0(directory,"NDVI_",imagename,".tif"), overwrite = T)



      #+END_SRC

***** add texture (glcm)
      #+BEGIN_SRC R
	ndvi <- raster("NAIP/NDVI_madison_naip.tif")
	imagename <- "madison_naip"
	statistics <- c("variance", "homogeneity", "contrast",
			"dissimilarity", "entropy","second_moment",
			"correlation")

	shift <- list(c(1,-1))
	window <- list(c(3,3),c(5,5))


	cl <- makeCluster(4 )

	registerDoParallel(cl)


	# I changed the code from %dopar% to %do% because I don't think I'll have enough memory to run in parallel.  Running on the server, I'll need to change back to %dopar%
	foreach(j = 1:length(statistics), .packages=c('raster', 'glcm')) %do% {
          for (i in 1:length(window)) {
              GLCM <- glcm(ndvi, window=window[[i]], statistics=statistics[j],
                           shift = shift, na_opt="ignore")
              file <- paste0(directory,"texture/",imagename,"_",names(GLCM),"_window",window[[i]][1],".tif")
              writeRaster(GLCM, filename = file, type = "GTIFF")
            }
          }

	stopCluster(cl)




      #+END_SRC

**** Perform PCA and select vegetation/urban related components
**** Segment Image
**** In QGIS spend X amount of time finding training segments for classification


     #+BEGIN_SRC python

   from skimage.segmentation import slic, mark_boundaries
   import gdal
   import numpy as np
   import matplotlib.pyplot as plt
   from skimage.util import img_as_float
   from skimage import io

   image_path = r"C:\_urban_fia\fia_data\SPOT_subset_subset_rgb.tif"
   image = io.imread(image_path)
   image = image[:, : , 0:-1]
   print image

   def build_tiff(source_raster_path, target_raster_path, image_array):

       source_raster = gdal.Open(source_raster_path)
       bands = source_raster.RasterCount
       columns = source_raster.RasterXSize
       rows = source_raster.RasterYSize
       driver = gdal.GetDriverByName("GTiff")
       new_image = driver.Create(target_raster_path, columns, rows, 1, gdal.GDT_Float64)
       new_image.SetGeoTransform(source_raster.GetGeoTransform())
       new_image.SetProjection(source_raster.GetProjectionRef())

       new_image.GetRasterBand(1).WriteArray(image_array)
       '''
       for band in range(bands):
           new_image.GetRasterBand(band+1).WriteArray(image_array[band, :, :])
       '''

       return

   source_raster = gdal.Open('pca_data.tif')
   bands = source_raster.RasterCount
   columns = source_raster.RasterXSize
   rows = source_raster.RasterYSize
   data_type = source_raster.GetRasterBand(1).DataType  # 2 = UInt16

   array = source_raster.ReadAsArray()

   array = array.transpose(1, 2, 0)

   array = array[:, :, 1:]  # Use only principal components 2-4
   num_segs = rows * columns / 20

   print 'Making segments'
   segments = slic(array, n_segments=num_segs, compactness=10.0, enforce_connectivity=True)
   print 'Made dem segments'

   build_tiff('pca_data.tif', 'segments.tif', segments)
   print 'Tiff made successfully'

   fig = plt.figure('%d Segments' % num_segs)
   ax = fig.add_subplot(1,1,1)
   ax.imshow(mark_boundaries(image, segments))
   plt.axis("off")
   plt.show()


     #+END_SRC

     #+BEGIN_SRC R

   2*pt(q= 2, df = 1, lower.tail = T, )


     #+END_SRC











*** Functions
   #+BEGIN_SRC R
 extract_bind_df_addclass <- function(x) {
   w <- raster::extract(image,x)
   w <- do.call("rbind",w)
   w <- data.frame(w)
   w$Class <- names(x)
   return(w)
 }

 clip<-function(raster,shape) {
           a1_crop<-crop(raster,shape)
           step1<-rasterize(shape,raster)
           a1_crop*step1}

 namedList <- function(...) {
   L <- list(...)
   snm <- sapply(substitute(list(...)),deparse)[-1]
   if (is.null(nm <- names(L))) nm <- snm
   if (any(nonames <- nm=="")) nm[nonames] <- snm[nonames]
   setNames(L,nm)
 }

 ndvi_nodrop <- function(image_w4bands,red_bandnumber,nir_bandnumber,...) {
   red_band <- image_w4bands[,,red_bandnumber,drop=FALSE]
   nir_band <- image_w4bands[,,nir_bandnumber,drop=FALSE]
   ndvi <- (nir_band - red_band)/(nir_band + red_band)
   return(ndvi)
 }

 savi_nodrop <- function(image_w4bands,red_bandnumber,nir_bandnumber,L,...) {
   red_band <- image_w4bands[,,red_bandnumber,drop=FALSE]
   nir_band <- image_w4bands[,,nir_bandnumber,drop=FALSE]
   savi <- (nir_band - red_band)/(nir_band + red_band + L) * (1+L)
   return(savi)
 }

 ratio <- function(image_w4bands, numerator_bandNumber) {
   r <- image_w4bands[,,numerator_bandNumber,drop = F] / sum(image_w4bands)
   return(r)
 }

 create_GLCM_layers_parallel <- function(list_rasterlayers, vec_window_sizes, dir, cpus) {
   cl <- makeCluster(spec = cpus, methods = FALSE)
   # Register the cluster with foreach:
   registerDoParallel(cl)
   GLCM_rasters <- foreach(i = 1:length(list_rasterlayers), .packages = c('glcm','raster')) %:%
     foreach (j = 1:length(window_sizes), .packages = c('glcm','raster')) %dopar% {
       raster <- list_rasterlayers[[i]]
       dir <- dir
       window_size <- vec_window_sizes[j]
       w_s <- c(window_size,window_size)
       a <- glcm(raster,shift = dir, window = w_s,na_opt = "center", na_val = 0, asinteger = T)
       names(a)<- paste0(names(list_rasterlayers[[i]]),"_",vec_window_sizes[j],"x",vec_window_sizes[j],"_",names(a))
       a
     }
   stopCluster(cl) # Stops the cluster
   registerDoSEQ()
   return(unlist(GLCM_rasters))
 }


   #+END_SRC




*** Read in images
*** NAIP
    #+BEGIN_SRC R :results graphics :file naip.png
   naip <- stack("../RD_NAIP-imagery/madison.tif")
   plotRGB(naip,4,3,2, stretch = "lin")
    #+END_SRC

*** Pansharpened SPOT
    #+BEGIN_SRC R :exports both :results graphics :file panspot.png :tangle yes
   panspot <- stack("../RD_PansharpenedSPOT/geomatica_SPOT_panshp.tif")
   plotRGB(panspot,4,3,2, stretch = "lin")
    #+END_SRC


*** Read in training data
*** ROI from ENVI
    #+BEGIN_SRC R :tangle yes
   ## Lei's ROIs for training data
   l_water <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_water", encoding = "ESRI Shapefile")
   l_grass <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_grass", encoding = "ESRI Shapefile")
   l_tree <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_tree", encoding = "ESRI Shapefile")
   l_soil <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_soil", encoding = "ESRI Shapefile")
   l_impervious <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_impervious", encoding = "ESRI Shapefile")
   names(l_water) <- "water"
   names(l_grass) <- "grass"
   names(l_tree) <- "tree"
   names(l_soil) <- "soil"
   names(l_impervious) <- "impervious"
    #+END_SRC



*** Read in accuracy assessment regions
*** Points that Robi created
    #+BEGIN_SRC R :exports both :results graphics :file point_assessment_points.png :tangle yes
   rand_points <- readOGR("../RD_UFIA_RobiAccuracyCover", "accuracy_cover_2500")

    #+END_SRC

*** Field data Points
    #+BEGIN_SRC R :exports both :results graphics :file field_assessment_points.png :tangle yes
   centers <- readOGR(dsn = "../Urban FIA/PlotCenter",layer = 'plotCenter')
   centers <- spTransform(centers,CRSobj = CRS("+proj=utm +zone=16 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
   # Each field plot is 50 feet by 50 feet diamond
   # 5 feet is 1.524 meters
   # 1 foot is .3048 m
   # that means the distance from center to a corner is 25 ft, 25*.3048 = 7.62m

   # I need to add a buffer of 3 pixels, or 3m in the case of naip to the boundary of the plot

   rad <- 25*.3048+3
   # Make Diamond polygons around each plot
   dia <- list()
   for (i in seq_along(centers)) {
     dia[i] <- gBuffer(centers[i,], width = rad,quadsegs = 1)
   }
   dia <-do.call(bind,dia)
   plot(dia)
    #+END_SRC

*** Boxs around grids of points that Andy created
    #+BEGIN_SRC R :exports both :results output graphics :file grid_assessment_points.png :tangle yes
   grd_ass <- readOGR( dsn = "../RD_UFIA_GridAccuracy", layer = "ufia-grid-points", drop_unsupported_fields = T, pointDropZ = T, )
   grd_ass <- spTransform(grd_ass, CRS("+init=epsg:26916"))
   grd_box <- gBuffer(grd_ass, width = 8)
    #+END_SRC


*** Add buffers to the training data regions and the accuracy assessment
   Buffers needed so that we can see context of areas we are trying to
   predict and to get proper texture variables
*** Lei's Training data
    #+BEGIN_SRC R :tangle yes
   l_water_buf <- gBuffer(l_water, width = 10)
   l_grass_buf <- gBuffer(l_grass, width = 10)
   l_tree_buf <- gBuffer(l_tree, width = 10)
   l_soil_buf <- gBuffer(l_soil, width = 10)
   l_impervious_buf <- gBuffer(l_impervious, width = 10)
    #+END_SRC

*** Robi's Points
    #+BEGIN_SRC R :exports both :results graphics :file point_assessment_points_wBuffer.png :tangle yes
   rand_points_buffer <- gBuffer(rand_points, width = 10)
   plot(rand_points_buffer)
    #+END_SRC

*** Field data
    #+BEGIN_SRC R :tangle yes
   dia_buf <- gBuffer(dia, width = 10)

    #+END_SRC


*** Crop images to the extents of the merged training and testing/accuracy assessment data locations
*** Cropping images to the extent of training and testing locations
    #+BEGIN_SRC R :tangle
 #  naip_randpoints_clip <- clip(naip, rand_points_buffer)
 #  plot(naip_randpoints_clip)

 #  naip_fielddata_clip <- clip(naip, dia_buf)

    #+END_SRC



**** merge training and testing polygons
     #+BEGIN_SRC R :exports both :results graphs :file combined_training_and_testing_locations.png :tangle yes


     #+END_SRC


*** Save cropped images
*** Read back in the cropped images
*** Stack derived layers with original layers

*** NAIP Train and create models
*** create the object image called by extract_bind_df_addclass
    #+BEGIN_SRC R
 image
    #+END_SRC
*** convert the training data into dataframe
    #+BEGIN_SRC R

   ## Lei's ROIs for training data
   l_water <- readOGR(dsn = "../PAN_SPOT/ROIs/lei", layer = "pan_spot_lei_water", encoding = "ESRI Shapefile")
   l_grass <- readOGR(dsn = "../PAN_SPOT/ROIs/lei", layer = "pan_spot_lei_grass", encoding = "ESRI Shapefile")
   l_tree <- readOGR(dsn = "../PAN_SPOT/ROIs/lei", layer = "pan_spot_lei_tree", encoding = "ESRI Shapefile")
   l_soil <- readOGR(dsn = "../PAN_SPOT/ROIs/lei", layer = "pan_spot_lei_soil", encoding = "ESRI Shapefile")
   l_impervious <- readOGR(dsn = "../PAN_SPOT/ROIs/lei", layer = "pan_spot_lei_impervious", encoding = "ESRI Shapefile")
   names(l_water) <- "water"
   names(l_grass) <- "grass"
   names(l_tree) <- "tree"
   names(l_soil) <- "soil"
   names(l_impervious) <- "impervious"

   list_classes <- list(l_water, l_grass, l_tree, l_soil, l_impervious)

   beginCluster()
   b <- lapply(list_classes, function(x) extract_bind_df_addclass(x))
   endCluster()


   classified_px <- do.call("rbind", b)

   classified_px$Class %<>% as.factor()

   lei_df <- classified_px
    #+END_SRC
**** Ted's ROI's, not using but in here in case I want to
     #+BEGIN_SRC R
   ## Ted's ROIs for training data
   ## t_water <- readOGR(dsn = "../PAN_SPOT/ROIs", layer = "pan_spot_subset_water", encoding = "ESRI Shapefile")
   ## t_grass <- readOGR(dsn = "../PAN_SPOT/ROIs", layer = "pan_spot_subset_grass", encoding = "ESRI Shapefile")
   ## t_tree <- readOGR(dsn = "../PAN_SPOT/ROIs", layer = "pan_spot_subset_tree", encoding = "ESRI Shapefile")
   ## t_soil <- readOGR(dsn = "../PAN_SPOT/ROIs", layer = "pan_spot_subset_soil", encoding = "ESRI Shapefile")
   ## t_impervious <- readOGR(dsn = "../PAN_SPOT/ROIs", layer = "pan_spot_subset_impervious", encoding = "ESRI Shapefile")
   ## names(t_water) <- "water"
   ## names(t_grass) <- "grass"
   ## names(t_tree) <- "tree"
   ## names(t_soil) <- "soil"
   ## names(t_impervious) <- "impervious"

   ## list_classes <- list(t_water, t_grass, t_tree, t_soil, t_impervious)


   ## beginCluster()
   ## b <- lapply(list_classes, function(x) extract_bind_df_addclass(x))
   ## endCluster()

   ## classified_px <- do.call("rbind", b)
   ## classified_px$Class %<>% as.factor()

   ## ted_df <- classified_px
   ## write.table(ted_df, paste0("../",image_directory,"/ExtractedTrainingDataFrames/ted_roi_train_df.txt"))

     #+END_SRC







     ----


*** Once I find the best overall classification and image, I want to predict on the entire urban area
*** Clip image to urban areas
    Cropping images to extent of Madison, ShorewoodHills, and Middleton, the places where we have field data.

**** Read in shapefile of municipalities extent
     #+BEGIN_SRC R :exports both :results graphics :file studycities.png :tangle yes
   ## panspot_path <- "../RD_PansharpenedSPOT/geomatica_SPOT_panshp.tif"
   ## municipalities <- readOGR("../RD_WI-Municipalities-Shapefiles", "MCD")

   ##                                         # Filter out Madison, Middleton, and Shorewood hills
   ## study.cities <- municipalities[municipalities$NAME10 == "MADISON" | municipalities$NAME10 == "Madison" | municipalities$NAME10 == "Shorewood Hills" | municipalities$NAME10 == "Middleton", ]
   ## study.cities <- spTransform(study.cities, CRS("+init=epsg:26916"))
   ## plot(study.cities)
     #+END_SRC

**** crop naip image
     #+BEGIN_SRC R :exports both :results graphics :file urban_naip.png :tangle yes
   ## urban.naip <- crop(naip, study.cities)  # I need to get them in same projection!
   ## plotRGB(urban.naip,4,3,2, stretch = "lin")
     #+END_SRC

**** save cropped naip image
     #+BEGIN_SRC R :exports both :tangle yes
   ## library(raster)
   ## writeRaster(x = urban.naip, "NAIP/madison_naip_croppedToCityExtent.tif")
     #+END_SRC


*** Add Layers to Image
*** Classify image

** Another Old way, "Small Urban FIA"
*** Load Libraries
   #+BEGIN_SRC R :results silent
     library(parallel)
     library(plyr)
     library(mlr)
     library(scales)
     library(rgdal)
     library(raster)
     library(rgeos)
     library(glcm)
     library(spatial.tools)
     library(dplyr)
     library(doParallel)
     library(rasterVis)
     library(devtools)
       #devtools::install_github('walkerke/tigris')
     library(tigris)
     library(leaflet)
     library(htmlwidgets)

   #+END_SRC

*** Functions
 #+BEGIN_SRC R

   gdal_polygonizeR <- function(x, outshape=NULL, gdalformat = 'ESRI Shapefile',
				pypath=NULL, readpoly=TRUE, quiet=TRUE) {
     if (isTRUE(readpoly)) require(rgdal)
     if (is.null(pypath)) {
       pypath <- Sys.which('gdal_polygonize.py')
     }
     if (!file.exists(pypath)) stop("Can't find gdal_polygonize.py on your system.")
     owd <- getwd()
     on.exit(setwd(owd))
     setwd(dirname(pypath))
     if (!is.null(outshape)) {
       outshape <- sub('\\.shp$', '', outshape)
       f.exists <- file.exists(paste(outshape, c('shp', 'shx', 'dbf'), sep='.'))
       if (any(f.exists))
         stop(sprintf('File already exists: %s',
                      toString(paste(outshape, c('shp', 'shx', 'dbf'),
                                     sep='.')[f.exists])), call.=FALSE)
     } else outshape <- tempfile()
     if (is(x, 'Raster')) {
       require(raster)
       writeRaster(x, {f <- tempfile(fileext='.asc')})
       rastpath <- normalizePath(f)
     } else if (is.character(x)) {
       rastpath <- normalizePath(x)
     } else stop('x must be a file path (character string), or a Raster object.')
     system2('python', args=(sprintf('"%1$s" "%2$s" -f "%3$s" "%4$s.shp"',
                                     pypath, rastpath, gdalformat, outshape)))
     if (isTRUE(readpoly)) {
       shp <- readOGR(dirname(outshape), layer = basename(outshape), verbose=!quiet)
       return(shp)
     }
     return(NULL)
   }


 #+END_SRC

*** Wisconsin Urban Areas
 #+BEGIN_SRC R

   ua <- urban_areas(cb = T) # Download genearlized (1:500k) boundary file of all US urban areas

   WI <- states(cb = T, ) %>%                 # Download WI shapefile
	subset(NAME == "Wisconsin") %>%
	gBuffer(width = .0001)


   ua_wi <- crop(x = ua, y = WI)  # Crop urban areas to Wisconsin


   factpal <- colorFactor(topo.colors(50), ua_wi$NAME10)
   ua_map <- ua_wi %>% leaflet() %>% addTiles() %>% addPolygons(color = ~factpal(NAME10), popup = ~NAME10)
   ua_map

   ua_map %>% saveWidget(file = "WisconsinUrbanAreas.html", selfcontained = FALSE)

   ua_wi

 #+END_SRC

 There are 124 urban areas in the state of Wisconsin

*** Naip seamlines
 #+BEGIN_SRC R

   seams <- readOGR(dsn = "../RD_NAIP-imagery/SeamLines", layer = "ortho_naip_seamline-naipseamlines2013_a_wi")

   seams_html <- seams %>% leaflet %>% addTiles() %>% addPolygons()
   seams_html %>% saveWidget(file = "NAIP_seams.html")


   # Add column that identifies each polygon by start and end date/time
   seams@data <- seams@data %>%
       mutate(uid = as.factor(paste0(SDATE,"_",EDATE)))

   #crop seams by urban areas
   seams_by_ua <- crop(seams,ua_wi)



   factpal <- colorFactor(topo.colors(50), seams_by_ua$uid)
   seams_by_ua_html <- seams_by_ua %>%
       leaflet() %>%
       addTiles() %>%
       addPolygons(color = ~factpal(uid), popup = paste("Date", seams_by_ua$IDATE, "<br>",
                                                          "Start Date", seams_by_ua$SDATE,"<br>",
                                                          "End Date", seams_by_ua$EDATE))


   seams_by_ua_html

   seams_by_ua_html %>% saveWidget(file = "seams_by_urbanArea.html", selfcontained = FALSE)

   seams_by_ua

   str(seams_by_ua@data$uid))
   n_distinct(seams_by_ua@data$uid)
 #+END_SRC

 There appear to be 96 NAIP seams over urban areas.  Do we need to
 build a model for each one?  Probably not, but training data should
 come from all of them if we are going to build on large classification model.


*** Specifiy parameters
 #+BEGIN_SRC R
   RD_imagePath <-"../RD_NAIP-imagery/small_Mad.tif"
   imageName <- "small_Mad"
   imageDirectory <- "NAIP/"
   nCores <- detectCores()
   segName <- "segRasterN10579C300"
 #+END_SRC

*** Classify Image

**** Read in Image
   #+BEGIN_SRC R :results graphics :file (org-babel-temp-file "./Figs-" ".png")
     image <- stack(RD_imagePath)
     names(image) <- c("blue", "green", "red", "nir")
     plotRGB(image,4,3,2, stretch = "lin")
   #+END_SRC

**** Add Layers to image
***** add ndvi
      #+BEGIN_SRC R :results graphics :exports both :file (org-babel-temp-file "./Figs-" ".png")
	calc_ndvi <- function(x){(x$nir - x$red)/(x$nir + x$red)}

	beginCluster()
	ndvi <- clusterR(x = image, fun = calc_ndvi)
	endCluster()
	names(ndvi) <- "NDVI"
	dir.create(paste0(imageDirectory,"NDVI"))
	writeRaster(x = ndvi, paste0(imageDirectory,"NDVI/",imageName,"_NDVI.tif"), overwrite = T)

	ndvi_plot <- gplot(ndvi)+ geom_tile(aes(fill = value)) +
            scale_fill_gradient2(low = muted("red"), mid = "white", high = muted('green')) +
            ggtitle("NDVI") + coord_equal()

	ggsave(filename = paste0("Figs/",imageName,"_ndvi.png"))
 #       print(ndvi_plot)
	ndvi_plot
 #+END_SRC


***** add texture (glcm)
      #+BEGIN_SRC R
         statistics <- c("mean","variance", "homogeneity", "contrast",
                         "dissimilarity", "entropy","second_moment",
                         "correlation")

         shift <- list(c(1,1)) # 135 degree,
         window <- list(c(3,3))


	dir.create(path = paste0(imageDirectory,"texture_",imageName))

         cl <- makeCluster(2)

	registerDoParallel(cl)


         # I changed the code from %dopar% to %do% because I don't think I'll have enough memory to run in parallel.  Running on the server, I'll need to change back to %dopar%
         foreach(j = 1:length(statistics), .packages=c('raster', 'glcm')) %dopar% {
           for (i in 1:length(window)) {
               GLCM <- glcm(ndvi, window=window[[i]], statistics=statistics[j],
                            shift = shift, na_opt="ignore")
               file <- paste0(imageDirectory,"texture_",imageName,"/",imageName,"_",names(GLCM),"_window",window[[i]][1],".tif")
               writeRaster(GLCM, filename = file, type = "GTIFF", overwrite = T)
             }
           }

         stopCluster(cl)

      #+END_SRC

**** Perform PCA and select vegetation/urban related components
 #+BEGIN_SRC R
     all <- stack(image, ndvi)

   # Function takes raster stack, samples data, performs pca and returns stack of first n_pcomp bands
   predict_pca_wSampling_parallel <- function(stack, sampleNumber, n_pcomp, nCores = detectCores()-1) {
       sr <- sampleRandom(stack,sampleNumber)
       pca <- prcomp(sr, scale.=T)
       beginCluster()
       r <- clusterR(stack, predict, args = list(pca, index = 1:n_pcomp))
       endCluster()
       return(r)
   }


   # perform PCA with 4 original bands and ndvi
   rgbnn_pca <- predict_pca_wSampling_parallel(all, 10000, 5)




   # Save these PCA's
   PCA_directory <- paste0(imageDirectory,"PCA_",imageName)
   dir.create(path = PCA_directory)
   writeRaster(rgbnn_pca, paste0(PCA_directory,"/",imageName,"rgbnn_pca.tif"), overwrite = T)
 #+END_SRC

**** plotting PCA's
***** PCA rgbn, bands r=2, g=1, b=3
 #+BEGIN_SRC R :results graphics :exports both :file (org-babel-temp-file "./Figs-" ".png")
 plotRGB(rgbnn_pca,2,1,3, stretch = "lin")
 #+END_SRC

***** PCA rgbn, bands r=2, g=4, b=3
 #+BEGIN_SRC R :results graphics :exports both :file (org-babel-temp-file "./Figs-" ".png")
 plotRGB(rgbnn_pca,4,2,3, stretch = "lin")
 #+END_SRC




**** Segment Image

 #+BEGIN_SRC sh :eval no
   python Python/fia_segment.py
 #+END_SRC

 #+BEGIN_SRC python :eval yes
   from skimage.segmentation import slic, mark_boundaries
   import gdal
   import os
   import numpy as np
   import matplotlib.pyplot as plt
   # from skimage.util import img_as_float
   from skimage import io

   # image_path = r"C:\_urban_fia\fia_data\SPOT_subset_subset_rgb.tif"
   # image = io.imread(image_path)
   # image = image[:, : , 0:-1]
   # print image

   number_pixels_per_seg = 20
   source_raster_path = os.path.expanduser('~/GoogleDrive/Pjt_UFIA/NAIP/PCA_small_Mad/small_Madrgbnn_pca.tif')
   target_raster_path = os.path.expanduser('~/GoogleDrive/Pjt_UFIA/NAIP/Seg/seg_raster.tif')

   def build_tiff(source_raster_path, target_raster_path, image_array):

       source_raster = gdal.Open(source_raster_path)
       bands = source_raster.RasterCount
       columns = source_raster.RasterXSize
       rows = source_raster.RasterYSize
       driver = gdal.GetDriverByName("GTiff")
       new_image = driver.Create(target_raster_path, columns, rows, 1, gdal.GDT_Float64)
       new_image.SetGeoTransform(source_raster.GetGeoTransform())
       new_image.SetProjection(source_raster.GetProjectionRef())

       new_image.GetRasterBand(1).WriteArray(image_array)
       '''
       for band in range(bands):
           new_image.GetRasterBand(band+1).WriteArray(image_array[band, :, :])
       '''
       return

   # Grab Raster metadata using GDAL
   source_raster = gdal.Open(source_raster_path)
   bands = source_raster.RasterCount
   columns = source_raster.RasterXSize
   rows = source_raster.RasterYSize
   data_type = source_raster.GetRasterBand(1).DataType  # 2 = UInt16

   array = source_raster.ReadAsArray()

   array = array.transpose(1, 2, 0)  # Transpose TIFF space to a more sensible numpy array orientation

   array = array[:, :, 0:3]  # Can be used to constrain number of principal components to use (change final list term)

   number_pixels_per_seg = np.array(100)
   num_segs = rows * columns / number_pixels_per_seg

   compactness = [20,25,30,35]
   print 'Making segments'

   n = num_segs
   c = 300

   target_raster_path = '/Users/tedward/GoogleDrive/Pjt_UFIA/NAIP/Seg/segRasterN%sC%s.tif' %(n,c)
   segments = slic(array, n_segments=n, compactness=c, enforce_connectivity=True)
   print 'Made dem segments'
   build_tiff(source_raster_path, target_raster_path, segments)
   print 'Tiff made successfully'


   #for n in num_segs :
   #    for c in compactness :
           # target_raster_path = '/Users/tedward/GoogleDrive/Pjt_UFIA/NAIP/Seg/segRasterN%sC%s.tif' %(n,c)
           # segments = slic(array, n_segments=n, compactness=c, enforce_connectivity=True)
           # print 'Made dem segments'
           # build_tiff(source_raster_path, target_raster_path, segments)
           #print 'Tiff made successfully'

     #+END_SRC



**** Polygonize Segments
 #+BEGIN_SRC R
   segPath <- paste0(imageDirectory,"/Seg")
   segNames <- list.files(path = segPath)

   lapply(segNames, function(x) {
       seg <- raster(paste0("NAIP/Seg/",x))
       s <- gdal_polygonizeR(seg)
       writeOGR(obj = s, dsn = paste0(imageDirectory,"/Seg_Shapefiles"), layer = x, driver = "ESRI Shapefile")
   })
 #+END_SRC


 Segmentation parameters that I think are good:

 |   | size | compactness |      |                                                                                                 |
 |---+------+-------------+------+-------------------------------------------------------------------------------------------------|
 |   |    7 |          30 | good |                                                                                                 |
 |   |    5 |          30 | good | This is the one I'm going to try to use.                                                        |
 |   |  100 |         300 | good | I've made training data for this one. The segments are bigger, but maybe better.  Need to test. |
 |   |      |             |      |                                                                                                 |




**** In QGIS spend X amount of time finding training segments for classification
 I spend 20 min classifying polygons in shapefile with size:100,
 compactness 300.  Classes: Tree, Grass, Impervious

 shapefile name is segRasterN10579C300.tif.shp

**** Calculate zonal statistics for image based on segmentation, extract features from segments

 #+BEGIN_SRC R
   segName <- "segRasterN10579C300"

   segDirectory <- paste0(imageDirectory,"Seg/")

   segmentationLayerPath <- paste0(segDirectory,segName,".tif")

   image <- stack(RD_imagePath)
   names(image) <- c("blue", "green", "red", "nir")

   ndvi <- raster(paste0(imageDirectory,"NDVI/",imageName,"_NDVI.tif"))
   names(ndvi) <- "ndvi"

   r <- stack(image, ndvi)

   seg <- raster(segmentationLayerPath)



                                           # Create a data_frame where mean and variances are calculated by zone
   # calculate means
   means <- zonal(x = r, z = seg, 'mean')
   colnames(means)[2:6] <- paste0(colnames(means)[2:6], "_mean")

                                           # calculate sd
   sds <- zonal(x = r, z = seg, 'sd')
   colnames(sds)[2:6] <- paste0(colnames(sds)[2:6], "_sd")

   d <- merge(means,sds)

   write.csv(d, paste0(imageDirectory, "SegmentFeatureDataFrames/",imageName,"_",segName,"_features.csv"))

                                           # Read in the training data from the shapefile
   DSN <- paste0(imageDirectory,"Seg_Shapefiles/")

   train <- readOGR(dsn = DSN, layer = paste0(segName,".tif"))@data %>%
                                              na.omit() %>%
                                              rename(zone = DN)

   dat <- left_join(train,d)

                                           # Save this dataframe


   write.csv(dat, paste0(imageDirectory,"TrainingDataFrames/",imageName,"_",segName,"TrainData.csv"))
 #+END_SRC




**** Create Model
 #+BEGIN_SRC R
   # Read in data
   trainingDataPath <- paste0(imageDirectory,"TrainingDataFrames/",imageName,"_",segName,"TrainData.csv")
   dat <- read.csv(trainingDataPath, header = T, stringsAsFactors = F)

   dat_T <- dat %>%
       mutate(LandClass = ifelse(LandClass == "T", LandClass, "O"))

   dat_G <- dat %>%
       mutate(LandClass = ifelse(LandClass == "G", LandClass, "O"))

   dat_I <- dat %>%
       mutate(LandClass = ifelse(LandClass == "I", LandClass, "O"))


   dropXandZone <- function(x) {dropFeatures(x, c("X","zone"))}

   # Create Tasks
   task <- makeClassifTask(id = paste0(imageName,"_",segName,"_all"), data = dat, target = "LandClass") %>%
       dropXandZone
   task_tree <- makeClassifTask(id = paste0(imageName,"_",segName,"_tree"), data = dat_T, target = "LandClass", positive = "T") %>%
       dropXandZone
   task_grass <- makeClassifTask(id = paste0(imageName,"_",segName,"_grass"), data = dat_G, target = "LandClass", positive = "G") %>%
       dropXandZone
   task_impervious <- makeClassifTask(id = paste0(imageName,"_",segName,"_impervious"), data = dat_I, target = "LandClass", positive = "I") %>%
       dropXandZone

   task_list <- list(all = task, tree = task_tree, grass = task_grass, impervious = task_impervious)

   # snippets of code that I don't think I'll need to use, but might
   # task = normalizeFeatures(task, method = "range")
   # summary(getTaskData(task))



                                           # Make Learners
   # RF
   RF_prob <- makeLearner(id = "rf_prob","classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
   RF_response <- makeLearner(id = "rf_resp", "classif.randomForest", predict.type = "response", fix.factors.prediction = TRUE)
   SVM_prob <- makeLearner(id = "svm_prob", "classif.svm", predict.type = "prob", fix.factors.prediction = TRUE)
   SVM_response <- makeLearner(id = "svm_resp", "classif.svm", predict.type = "response", fix.factors.prediction = TRUE)

   learner_list <- list(RF_prob = RF_prob, RF_response = RF_response, SVM_prob = SVM_prob, SVM_response = SVM_response)


                                           # Train Learners on Tasks, Make models


   library(foreach)


   cl<-makeCluster(nCores)
   registerDoParallel(cl)


   models <- foreach(task = task_list, .packages = "mlr") %:%
       foreach(learner = learner_list) %dopar% {
           train(learner, task)
       }

   endCluster()

   save(models, file = paste0(imageDirectory, "Models/",imageName, "_", segName,".Rdata"))
   save(models, file = paste0(imageDirectory, "Models/",imageName, "_", segName,".rda"))







 #+END_SRC

**** Predict
 #+BEGIN_SRC R
         #load models
         modelsPath <- paste0(imageDirectory, "Models/",imageName, "_", segName,".Rdata")
     load(file = modelsPath)
     models <- unlist(models, recursive = F)

                                                 #load segmenation
         seg <- raster(paste0("NAIP/Seg/",segName,".tif"))

         imgdata <- read.csv(paste0(imageDirectory, "SegmentFeatureDataFrames/",imageName,"_",segName,"_features.csv"))


   # Write Classified Rasters
     cl<-makeCluster(nCores)
     registerDoParallel(cl)
   #  mod <- models[[5]]
     foreach(mod = models, .packages = c("leaflet", "mlr", "raster", "htmlwidgets", "dplyr")) %dopar% {
         predicted <- predict(mod, newdata = imgdata[3:12])  # predict model on image segments df
         # create reclassification, reclassify and assign levels
         response <- factor(as.character(predicted$data$response), levels = c("G","I","T","O"))
         m <- cbind(imgdata$zone,response)
         r <- reclassify(seg, m)
         x <- data.frame(ID = 1:4, LandCover = c("G","I","T","O")) %>%
             filter(LandCover == levels(factor(response)))
         levels(r) <- x
         writeRaster(r, paste0("ClassifiedRasters",mod$task.desc$id,"_",mod$learner$id,".tif")
     }

         setwd("/Users/tedward/GoogleDrive/Pjt_UFIA")

     endCluster()

                                           # Send Raster to Leaflet

   rasterFileNames <- list.files("ClassifiedRasters", pattern = ".+tif$", full.names = T)
   r <- lapply(rasterFileNames, raster)



   setwd("LeafletMaps")

       coldf <- data_frame(colors = c("#ffff99", "#beaed4", "#7fc97f", "#7e7e7e"), LandCover = c("G","I","T","O"))
	cols <- coldf %>%
             filter(LandCover %in% levels(r[[13]])[[1]]$category) %>%
             select(colors)

         types1 <- levels(r[[13]])[[1]][,1]
         pal1 <- colorFactor(cols$colors, types1)

         types2 <- levels(r[[14]])[[1]][,1]
         pal2 <- colorFactor(cols$colors, types2)

         types3 <- levels(r[[15]])[[1]][,1]
         pal3 <- colorFactor(cols$colors, types3)

         types4 <- levels(r[[16]])[[1]][,1]
         pal4 <- colorFactor(cols$colors, types3)

         map <- leaflet() %>% addProviderTiles("Esri.WorldImagery", group = "ESRI") %>%
             addRasterImage(r[[13]], group = names(r[[13]]), colors = pal1, opacity = 0.6) %>%
             addRasterImage(r[[14]], group = names(r[[14]]), colors = pal2, opacity = 0.6) %>%
             addRasterImage(r[[15]], group = names(r[[15]]), colors = pal1, opacity = 0.6) %>%
             addRasterImage(r[[16]], group = names(r[[16]]), colors = pal2, opacity = 0.6) %>%
             addLayersControl(
                 overlayGroups = c("ESRI",names(r[[13]]),names(r[[14]]), names(r[[15]]), names(r[[16]])),
                 options = layersControlOptions(collapsed = FALSE)
             )


   map

         saveWidget(map, file = "tree.html", selfcontained = F)

         saveWidget(map, file = paste0(mod$task.desc$id,mod$l,"_",mod$learner$id,".html"),selfcontained = F)







         predicted <- predict(mod, newdata = imgdata[3:12])

         m <- cbind(imgdata$zone,predicted$data$response)
         r <- reclassify(seg, m)



         x <- levels(as.factor(r))[[1]]
         x$LandCover <- levels(predicted$data$response)

         levels(r) <- x

         cols <- coldf %>%
             filter(LandCover == levels(predicted$data$response)) %>%
             select(colors)

         ## m <- data.frame(zone = imgdata$zone,predicted$data$response)
         ## str(m)
         ## rdf <- as.data.frame(seg, xy  = T)
         ## names(rdf)[3] <- "zone"
         ## dat <- left_join(rdf, m)

       # Send Raster to Leaflet

         types <- levels(r)[[1]][,1]
         pal <- colorFactor(cols, types)
         map <- leaflet() %>% addProviderTiles("Esri.WorldImagery", group = "ESRI") %>%
             addRasterImage(r, group = "r", colors = pal, opacity = 0.6) %>%
             addLayersControl(
                 overlayGroups = c("ESRI","r"),
                 options = layersControlOptions(collapsed = FALSE)
                 )


         saveWidget(map, file = paste0(mod$task.desc$id,mod$l,"_",mod$learner$id,".html"),selfcontained = F)
     }
         setwd("/Users/tedward/GoogleDrive/Pjt_UFIA")


     endCluster()




      types <- levels(r)[[1]][,2]
         pal <- colorFactor(c("#ffff99", "#beaed4", "#7fc97f"), types)
     pal <- colorFactor(c("#ffff99", "#beaed4", "#7fc97f","#7e7e7e"), types, levels = c("G","I","T","O"))
         #pal <- colorFactor(c("#ffff99", "#beaed4", "#7fc97f","#7e7e7e"), c("G","I","T","O"))

         map <- leaflet() %>% addProviderTiles("Esri.WorldImagery", group = "ESRI") %>%
             addRasterImage(r, group = "r", colors = pal, opacity = 0.6) %>%
             addLayersControl(
                 overlayGroups = c("ESRI","r"),
                 options = layersControlOptions(collapsed = FALSE)
                 )
 #+END_SRC











** From first accuracy assessment attempt
****** pan spot?
#+BEGIN_SRC R
#############
#                The objective of this document
#
#         I have to provide an accuracy assesment tomorrow
#        This document strives to get me there as fast as possible
#         I will have to organize this code next week
#
##############################


##### Loading Libraries

library(dplyr)
  library(rgdal)
  library(raster)
  library(rgeos)
  library(glcm)
  library(spatial.tools)
  library(dplyr)
  library(doParallel)
  library(mlr)
library(randomForest)

# First I am going to perform accuracy assessment for the pansharpened spot
## Loading the Pansharpened image and addtional layers

PAN_SPOT <- brick("../PAN_SPOT/geomatica_SPOT_panshp_wRatios.tif")
PAN_SPOT_tex<- brick("../PAN_SPOT/geomatica_SPOT_panshp_wRatios_wTexture.tif")
image <- stack(PAN_SPOT, PAN_SPOT_tex)
image <- image[[-18]]
image

## Load shapefiles of training data
  l_water <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_water", encoding = "ESRI Shapefile")
  l_grass <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_grass", encoding = "ESRI Shapefile")
  l_tree <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_tree", encoding = "ESRI Shapefile")
  l_soil_readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_soil", encoding = "ESRI Shapefile")
  l_impervious <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_impervious", encoding = "ESRI Shapefile")
  names(l_water) <- "water"
  names(l_grass) <- "grass"
  names(l_tree) <- "tree"
  names(l_soil) <- "soil"
  names(l_impervious) <- "impervious"

### create buffers around training data
  l_water_buf <- gBuffer(l_water, width = 10)
  l_grass_buf <- gBuffer(l_grass, width = 10)
  l_tree_buf <- gBuffer(l_tree, width = 10)
  l_soil_buf <- gBuffer(l_soil, width = 10)
  l_impervious_buf <- gBuffer(l_impervious, width = 10)

## Load shapefiles of accuracy asssesment areas
### field data
plot_centers <- readOGR(dsn = "../FieldData/PlotCenterShpFile", layer = "plotCenter")
plot_centers <- spTransform(plot_centers,CRSobj = CRS("+proj=utm +zone=16 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
rad <- 25*.3048+3  #radius of fieldplot plus 3m buffer
#rad <- 15
dia <- list()
for (i in seq_along(plot_centers)) {
      dia[i] <- gBuffer(plot_centers[i,], width = rad,quadsegs = 1)
  }

r_list <- list()
for (i in seq_along(dia)){
    r_list[[i]] <- crop(image,dia[[i]])
}
plots_101 <- r_list

plots_101_merged <- do.call(merge, args = r_list)

### Andy's Grid of Points
  grd_ass <- readOGR( dsn = "../RD_UFIA_GridAccuracy", layer = "ufia-grid-points")
  grd_ass <- spTransform(grd_ass, CRS("+init=epsg:26916"))
  grd_box <- gBuffer(grd_ass, width = 8)
grd_box <- disaggregate(grd_box)
str(grd_box)
grd_box

r_list <- list()
for (i in seq_along(grd_box@polygons)){
    r_list[[i]] <- crop(image, grd_box[i,])
}
grds_30 <- r_list

grds_30_merged <- do.call(merge, args = r_list)


### Robi's Random Points
rand_points <- readOGR("../RD_UFIA_RobiAccuracyCover", "accuracy_cover_2500")
proj4string(rand_points) <- "+init=epsg:26916"
#### crop to extent of madison, middleton, and shorewood hills
 municipalities <- readOGR("../RD_WI-Municipalities-Shapefiles", "MCD")

  ## ##                                         # Filter out Madison, Middleton, and Shorewood hills
 study.cities <- municipalities[municipalities$NAME10 == "MADISON" | municipalities$NAME10 == "Madison" | municipalities$NAME10 == "Shorewood Hills" | municipalities$NAME10 == "Middleton", ]
 study.cities <- spTransform(study.cities, CRS("+init=epsg:26916"))

rand_points <- rand_points[study.cities,]

#### add buffer
rand_points_buffer <- gBuffer(rand_points, width = 5)
rand_points_buffer <- disaggregate(rand_points_buffer)

r_list <- list()
for (i in seq_along(rand_points_buffer@polygons)){
    r_list[[i]] <- crop(image, rand_points_buffer[i,])
}
rand_points_r <- r_list
rand_points_r_merged <- do.call(merge, args = r_list)


#### Combine all these cropped images into one to create
#### the pansharpened spot image (with all added layers)
#### which contains the areas
#### for training and testing accuracy

list_rasters <- list(plots_101_merged, grds_30_merged, rand_points_r_merged)
r <- do.call(merge, list_rasters)
plot(r[[1]])
writeRaster(r, "../PAN_SPOT/raster_at_accuracyLocations.tif")



### drop last layer of r
d <- dropLayer(r, 18)



#### Create Pansharp models
image


#### Apply RF model to raster
rf_mod <- best_models[[1]]
names(r) <- attributes(rf_mod$learner.model$terms)$term.labels # assign names that the model was trained on.


rf_spot_raster <- raster::predict(object = r, rf_mod$learner.model)

#### save pan_spot_rf_accuracy_raster.tif
writeRaster(rf_spot_raster, "../PAN_SPOT/ClassifiedImages/pan_spot_rf_accuracy_raster.tif")




#### Apply SVM model to raster
svm_mod <- best_models[[i]]$learner.model
svm_spot_raster <- raster::predict(object = r, svm_mod)

#### save pan_spot_svm_accuracy_raster.tif
writeRaster(svm_spot_raster, "../PAN_SPOT/ClassifiedImages/pan_spot_svm_accuracy_raster.tif")








### Calculate Accuracy of pan spot rf using 1 ha blocks
#### read in 1 ha block data, tree defined by Andy via google earth
grid_accuracy <- read.csv("../RD_UFIA_GridAccuracy/grid_accuracy_assessment_andy.csv", header = T)
head(grid_accuracy)



#### get percent tree cover by plot
d <- grid_accuracy %>%
      group_by(Plot, Cover_Type) %>%
      summarize(number = n()) %>%
      mutate(percent_cover = number/225) %>%
      filter(Cover_Type == "t")

e <- rbind(d,data.frame(Plot = c(1,11,26), Cover_Type = c("t","t","t"), number = c(0,0,0),percent_cover = c(0,0,0)))
e <- arrange(e, Plot)

R_plotNames <- c(10,1,4,29,16,13,14,18,6,22,5,11,15,26,17,9,25,30,20,3,21,23,7,12,2,27,8,19,24,28)
d <- cbind(e,R_plotNames)
d
d <- arrange(d, R_plotNames)


#### overlay of grid boxes on top of pan spot rf classification and extract pixel values by plot
best_models <- readRDS("../PAN_SPOTbestModels.Rdata")
#### Apply RF model to raster

pred_grd30 <- lapply(grds_30, FUN= function(x){
           raster::predict(object = x, model = lei.rf.mod$learner.model)
       })
lei.rf.mod$learner.model

#### get frequency table for each raster
freq_grd30 <- lapply(pred_grd30, raster::freq)
freqdf_grd30 <- lapply(freq_grd30, as.data.frame)
#### get percent tree cover by plot (according to image)


percent_tree <- lapply(freqdf_grd30, FUN = function(x) {
                x[x$value == 4,2]/sum(x[,2])
            })
percent_tree <- unlist(percent_tree)


freqdf_grd30[[2]][freqdf_grd30[[2]]$value == 4,2]
freqdf_grd30[[2]]

percent_tree
#### calculate RMSE
andy_pctTree <- d$percent_cover
andy_pctTree



mean((percent_tree - andy_pctTree)^2)^0.5
plot(percent_tree,andy_pctTree)
d$percent_cover
percent_tree

png(filename = "pan_spot_rf_grid_prediction.png")
pan_spot_rf_predicted_percent_tree <- percent_tree
plot(andy_pctTree,pan_spot_rf_predicted_percent_tree, main = "RMSE = 18.288%")
abline(a=0, b= 1, col = "red")
dev.off()



### Calculate Accuracy of pan spot svm using 1 ha blocks
#### read in 1 ha block data, tree defined by Andy via google earth
#### get percent tree cover by plot
# This is andy_pctTree


best_models <- readRDS("../PAN_SPOTbestModels.Rdata")
#### Apply RF model to raster
#best_models
#lei.svm.mod <- best_models$lei.svm.mod

pred_grd30 <- lapply(grds_30, FUN= function(x){
           raster::predict(object = x, model = lei.svm.mod$learner.model)
       })
plot(pred_grd30[[1]])

#### get frequency table for each raster
freq_grd30 <- lapply(pred_grd30, raster::freq)
freqdf_grd30 <- lapply(freq_grd30, as.data.frame)
#### get percent tree cover by plot (according to image)


percent_tree <- lapply(freqdf_grd30, FUN = function(x) {
                x[x$value == 4,2]/sum(x[,2])
            })
percent_tree <- unlist(percent_tree)
pan_spot_svm_predicted_precent_tree <- percent_tree
#### calculate RMSE
mean((percent_tree - andy_pctTree)^2)^0.5

png(filename = "pan_spot_svm_grid_prediction.png")
plot(andy_pctTree, pan_spot_svm_predicted_precent_tree, main = "RMSE = 19.6%")
abline(a=0,b=1, col = "red")
dev.off()





### Calculate Accuracy of pan spot rf using field data
best_models <- readRDS("../PAN_SPOTbestModels.Rdata")
#### Apply RF model to raster

pred_plots <- lapply(plots_101, FUN = function(x){
           raster::predict(object = x, model = lei.rf.mod$learner.model)
       })


#### read field data, get percent tree cover by plot
field <- read.csv("../FieldData/UFIA_FieldData_summarizedbyPlot.csv", header = T)
str(field)
field_pct_tree <- field$PctGreaterThan0percentVegAbove


#### overlay of field plot diamonds on top of pan spot rf classification and extract pixel values by plot
pred_plots_inBuf <- list()
for (i in 1:101){
    pred_plots_inBuf[[i]] <- extract(x = pred_plots[[i]], y = dia[[i]])
}

plot(pred_plots[[94]])
pred_plots_inBuf <- unlist(pred_plots_inBuf,recursive = F)

#### get percent tree cover by plot (according to image)
pan_sharp_rf_pct_tree_fieldplots <- list()
for (i in 1:101){
    pan_sharp_rf_pct_tree_fieldplots[[i]] <- sum(pred_plots_inBuf[[i]] == 4)/length(pred_plots_inBuf[[i]])
}
pan_sharp_rf_pct_tree_fieldplots <- unlist(pan_sharp_rf_pct_tree_fieldplots)




#### calculate RMSE
mean((field_pct_tree - pan_sharp_rf_pct_tree_fieldplots)^2)^0.5

png(filename = "pan_spot_rf_fielddata_prediction.png")
plot(field_pct_tree, pan_sharp_rf_pct_tree_fieldplots, main = "RMSE = 22.5%")
abline(a= 0, b = 1, col = "red")
dev.off()






### Calculate Accuracy of pan spot svm using field data
#### Apply SVM to raster
pred_plots <- lapply(plots_101, FUN = function(x){
           raster::predict(object = x, model = lei.svm.mod$learner.model)
       })

pred_plots_inBuf <- list()
for (i in 1:101){
    pred_plots_inBuf[[i]] <- extract(x = pred_plots[[i]], y = dia[[i]])
}

plot(pred_plots[[1]])
pred_plots_inBuf <- unlist(pred_plots_inBuf,recursive = F)

#### get percent tree cover by plot (according to image)
pan_sharp_svm_pct_tree_fieldplots <- list()
for (i in 1:101){
    pan_sharp_svm_pct_tree_fieldplots[[i]] <- sum(pred_plots_inBuf[[i]] == 4)/length(pred_plots_inBuf[[i]])
}
pan_sharp_svm_pct_tree_fieldplots <- unlist(pan_sharp_svm_pct_tree_fieldplots)
#### calculate RMSE
mean((field_pct_tree - pan_sharp_svm_pct_tree_fieldplots)^2)^0.5

png(filename = "pan_spot_svm_fielddata_prediction.png")
plot(field_pct_tree, pan_sharp_svm_pct_tree_fieldplots, main = "RMSE = 23.4")
abline(a= 0, b = 1, col = "red")
dev.off()


#+END_SRC
****** naip ?
#+BEGIN_SRC R
##### Loading Libraries

  library(rgdal)
  library(raster)
  library(rgeos)
  library(glcm)
  library(spatial.tools)
  library(dplyr)
  library(doParallel)
  library(mlr)
library(randomForest)

## This is the accuracy assessment for the NAIP image.
## I will need to load the layers of naip that I have, crop to training and testing areas, then add texture
## Then build models and predict
## Then Assess accuracy.

naip <- stack(x = "../NAIP/madison.tif")
ratio1 <- stack(x = "../NAIP/madison_wRatio1.tif")
ratio2 <- stack(x = "../NAIP/madison_wRatio2.tif")
ratio3 <- stack(x = "../NAIP/madison_wRatio3.tif")
ndvi <- stack("../NAIP/madison_NDVI.tif")
savi <- stack("../NAIP/madison_SAVI.tif")
image <- stack(naip, ratio1, ratio2, ratio3,ndvi, savi)

## Load shapefiles of training data
  l_water <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_water", encoding = "ESRI Shapefile")
  l_grass <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_grass", encoding = "ESRI Shapefile")
  l_tree <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_tree", encoding = "ESRI Shapefile")
  l_soil <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_soil", encoding = "ESRI Shapefile")
  l_impervious <- readOGR(dsn = "../RD_UFIA_leiROI", layer = "pan_spot_lei_impervious", encoding = "ESRI Shapefile")
  names(l_water) <- "water"
  names(l_grass) <- "grass"
  names(l_tree) <- "tree"
  names(l_soil) <- "soil"
  names(l_impervious) <- "impervious"

### create buffers around training data
  l_water_buf <- gBuffer(l_water, width = 10) %>%
      disaggregate()
  l_grass_buf <- gBuffer(l_grass, width = 10) %>%
      disaggregate()
  l_tree_buf <- gBuffer(l_tree, width = 10) %>%
      disaggregate()
  l_soil_buf <- gBuffer(l_soil, width = 10) %>%
      disaggregate()
  l_impervious_buf <- gBuffer(l_impervious, width = 10) %>%
      disaggregate()




## get rasterlayers at location of training data
r_list <- list()
for (i in seq_along(l_water_buf@polygons)){
        r_list[[i]] <- crop(image, l_water_buf[i,])
    }
water_rasters <- r_list


r_list <- list()
for (i in seq_along(l_grass_buf@polygons)){
        r_list[[i]] <- crop(image, l_grass_buf[i,])
    }
grass_rasters <- r_list

r_list <- list()
for (i in seq_along(l_tree_buf@polygons)){
        r_list[[i]] <- crop(image, l_tree_buf[i,])
    }
tree_rasters <- r_list

r_list <- list()
for (i in seq_along(l_soil_buf@polygons)){
        r_list[[i]] <- crop(image, l_soil_buf[i,])
    }
soil_rasters <- r_list

r_list <- list()
for (i in seq_along(l_impervious_buf@polygons)){
        r_list[[i]] <- crop(image, l_impervious_buf[i,])
    }
impervious_rasters <- r_list


## Load shapefiles of accuracy asssesment areas
### field data
plot_centers <- readOGR(dsn = "../FieldData/PlotCenterShpFile", layer = "plotCenter")
plot_centers <- spTransform(plot_centers,CRSobj = CRS("+proj=utm +zone=16 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
# rad_25*.3048+3  #radius of fieldplot plus 3m buffer
rad <- 15
dia <- list()
for (i in seq_along(plot_centers)) {
          dia[i] <- gBuffer(plot_centers[i,], width = rad,quadsegs = 1)
      }

r_list <- list()
for (i in seq_along(dia)){
        r_list[[i]] <- crop(image,dia[[i]])
    }
plots_101 <- r_list

plots_101_merged <- do.call(merge, args = r_list)

### Andy's Grid of Points
  grd_ass <- readOGR( dsn = "../RD_UFIA_GridAccuracy", layer = "ufia-grid-points", drop_unsupported_fields = T, pointDropZ = T)
  grd_ass <- spTransform(grd_ass, CRS("+init=epsg:26916"))
  grd_box <- gBuffer(grd_ass, width = 8)
grd_box <- disaggregate(grd_box)

r_list <- list()
for (i in seq_along(grd_box@polygons)){
        r_list[[i]] <- crop(image, grd_box[i,])
    }
grds_30 <- r_list
grds_30_merged <- do.call(merge, args = r_list)



### Robi's Random Points
rand_points <- readOGR("../RD_UFIA_RobiAccuracyCover", "accuracy_cover_2500")
proj4string(rand_points) <- "+init=epsg:26916"
#### crop to extent of madison, middleton, and shorewood hills
 municipalities <- readOGR("../RD_WI-Municipalities-Shapefiles", "MCD")

  ## ##                                         # Filter out Madison, Middleton, and Shorewood hills
 study.cities <- municipalities[municipalities$NAME10 == "MADISON" | municipalities$NAME10 == "Madison" | municipalities$NAME10 == "Shorewood Hills" | municipalities$NAME10 == "Middleton", ]
 study.cities <- spTransform(study.cities, CRS("+init=epsg:26916"))

rand_points <- rand_points[study.cities,]

#### add buffer
rand_points_buffer <- gBuffer(rand_points, width = 5)
rand_points_buffer <- disaggregate(rand_points_buffer)


r_list <- list()
for (i in seq_along(rand_points_buffer@polygons)){
        r_list[[i]] <- crop(image, rand_points_buffer[i,])
    }

rand_points_r <- do.call(merge, args = r_list)


list_rasters <- list(soil_rasters, grass_rasters, tree_rasters, impervious_rasters, plots_101, grds_30)
training_testing_rasters <- unlist(list_rasters)
training_testing_rasters[[71]] <- NULL


training_testing_rasters_wText <- lapply(training_testing_rasters, FUN = function(x) {
                                             texture <- glcm(x[[8]], na_opt = "ignore")
                                             stack(x,texture)
                                         })


training_testing_rasters_wText

## training_testing_rasters_wText <- list()
## for (i in seq_along(training_testing_rasters)){
##     texture <- glcm(training_testing_rasters[[i]][[8]])
##     training_testing_rasters_wText[[i]] <- stack(training_testing_rasters[[i]],texture)
##                     print(i)
## }



plot(training_testing_rasters_wText[[71]])






### Merge the training testing rasters with all the layers into one raster

training_testing_rasters_wText_merged <- do.call(merge, args = training_testing_rasters_wText)
image <- training_testing_rasters_wText_merged
image
### extract the values of the raster at the training data locations
extract_bind_df_addclass <- function(x) {
  w <- raster::extract(image,x)
  w <- do.call("rbind",w)
  w <- data.frame(w)
  w$Class <- names(x)
  return(w)
}

list_classes <- list(l_water, l_grass, l_tree, l_soil, l_impervious)

beginCluster()
b <- lapply(list_classes, function(x) extract_bind_df_addclass(x))
endCluster()


classified_px <- do.call("rbind", b)

classified_px$Class %<>% as.factor()

lei_naip_df <- classified_px
write.csv(lei_naip_df, "../NAIP/lei_naip_df.csv")

### remove water
lei_naip_df <- lei_naip_df %>%
    filter(Class != "water")

#remove missing values
lei_naip_df <- lei_naip_df %>%
    filter(complete.cases(.))

### build models

# Make Task
lei.classif.task <- makeClassifTask(id= "lei_naip", data = lei_naip_df, target = "Class")

# Set parameters for tuning
ctrl <- makeTuneControlIrace(maxExperiments = 200L)
rdesc <- makeResampleDesc("CV",iters = 3L)

rf.ps <- makeParamSet(
    makeIntegerParam("nodesize", lower = 1L, upper = 20L),
    makeIntegerParam("ntree", lower = 1L, upper = 10L,
                      trafo = function(x) 2^x),
    makeIntegerLearnerParam("mtry", lower = 1L, upper = 10L))

svm.ps <- makeParamSet(
                    makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot")),
                    makeNumericParam("sigma", lower = -10, upper = 10,
                                     trafo = function(x) 2^x,
                                     requires = quote(kernel == "rbfdot")),
                    makeIntegerParam("degree", lower = 2L, upper = 5L,
                                     requires = quote(kernel == "polydot")),
                    makeIntegerParam("C", lower = 5L, upper = 30L,
                                     trafo = function(x) 5*x + 50))
# Tune    ... This is not working
lei.rf.res <-  tuneParams("classif.randomForest", lei.classif.task, rdesc, par.set = rf.ps, control = ctrl)
lei.svm.res <- tuneParams("classif.ksvm", lei.classif.task, rdesc, par.set = svm.ps, control = ctrl)

# Create Learners
lei.rf.lrn <-setHyperPars(makeLearner("classif.randomForest",predict.type="prob", fix.factors.prediction = T))
lei.rf.lrn

lei.svm.lrn <- setHyperPars(makeLearner("classif.ksvm", predict.type = "prob", fix.factors.prediction = T))
lei.svm.lrn

# Train Learners to create models
lei.rf.mod <- train(lei.rf.lrn, lei.classif.task)
lei.svm.mod <- train(lei.svm.lrn, lei.classif.task)

### predict on the list of rasters
#### Apply RF model to raster
training_testing_rasters_wText[[443]]

for (i in seq_along(training_testing_rasters_wText)){
                                             names(training_testing_rasters_wText[[i]]) <-attributes(lei.rf.mod$learner.model$terms)$term.labels
}


rf_pred_training_testing_rasters_wText<- lapply(training_testing_rasters_wText, FUN= function(x){
                                       raster::predict(object = x, model = lei.rf.mod$learner.model, na.rm = T, inf.rm = T)
                                   })

### grids 414-443
grd <- rf_pred_training_testing_rasters_wText[414:443]
grd
freq(grd[[1]])

#### get frequency table for each raster
freq_grd30 <- lapply(grd, raster::freq)
freqdf_grd30 <- lapply(freq_grd30, as.data.frame)
#### get percent tree cover by plot (according to image)


percent_tree <- lapply(freqdf_grd30, FUN = function(x) {
                                              x[x$value == 4,2]/sum(x[,2])
                                          })


percent_tree <- unlist(percent_tree)


percent_tree <- percent_tree[seq(1,60,2)]
percent_tree

### read in andy's accuracy assessment
grid_accuracy <- read.csv("../RD_UFIA_GridAccuracy/grid_accuracy_assessment_andy.csv", header = T)
head(grid_accuracy)



#### get percent tree cover by plot
d <- grid_accuracy %>%
          group_by(Plot, Cover_Type) %>%
          summarize(number = n()) %>%
          mutate(percent_cover = number/225) %>%
          filter(Cover_Type == "t")

e <- rbind(d,data.frame(Plot = c(1,11,26), Cover_Type = c("t","t","t"), number = c(0,0,0),percent_cover = c(0,0,0)))
e <- arrange(e, Plot)

R_plotNames <- c(10,1,4,29,16,13,14,18,6,22,5,11,15,26,17,9,25,30,20,3,21,23,7,12,2,27,8,19,24,28)
d <- cbind(e,R_plotNames)
d
d <- arrange(d, R_plotNames)


andy_pctTree <- d$percent_cover

### RMSE
mean((percent_tree - andy_pctTree)^2)^0.5
png(filename = "naip_rf_grid_prediction.png")
plot(andy_pctTree,percent_tree, main = "RMSE = 16.5%")
abline(a=0,b=1,col="red")
dev.off()


#### predicting on field data
plots_101

### assess accuracy




library(dplyr)
grid_accuracy <- read.csv("../RD_UFIA_GridAccuracy/grid_accuracy_assessment_andy.csv", header = T)
head(grid_accuracy)


d <- grid_accuracy %>%
    group_by(Plot, Cover_Type) %>%
    summarize(number = n()) %>%
    mutate(percent_cover = number/225) %>%
    filter(Cover_Type == "t")
d


#+END_SRC

** More old
*** Accuracy Assessment
1) Point based accuracy
2) Grid based accuracy
3) Field plot based accuracy
4) Random forest or SVM image classification


Need to create functions for each of the accuracy assessments.

Input:
- Classified Raster Layer
- The "ground truth" data

Output:
- Overall Accuracy
- Confusion Matrix
-

*** Point based, google earth
*** 100m2 grid, google earth
*** Field Data

For each accuracy assessment I need to save the output, especially the
overall accuracy so I can include it in the final accuracy table at
the end.

*** Classifications
*** SVM or RF classification
input:
- SVM or RF, indicate which one to use
- raster brick of bands to predict on
- training data (points or polygons)
- if segmented, the segment raster layer

output:
- raster layer
- preserve name of segmentation method, bands that go into
  classification, classification method and parameters


*** Segmentation
input:
- pca image
- the bands to use in the pca
- compactness
- size of segments

output:
- raster of segmentations

*
















*** Perform PCA
I've decided I can perform the pca on each tile separately and apply
the segmentation.

#+BEGIN_SRC R

  Mad.Path <- "../DD_NAIP-imagery/madison_wausau_mounthoreb_NAIP/Madison"
                                              # make directory for PCA tiles
      dir.create(path = str_c(image.training.testing.dir,"/PCATiles"))

                                              # Sample from every raster
      tile.paths <- list.files(str_c(Mad.Path,"/RatioTiles"), pattern = "*.tif$", full.names = T)
      tile.names <- list.files(str_c(Mad.Path,"/RatioTiles"), pattern = "*.tif$", full.names = F)


      cl <- makeCluster(cores)
      registerDoParallel(cl)

      sr <- foreach (i = seq_along(tile.names), .packages = c("raster"), .combine ="rbind") %dopar% {
          tile <- stack(tile.paths[i])
          s <- sampleRandom(tile, 100)
      }

      colnames(sr) <- c("blue","green","red","nir","b_ratio","g_ratio","r_ratio","n_ratio","ndvi")

                                              # Perform PCA on sample
      pca <- prcomp(sr, scale = T)



                                              # Apply PC rotation to each testing/training region
      cl <- makeCluster(cores)
      registerDoParallel(cl)

      tile.paths <- list.files(str_c(image.training.testing.dir,"/RatioTiles"), pattern = "*.tif$", full.names = T)
      tile.names <- list.files(str_c(image.training.testing.dir,"/RatioTiles"), pattern = "*.tif$", full.names = F)


      foreach (i = seq_along(tile.paths), .packages = c("raster")) %dopar% {
          predict_pca(tile.paths[i],
                      pca,
                      n.comps = 3,
                      out.path = paste0(image.training.testing.dir,"/PCATiles/",tile.names[i])
                      )
      }

                                              # scale pca tiles between 0 and 255
      dir.create(str_c(image.training.testing.dir,"/ScaledPCATiles"))
                                              # find min and max
      tile.paths <- list.files(str_c(image.training.testing.dir,"/PCATiles"), pattern = "*.tif$", full.names = T)
      tile.names <- list.files(str_c(image.training.testing.dir,"/PCATiles"), pattern = "*.tif$", full.names = F)

                                              #get min and max

      getRasterMinMax <- function(t.path) {
          tile <- stack(t.path)
          mn <- cellStats(tile, stat = "min")
          mx <- cellStats(tile, stat = "max")
          mnmx <- c(mn,mx)
          return (mnmx)
      }

      cl <- makeCluster(cores)
      registerDoParallel(cl)

      minmax <- foreach(i = seq_along(tile.paths), .packages = "raster", .combine = "rbind") %dopar% {
          getRasterMinMax(tile.paths[i])
      }

      mn <- apply(minmax, 2, min, na.rm = T)[1:3]
      mx <- apply(minmax, 2, max, na.rm = T)[4:6]



      range0255 <- function(tile.path, tile.name){
          r <- stack(tile.path)
          r <- (r - mn)/(mx-mn) * 255
          writeRaster(r, paste0(image.training.testing.dir,"/ScaledPCATiles/",tile.name), overwrite=TRUE, datatype = 'INT1U')
        }


      registerDoParallel(cl)

      foreach (i = seq_along(tile.paths), .packages = "raster") %dopar% {
          range0255(tile.paths[i], tile.names[i])
      }


    # Remove the unscaled PCA tiles to make some disk space
  #  do.call(file.remove,list(tile.paths))

#+END_SRC
** segment df messing around
#+BEGIN_SRC R
    out <- r %>%
        group_by(segment) %>%
        arrange(segment) %>%
        mutate(x.center = x - quantile(x = x, probs = .5),
               y.center = y - quantile(x = y, probs = .5))

    s1 <- filter(out, segment %in% 1)
    s2 <- filter(out, segment %in% 2)
    s3 <- filter(out, segment %in% 3)
    s6 <- filter(out, segment %in% 6)
    s10 <- filter(out, segment %in% 1:10)

    ggplot(s10,aes(x = n_ratio)) + geom_histogram() +
        facet_wrap(~segment)






    s1 %>%
    group_by(segment) %>%
    do(fitPolyXYlm(x = .$x.center, y = .$y.center, z = .$ndvi))



    look <- r %>%
        group_by(segment) %>%
        summarize(sum(ndvi >1))

    summary(look)

#+END_SRC

#+BEGIN_SRC R :results graphics :file figs/test.png
  ggplot(s10,aes(x = x.center, y = y.center, fill = n_ratio)) + geom_raster() + coord_equal() + facet_wrap(~segment)
#+END_SRC

#+BEGIN_SRC R :results graphics :file figs/test2.png
  ggplot(s10,aes(x = n_ratio)) + geom_histogram() +
      facet_wrap(~segment)

#+END_SRC




*** <2016-03-11 Fri> Old: Foreach image in Naip and Panspot:

**** Foreach Training Region, grid and field data
***** Make PixelFeatureDFs and SegmentationFeatureDFs for Training Regions
  1) Input
     - Testing Region Shapefiles
     - image
  2) Operation
     - Reproject Shapefiles to that of image
     - Crop image to each polygon in the shapefile
     - Derive PixelfeatureDFs and SegmentationFeatureDF from each tile of the image in region of each polygon
  3) Output
     - SegmentationFeatureDFs for every training polygon
     - PixelFeatureDFs for every pixel

****** Reproject Training Region Shapefile to Image
 #+BEGIN_SRC R
    foreach(i = seq_along(image.names)) %do% {
          Reproject_Shapefile_to_Image_CRS(training.region.dsn,
                                           training.region.layer,
                                           image.paths[i],
                                           training.region.imageCRS.dsn[i])
      }

  #+END_SRC

  #+RESULTS:
  #+begin_example
   OGR data source with driver: ESRI Shapefile
  Source: "../RD_Training_Regions/Madison_TrainingRegions", layer: "madisonTrainingPolygons"
  with 15 features
  It has 1 fields
  OGR data source with driver: ESRI Shapefile
  Source: "../RD_Training_Regions/Madison_TrainingRegions", layer: "madisonTrainingPolygons"
  with 15 features
  It has 1 fields
  [[1]]
  NULL

  [[2]]
  NULL
 #+end_example

****** Crop image to create a smaller image around each of the testing polygons
 #+BEGIN_SRC R :results none
 foreach(i = seq_along(image.names)) %do% {
    Crop_image_to_each_Shapefile_polygon(training.region.imageCRS.dsn[i],
					 training.region.layer,
					 image.paths[i],
					 cores = cores,
					 output.dir = image.cropped.to.training.dir[i])
 }
  #+END_SRC

****** Foreach tile image around the groups of testing points
******* Create Pixel and Segment Feature Dataframe for each of these smaller images
******** Start R Loop, for every smaller image, do in parallel, :
  #+BEGIN_SRC R

     cl <- makeCluster(cores)
     registerDoParallel(cl)

  pixel.added.features.raster.list <- foreach(j = seq_along(image.names)) %do% {

     tile.names <- list.files(image.cropped.to.training.dir[j]) %>%
	 str_extract(., pattern = "[0-9]+.tif") %>%
	 str_extract(., pattern = "[0-9]+") %>%
	 na.omit()

     foreach (i = tile.names,
              .packages = c("raster","stringr")) %dopar% {
   #+END_SRC

   #+RESULTS:

******** Add Ratios
  #+BEGIN_SRC R
     add.ratios.ndvi(tile.dir = image.cropped.to.training.dir[j],
                     tile.name = i)
  }
  }
   #+END_SRC

   #+RESULTS:
******** Save Pixel Feature Dataframe
  #+BEGIN_SRC R
      pixel.feature.dfs <- foreach(i = seq_along(image.names)) %do% {
          pixel.feature.df <- Create.Pixel.Feature.df(pixel.added.features.raster.list[[i]])
      }

    foreach(i = seq_along(image.names)) %do% {
	saveRDS(pixel.feature.dfs[[i]], file = str_c(image.cropped.to.training.dir[i],"/","PixelFeatureDF",".rds"))
    }

  #+END_SRC

  #+RESULTS:
  :  There were 30 warnings (use warnings() to see them)
  : [[1]]
  : NULL
  :
  : [[2]]
  : NULL

******** Perform PCA
  #+BEGIN_SRC R :results none

       cl <- makeCluster(cores)
       registerDoParallel(cl)



    foreach(j = seq_along(image.names)) %do% {

	tile.names <- list.files(image.cropped.to.training.dir[j]) %>%
           str_extract(., pattern = "[0-9]+.tif") %>%
           str_extract(., pattern = "[0-9]+") %>%
           na.omit()

	foreach(i = tile.names, .packages = c("stringr","raster")) %dopar%
            image.pca(image.dir = image.cropped.to.training.dir[j],
		 tile.name = i,
		 pca.model = pca[[j]])
    }
   #+END_SRC

******** Segmentation

  #+NAME: training.dir.NAIP
  #+BEGIN_SRC R
  message(image.cropped.to.training.dir[1])
  #+END_SRC

  #+RESULTS: training.dir.NAIP
  : ../DD/madisonNAIP/Madison_Training


  #+BEGIN_SRC sh :var dir=training.dir.NAIP
     cd $dir
     # pixel size
     # desired area for superpixel/segments
     # compactness value
     # directory
     python ../../../code/fia_segment_cmdArgs.py 1 30 15 &
     python ../../../code/fia_segment_cmdArgs.py 1 60 30 &
     python ../../../code/fia_segment_cmdArgs.py 1 105 32 &
  #+END_SRC

  #+RESULTS:

  #+NAME: training.dir.panSPOT
  #+BEGIN_SRC R
  message(image.cropped.to.training.dir[2])
  #+END_SRC

  #+RESULTS: training.dir.panSPOT
  : ../DD/geomatica_SPOT_panshp/Madison_Training

  #+BEGIN_SRC sh :var dir=training.dir.panSPOT
     cd $dir
     # pixel size
     # desired area for superpixel/segments
     # compactness value
     # directory
     python ../../../code/fia_segment_cmdArgs.py 1.5 30 10 &
     python ../../../code/fia_segment_cmdArgs.py 1.5 60 20 &
     python ../../../code/fia_segment_cmdArgs.py 1.5 105 21 &
  #+END_SRC

  #+RESULTS:


******** Create Segment Feature Dataframe

  #+BEGIN_SRC R :results none
    cl <- makeCluster(cores)
    registerDoParallel(cl)

    seg.feature.dfs <- foreach(j = seq_along(image.names)) %do% {

	tile.names <- list.files(image.cropped.to.training.dir[j]) %>%
            str_extract(., pattern = "[0-9]+_N-[0-9]+_C-[0-9]+") %>%
		na.omit()

	seg.params <- unique(str_extract(tile.names,"N-[0-9]+_C-[0-9]+"))

	foreach(seg.param.set = seg.params) %do% {

            tile.names.sub <- tile.names[which(complete.cases(str_extract(tile.names,seg.param.set)))]

            out <- foreach (i = tile.names.sub,
                            .packages = c("raster","stringr","dplyr","broom","tidyr")) %dopar% {
				Create.Segment.Feature.df(image.dir = image.cropped.to.training.dir[j],
                                                          tile.name = i)}
            out <- bind_rows(out)

            saveRDS(out, file = paste0(image.cropped.to.training.dir[j],"/", "segment_",seg.param.set,"_FeatureDF.rds"))
	    out
	}
    }

   #+END_SRC

***** Create Dataframes to build models, assign Class to feature dfs
  1) Input
     - Segmentation Layer from the Training Regions
     - Classified Training Polygons for each image (NAIP and panSPOT)
  2) Operation
     - For Pixels, extract coordinates of pixels that are inside training polygons
       - columns: x,y,class
       - join to pixel feature df
     - For Segments
       - Determine which segments fall majority within training polygons
       - Assign segments the class of the training polygon
       - columns: segment id, class
       - join to segment df

  3) Output
     - Model Building Dataframes, 1 for each image and segmentation combination
 #+BEGIN_SRC R :results none

     ## i <- 1
     ## feature.df.rds <- featureDF.files[1]
     ## SegmentFeatureDF = feature.df
     ## training.sp = training.polygons
     ## seg.tiles.dir = image.cropped.to.training.dir
     ## feature.df.rds <- featureDF.files[6]
     ## seg.params <- segment.params[1]
     ## seg.file <- seg.files[6]
     ## PixelFeatureDF = feature.df
     ## training.sp = training.polygons



   model.building.dfs <-  foreach(i = seq_along(image.names)) %do% {
         files.in.dir <- list.files(image.cropped.to.training.dir[i])
         index.featureDF.files <- which(complete.cases(str_extract(files.in.dir,"FeatureDF.rds$")))
         featureDF.files <- files.in.dir[index.featureDF.files]

         training.polygons <- readOGR(dsn = training.region.dsn, layer = training.polygon.layer[i])

         foreach(feature.df.rds = featureDF.files, .packages = c("mlr","foreach","doParallel", "stringr")) %dopar% {

             feature.df <- readRDS(file = str_c(image.cropped.to.training.dir[i],"/",feature.df.rds))

             if(complete.cases(str_extract(feature.df.rds, "Pixel"))) {
                 model.building.df <- getPixel.Class.and.Coords.Within.Polygon(PixelFeatureDF = feature.df,
                                                          training.sp = training.polygons)
             saveRDS(object = model.building.df, file = paste0(image.cropped.to.training.dir[i],"/","PixelModelBuilding.rds"))
             }

             if(complete.cases(str_extract(feature.df.rds, "segment"))) {
                 segment.params <- str_extract(feature.df.rds, "N-[0-9]+_C-[0-9]+")
                 model.building.df <- getSegment.class.and.features.Within.Polygon(SegmentFeature = feature.df,
                                              training.sp = training.polygons,
                                              seg.tiles.dir = image.cropped.to.training.dir,
                                              seg.params = segment.params)
             saveRDS(model.building.df, file = str_c(image.cropped.to.training.dir[i],"/",segment.params,".ModelBuilding.rds"))
             }
             model.building.df
         }
     }

 #+END_SRC



****** Obsolete: Convert Segmentation Layer to Polygons
  #+BEGIN_SRC R :results none :eval no
      image.names <- list.files(image.cropped.to.training.dir) %>%
          str_extract(., pattern = "[0-9]+_N-[0-9]+_C-[0-9]+.tif") %>%
          str_extract(., pattern = "[0-9]+") %>%
          na.omit()

      cl <- makeCluster(cores)
      registerDoParallel(cl)


      foreach (i = image.names, .packages = c("raster","sp","gdalUtils")) %dopar% {
          polygonize.and.add.Class(image.dir = image.cropped.to.training.dir,
                                   image.name = i)
    }

  #+END_SRC

****** Obsolete: Merge Training Polygons with Segment Feature dataframe
  #+BEGIN_SRC R :eval no
    create.df.toBuildModel.fromTrainingPolygons.and.SegmentFeatureDFs(manuallyClassifiedPolygondir = ManuallyClassifiedTrainingPolygons.dir,
                                                                      image.dir = image.cropped.to.training.dir)
  #+END_SRC


***** Create Models
  1) Input
     - Model Building data frames

  2) Operation
     - Build Models using mlr
       - untuned
       - tuned

  3) Output
     - Models for classifying images
       - RF or SVM (2 options)
	 - All 3 classes in one model, or just one class in a model (4 options)
	   - Highly tuned or default parameters (2 options)

****** Build and Save models
 #+BEGIN_SRC R
   foreach(i = seq_along(image.names)) %do% {
       ModelBuildingRDSs <- list.files(image.cropped.to.training.dir[i]) %>%
           str_extract(., ".*ModelBuilding.rds") %>%
           na.omit()

       foreach(ModelBuildingRDS = ModelBuildingRDSs) %do% {
           Build.and.Save.models(dir = image.cropped.to.training.dir[i],
                                 modelBuildingData = ModelBuildingRDS,
                                 models.dir = Models.dir[i])
       }
   }
 #+END_SRC


  #+BEGIN_SRC R
      rdesc <- makeResampleDesc("CV", iters = 3)

      r <- resample(learner = SVM_response, task = all.task, resampling = rdesc)

      rf <- models[[1]][[1]]

      p <- predict(rf, task = all.task)


  #+END_SRC

******* obsolete some graphic explorations
  #+BEGIN_SRC R :eval no
                  dir = image.cropped.to.training.dir
		 modelBuildingData = "modelBuildingData.rds"


          dat <- readRDS(str_c(dir,"/",modelBuildingData)) %>%
              dplyr::select(-zone)


	library(GGally)

      #  dat2 <- rename(dat, "xy.inter" = `x:y`)
      names <- colnames(dat)
      names <- str_replace(names, "\\(",".")
      names <- str_replace(names, "\\)",".")
      names <- str_replace(names, "\\:",".")

    colnames(dat) <- names

  dat <- mutate(dat, Class = as.factor(Class))

  #+END_SRC

  #+RESULTS:
  [[file:figs/pairs.png]]

  #+BEGIN_SRC R :results graphics :file figs/pairs.png :eval no :width 3000 :height 3000
  #  ?ggpairs
          ggpairs(dat, mapping = ggplot2::aes(color = Class), axisLabels = "show")
  #+END_SRC

  #+RESULTS:
  [[file:figs/pairs.png]]

  #+BEGIN_SRC R :results graphics :file figs/byGroup.png :height 2000 :width 200 :eval no
    dat.g <- gather(dat, key = band, value = value, -Class)
    ggplot(dat.g, aes(x = Class, y = value)) + geom_boxplot() + facet_grid(band~1, scales = "free")
  #+END_SRC

   #+BEGIN_SRC R

   #+END_SRC


***** Classify Testing Regions
****** Grids and Fieldplot data

******* Reproject Testing Region Shapefile to Image
 #+BEGIN_SRC R
     foreach(i = seq_along(image.names)) %do% {
           Reproject_Shapefile_to_Image_CRS(grid.accuracy.region.dsn,
                                            grid.accuracy.region.layer,
                                            image.paths[i],
                                            grid.accuracy.region.imageCRS.dsn[i])
       }

   #+END_SRC

  #+RESULTS:
  #+begin_example
   OGR data source with driver: ESRI Shapefile
  Source: "../RD_Accuracy/Grids", layer: "All_Grids_Accuracy_Assessment_pts"
  with 18365 features
  It has 10 fields
  OGR data source with driver: ESRI Shapefile
  Source: "../RD_Accuracy/Grids", layer: "All_Grids_Accuracy_Assessment_pts"
  with 18365 features
  It has 10 fields
  [[1]]
  NULL

  [[2]]
  NULL

  There were 15 warnings (use warnings() to see them)
 #+end_example

******* Crop image to create a smaller image around each of the testing polygons
  #+BEGIN_SRC R :results none
  foreach(i = seq_along(image.names)) %do% {
   Crop_image_to_regions_around_points_nameBygrid(grid.accuracy.region.imageCRS.dsn[i],
					  grid.accuracy.region.layer,
					  image.path = image.paths[i],
					  cores = cores,
					  output.dir = image.cropped.to.grid.accuracy.dir[i])
  }
   #+END_SRC

******* Create Pixel and Segment Feature Dataframe for each of these smaller images
******** Start R Loop, for every smaller image, do in parallel, :
 #+BEGIN_SRC R

     cl <- makeCluster(cores)
     registerDoParallel(cl)

  pixel.added.features.raster.list <- foreach(j = seq_along(image.names)) %do% {

     tile.names <- list.files(image.cropped.to.grid.accuracy.dir[j]) %>%
	 str_extract(., pattern = ".*[0-9]+.tif") %>%
	 str_extract(., pattern = ".*[0-9]+") %>%
	 na.omit()

     foreach (i = tile.names,
              .packages = c("raster","stringr")) %dopar% {
   #+END_SRC

   #+RESULTS:

******** Add Ratios
  #+BEGIN_SRC R
     add.ratios.ndvi(tile.dir = image.cropped.to.grid.accuracy.dir[j],
                     tile.name = i)


   #+END_SRC

   #+RESULTS:
   :  There were 50 or more warnings (use warnings() to see the first 50)
******** Save Pixel Feature Dataframe
  #+BEGIN_SRC R

		pixel.feature.df <- Create.Pixel.Feature.df.foreachTile(dir = image.cropped.to.grid.accuracy.dir[j],
		base_pattern = "mad-[0-9]+m-[0-9]+_with_ratios.tif")

            }
       }
  #+END_SRC

  #+RESULTS:

******** Perform PCA
 #+BEGIN_SRC R :results none

       cl <- makeCluster(cores)
       registerDoParallel(cl)

    foreach(j = seq_along(image.names)) %do% {

	tile.names <- list.files(image.cropped.to.grid.accuracy.dir[j]) %>%
           str_extract(., pattern = ".*[0-9]+.tif") %>%
           str_extract(., pattern = ".*[0-9]+") %>%
           na.omit()

	foreach(i = tile.names, .packages = c("stringr","raster")) %dopar%
            image.pca(image.dir = image.cropped.to.grid.accuracy.dir[j],
                 tile.name = i,
                 pca.model = pca[[j]])
    }
   #+END_SRC

******** Segmentation

  #+NAME: testing.dir.NAIP
  #+BEGIN_SRC R
  message(image.cropped.to.grid.accuracy.dir[1])
  #+END_SRC

  #+RESULTS: testing.dir.NAIP
  : ../DD/madisonNAIP/Testing.Accuracy/Grid


  #+BEGIN_SRC sh :var dir=testing.dir.NAIP
     cd $dir
     # pixel size
     # desired area for superpixel/segments
     # compactness value
     # directory
     python ../../../../code/fia_segment_cmdArgs.py 1 30 15 &
     python ../../../../code/fia_segment_cmdArgs.py 1 60 30 &
     python ../../../../code/fia_segment_cmdArgs.py 1 105 32 &
  #+END_SRC

  #+RESULTS:

  #+NAME: testing.dir.panSPOT
  #+BEGIN_SRC R
  message(image.cropped.to.grid.accuracy.dir[2])
  #+END_SRC

  #+RESULTS: testing.dir.panSPOT
  : ../DD/geomatica_SPOT_panshp/Testing.Accuracy/Grid

  #+BEGIN_SRC sh :var dir=testing.dir.panSPOT
     cd $dir
     # pixel size
     # desired area for superpixel/segments
     # compactness value
     # directory
     python ../../../../code/fia_segment_cmdArgs.py 1.5 30 10 &
     python ../../../../code/fia_segment_cmdArgs.py 1.5 60 20 &
     python ../../../../code/fia_segment_cmdArgs.py 1.5 105 21 &
  #+END_SRC

  #+RESULTS:


******** Create Segment Feature Dataframe
 #+BEGIN_SRC R :results none
       cl <- makeCluster(cores)
       registerDoParallel(cl)

       seg.feature.dfs <- foreach(j = seq_along(image.names)) %do% {

           tile.names <- list.files(image.cropped.to.grid.accuracy.dir[j]) %>%
               str_extract(., pattern = ".*[0-9]+_N-[0-9]+_C-[0-9]+") %>%
                   na.omit()

           seg.params <- unique(str_extract(tile.names,"N-[0-9]+_C-[0-9]+"))

           foreach(seg.param.set = seg.params) %do% {

               tile.names.sub <- tile.names[which(complete.cases(str_extract(tile.names,seg.param.set)))]

               out <- foreach (i = tile.names.sub,
                               .packages = c("raster","stringr","dplyr","broom","tidyr")) %dopar% {
                                   seg.df <- Create.Segment.Feature.df(image.dir = image.cropped.to.grid.accuracy.dir[j],
                                                             tile.name = i)
                                   saveRDS(seg.df, file = paste0(image.cropped.to.grid.accuracy.dir[j],"/", i,"segment_FeatureDF.rds"))
                               }
               out
           }
       }

   #+END_SRC

******** Predict Class of each Segment and create classified images
  #+BEGIN_SRC R :results none
     classified.grid.tiles <-
         foreach(j = seq_along(image.names)) %do% {

             models <- list.files(Models.dir[j])

             tile.names <- list.files(image.cropped.to.grid.accuracy.dir[j]) %>%
                 str_extract(., pattern = ".*[0-9]+.tif") %>%
                     str_extract(., pattern = ".*[m]-[0-9]+") %>%
                         na.omit() %>%
                             unique()

             foreach(tile.nm = tile.names) %do% {

                 foreach(model = models) %do% {

                     segmentation.params <- str_extract(model, "N-[0-9]+_C-[0-9]+|Pixel")

                     if(grepl("N-[0-9]+_C-[0-9]+",segmentation.params) {
                            segment.tile.name.append <- paste0("_",segmentation.params,".tif")
                            segment.feature.df.name.append <- paste0("_",segmentation.params,"segment_FeatureDF")
                            classify.raster(segment.feature.df.dir = image.cropped.to.grid.accuracy.dir[j],
                                            model.dir = Models.dir[j],
                                            segment.dir = image.cropped.to.grid.accuracy.dir[j],
                                            classify.out.dir = image.cropped.to.grid.accuracy.dir[j],
                                            tile.name = tile.nm,
                                            segmentation.appendage = segment.tile.name.append,
                                            model.name.rds = model,
                                            segment.feature.appendage = segment.feature.df.name.append)
			} else

                            classify.raster.pixel(pixel.feature.df,
                 }
             }
         }


    stopCluster(cl)



                 if(grepl("N-[0-9]+_C-[0-9]+",segmentation.param) {
  #+END_SRC


**** Assess Accuracy
***** For each accuracy assessment type, field data and google earth grids

***** Grid
 1) Input:
    - classified images for each model for each grid polygon
    - Grid Shapefile

 2) Operation
    - For the 4 different grid sizes (50x50, 100x100, 150x150, 200x200)
    - Depending on the Model Target (all three cover types or a single
      cover type), calculate the proportion of cover in the classified image.
    - Calculate Proportion of cover within the grid shapefile
    - Combine shapefile information with classified image information
      and create RMSEs and plots.

 3) Output

    - Table of Accuracy by model and grid size
    - RMSE plots


****** Calculate the cover of each grid, according to andy's assessment of google earth

 The grids, that are 200mx200m will have 8 sizes, smaller grids will
 have fewer.

 Grid sizes : 25x25, 50x50,75x75,100x100,125x125,150x150,175x175,200x200


 | Grid Number | Grid Size | % T | % G | % I |
 |-------------+-----------+-----+-----+-----|
 | 1           |           |     |     |     |
 | 1           |           |     |     |     |
 |             |           |     |     |     |
 #+BEGIN_SRC R
   grd <- readOGR(dsn = grid.accuracy.region.dsn, layer = grid.accuracy.region.layer)
   grd <- spTransform(grd, CRS("+init=epsg:32616"))
   grd.df <- grd@data



   filter.by.row.and.col <- function(df,nrow.and.col) {
       nrow <-df %>%
           group_by(unq__ID) %>%
           summarize(nrow = max(row))

       df <- left_join(df,nrow)

       df %>%
           filter(nrow >= nrow.and.col,   # remove grids that have fewer than the number of rows & columns
                  row <= nrow.and.col,    # remove rows greater than the number we are interested in
                  column <=nrow.and.col)  # same for columns as rows
   }

   n.rows.and.columns.for.subset <- 15

   out <- filter.by.row.and.col(grd.df, n.rows.and.columns.for.subset)


   add.n.pts.per.grid <- function(df){
       n.pts<-df %>%
           group_by(unq__ID) %>%
           summarize(n.points = n())

       left_join(df,n.pts)
   }


   grd.pts.subset.by.nrow.and.col <- add.n.pts.per.grid(out)


   get.pct.cvr.typ <- function(df) {
       df %>%
           group_by(unq__ID, cvr_typ,n.points) %>%
           summarize(number = n()) %>%
           ungroup() %>%
           mutate(google.truth.pct.cover = number/n.points) %>%
           dplyr::select(-n.points, -number)
   }

   true.pct.cover.15x15grd <- get.pct.cvr.typ(grd.pts.subset.by.nrow.and.col)

   # eventually I'll want to loop through a number of sizes.  For now I'll do 15 rows and 15 columns.
   #a <- lapply(c(5,10,15,20,25,29), function(x) filter.by.row.and.col(grd.df, x))
 #+END_SRC

 #+RESULTS:
 #+begin_example
 OGR data source with driver: ESRI Shapefile
 Source: "../RD_Accuracy/Grids", layer: "All_Grids_Accuracy_Assessment_pts"
 with 18365 features
 It has 10 fields
 Warning: closing unused connection 32 (<-localhost:11289)
 Warning: closing unused connection 31 (<-localhost:11289)
 Warning: closing unused connection 30 (<-localhost:11289)
 Warning: closing unused connection 29 (<-localhost:11289)
 Warning: closing unused connection 28 (<-localhost:11289)
 Warning: closing unused connection 27 (<-localhost:11289)
 Warning: closing unused connection 26 (<-localhost:11289)
 Warning: closing unused connection 25 (<-localhost:11289)
 Warning: closing unused connection 24 (<-localhost:11289)
 Warning: closing unused connection 23 (<-localhost:11289)
 Warning: closing unused connection 22 (<-localhost:11289)
 Warning: closing unused connection 21 (<-localhost:11289)
 Warning: closing unused connection 20 (<-localhost:11289)
 Warning: closing unused connection 19 (<-localhost:11289)
 Warning: closing unused connection 18 (<-localhost:11289)
 Joining by: "unq__ID"
 Joining by: "unq__ID"
 #+end_example

****** Calculate the cover of each classified tile over grid

 for every grid

 for every target (all, tree, grass, impervious)

 for every model  (rf prob, rf response, svm response)

 calculate the percent cover



 read in grid points shapefile

 filter out the madison ones

 filter out points according to size testing

 read in classfied tile

 extract classified pixels at each point in the grid

 calculate % cover of all pixels in grid



 | Grid_unique_ID | whole grid based classification |   |   |
 |----------------+---------------------------------+---+---|
 |                |                                 |   |   |



 #+BEGIN_SRC R
   xy <- dplyr::select(grd.pts.subset.by.nrow.and.col, x, y)
   coordinates(grd.pts.subset.by.nrow.and.col) <- xy
   proj4string(grd.pts.subset.by.nrow.and.col) <- CRS("+init=epsg:3071") # it's WTM for some reason
   grd.pts.subset.by.nrow.and.col <- spTransform(grd.pts.subset.by.nrow.and.col,CRS("+init=epsg:26916"))

   pct.cover.acc.img.classification <- calculate.prct.cover.in.classified.tiles(pts = grd.pts.subset.by.nrow.and.col,
						img.dir = image.cropped.to.grid.accuracy.dir)
 #+END_SRC

 #+RESULTS:
 #+begin_example
 Error in UseMethod("select_") :
   no applicable method for 'select_' applied to an object of class "c('SpatialPointsDataFrame', 'SpatialPoints', 'Spatial', 'SpatialPointsNULL', 'SpatialVector')"
 Error in `coordinates<-`(`*tmp*`, value = list(x = c(573757.451807, 573764.523236,  :
   setting coordinates cannot be done on Spatial objects, where they have already been set
 Warning in `proj4string<-`(`*tmp*`, value = <S4 object of class "CRS">) :
   A new CRS was assigned to an object with an existing CRS:
 +init=epsg:32616 +proj=utm +zone=16 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0
 without reprojecting.
 For reprojection, use function spTransform in package rgdal
  Error in { (from #29) : task 1 failed - "extents do not overlap"
 #+end_example


****** Combine Classified Results with Google "Truth" Results

 #+BEGIN_SRC R
   pct.cover.acc.img.classification %>%
       head()


   pct.cover.acc.img.classification <- pct.cover.acc.img.classification %>%
       gather(key = Cover, value = ClassifiedImagePercentCover, -grid.img.target.model, -grid, -img, -target, -model) %>%
       filter(target == "all" & Cover == "pct_g" |
              target == "all" & Cover == "pct_i" |
              target == "all" & Cover == "pct_t" |
              target == "grass" & Cover == "pct_g" |
              target == "impervious" & Cover == "pct_i" |
              target == "tree" & Cover == "pct_t") %>%
       mutate(cvr_typ = str_sub(Cover, 5,5)) %>%
       arrange(img, grid, model,target)


   d <- d %>%
       rename(grid = unq__ID,
              google.truth.pct.cover = pct.cover)


   b<-left_join(pct.cover.acc.img.classification, d)

		by = "grid")

   head(b, n = 200)





 #+END_SRC


 #+BEGIN_SRC R
   dat <-left_join(pct.cover.acc.img.classification,d.t, by = c("grid" = 'unq__ID')) %>%
       filter(target == "all", cvr_typ =="t")
 #+END_SRC

 #+RESULTS:
 :  Warning in left_join_impl(x, y, by$x, by$y) :
 :   joining factor and character vector, coercing into character vector

 #+BEGIN_SRC R :results graphics :file figs/acc.png
   ggplot(dat, aes(x = pct.cover, y = pct_t, color = model, size = size.meters)) + geom_point()
 #+END_SRC

 #+RESULTS:
 [[file:figs/acc.png]]

 #+BEGIN_SRC R :results graphics :file figs/acc.png
   ggplot(filter(dat,size.meters == 50), aes(x = pct.cover, y = pct_t, color = model, size = size.meters)) + geom_point() +
 facet_wrap(~grid)
 #+END_SRC

 #+RESULTS:
 [[file:figs/acc.png]]


 #+BEGIN_SRC R :results graphics :file figs/test.png
 ggplot(filter(grid2_accuracy,grid_type == "200m"), aes(x = row, y = column, color = cover_type)) + geom_point() +
 facet_wrap(~CID)

 #+END_SRC

 #+RESULTS:
 [[file:figs/test.png]]


 #+BEGIN_SRC R
   library(rCharts)

   r1 <- rPlot(pct_t ~ pct.cover, data = d, color = "model")
   r1

 #  r1$publish('Scatterplot3', host = 'gist')
   print(r1)
 #+END_SRC

 #+RESULTS:

***** Fieldplot

 Same as with Grid, but adjust the definitions of "tree" in the field
 data and see how accuracy varies.

***** Combine NAIP Accuracy Assessments


